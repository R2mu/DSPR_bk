[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GWS Data Science and Problem Recognition",
    "section": "",
    "text": "Preface\nThis is a Quarto book is designed as a brief introduction to data science and problem/pattern recognition. Look to copy code chunks that exist throughout the book and try run them in your r studio console.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html#why-r",
    "href": "intro.html#why-r",
    "title": "1  Overview",
    "section": "1.1 Why R?",
    "text": "1.1 Why R?\nComparing between programming languages whilst fun is really not a useful exercise. Long story short, there a three programming languages currently that saturate sports analytics. Python, R and SQL. We are using R within R studio, namely because\n\nSlightly more user friendly for non programmers hence, faster to prototype ideas\nThere are more resources available for common analytically techniques that we will use in sport.\nRstudio is a fantastic programming IDE, you are able to also write, SQL, Python, Javascript and more whilst working in an R session.\n\nBelow is a photo which I think encapsulates the goal of R as a programming language very nicely.\n\nThe above photo is taken from https://r4ds.had.co.nz/introduction.html which is a fantastic resource for learning R.\nUltimately whatever you choose, it is better to get really good at one language than get just okay at a couple. You will find once you learn one it will become easier each time you attempt to learn another. Below are some of the aspects that I would consider strengths of other programming languages, below is very much a subjective spiel of my experience so take it with a grain of salt.\nPython: Is the most popular programming language at the moment and there is good reason for it. As the creator of Shiny R Joe Cheng said, “Python is the second-best language for anything”(Initial quote from Dan Callahan). In particular, Python will have a lot more resources available for deep learning, developing servers and working with external hardware or IOT applications. While there are definitely some questions in sports performance that could utilize deep learning approaches these are exceptions and not the rule in my opinion.\nSQL: Is mainly thought of a database language in that it is largely responsible for calling databases that are stored on servers. In particular, SQL utilities key verbs such as From, Select, Filter, Inset, Group, Join, Summarize. One of the reasons why I am teaching you tidyverse is as you will see later, a lot of the key verbs discussed above are used within dplyr, so by learning tidyverse (dplyr in particular) you are developing your SQL knowledge.\nJulia: A newer scientific programming language that is aiming to have the speed of compiled languages such as C++ but the readability of languages like R and Python. I’ve played around with it a little and it is quite a nice language, in particular its ability to program CUDA for utilizing GPU and also how easy it makes it to parallelize code. That being said, I haven’t had a use case yet where it has made sense for me to use it over R or Python, additionally as it is a newer language the amount of available support and information is lacking when compared to other languages.\nJavascript: Love hate relationship with this language. I love that it is non event blocking which means you can run some pretty fast real time simulations with packages such as D3. It is just a hard language to learn added with\n\n1.1.1 A quick note on best coding practices\nThis is a classic case of do as I say not as I do. I have no doubt over the coming months there will be times when I don’t adhere to some of my suggestions. However, I have tried to put down things I wish I did when I was starting my R journey.\n\nComment more than necessary. This can be really useful if you get stuck with code not working and you want to get advice from something like CHATGPT. An example comment may be something like “aggregate value column by group and plot as horizontal bar chart sorted from highest to lowest”\nDon’t be afraid to be overly modular with your code. This will make more sense later on, but is is better to slowly build up steps making sure they are correct then building once massive function then working backwards trying to solve where it may not be working\nAttempt to be consistent with naming conventions\n“We should forget about small inefficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.” Donald Knuth. This is an example of advice I should listen to as I will often spend an hour trying to optimize something that took 15 seconds to run down to 5 seconds.\nTry to start with the end in mind. What do you want your report to look like? What analysis method do you think it will be most appropriate? Are there visuals that you want to use? This should help you then work backwards to the data you current have available.\nBe open minded. Unfortunately/fortunately with many data science problems there are many ways to get to the same outcome."
  },
  {
    "objectID": "module1.html#installing-and-loading-r-packages",
    "href": "module1.html#installing-and-loading-r-packages",
    "title": "2  Module 1",
    "section": "2.1 Installing and loading R packages",
    "text": "2.1 Installing and loading R packages\nIf you have never installed an R package there is a couple of options which I have outlined below :\n\n# \"#\" Will comment out R code and will just print text\n# as I already have pacman loaded I will comment it out.\n# the short cut to remove comments from code is ctrl+shift+c\n\n#install.packages(\"pacman\")\n\npacman::p_load(tidyverse,data.table,httr,jsonlite,htmlTable,sf,tidytext)"
  },
  {
    "objectID": "module1.html#loading-data-locally",
    "href": "module1.html#loading-data-locally",
    "title": "2  Module 1",
    "section": "2.2 Loading data locally",
    "text": "2.2 Loading data locally\nThere a couple of different ways to load data. As I want to attempt to future proof your learning we will use the fread() function from the data.table package. Many options for reading in csv;s exist but this by far is one of the fastest ways to load data in R.\nOption 1: if the data is located in your directory it is a sample as.\n\n# option 1: if the data is located in your directory it is a sample as.\n\ndata1   &lt;- fread(\"PlayerLong.csv\")\nplaypos &lt;- fread(\"PlayPos3.csv\")\n\nOption 2: Data is located elsewhere on your computer you will need to put the full path of where it is located. A simple way to get started with your path is with the getwd() function\n\ngetwd()\n\n[1] \"C:/Users/Research/Desktop/GWS_DSPR\"\n\n\n\n#data1 &lt;- fread(\"/Users/Research/Dropbox/Rscripts/PlayerLong.csv\")\n\nOption 3: Finally if you are feeling lazy or struggling to put the correct path in you file.choose();\n\n#data1 &lt;- fread(file.choose())\n\n\n2.2.1 Loading data from API\nWith the way technologies are heading, connecting to API’s to extract data are only going to become more common. Hence, I will give a quick example of how you may do that in R. Unfortunately, due differing data privacy and safety protocols, some API’s will require different methods of securing a “handshake”. Below is a simple example that does not require authentication.\n\n## get current location of ISS\nurl &lt;- \"https://api.wheretheiss.at/v1/satellites/25544\"\n\nworld_coordinates &lt;- map_data(\"world\") \n\n\n  df  &lt;- fromJSON(url,simplifyDataFrame = T)|&gt;data.frame()\n  \n  #dff &lt;- rbind(dff,df)\n  \n\n  print(ggplot(df,aes(longitude,latitude))+\n          geom_map( \n            data = world_coordinates, map = world_coordinates, \n            aes(long,lat, map_id = region) \n          )+\n          geom_point(col=\"orange\",size=2)+\n          theme_bw())\n\nWarning in geom_map(data = world_coordinates, map = world_coordinates,\naes(long, : Ignoring unknown aesthetics: x and y\n\n\n\n\n\n\n\n2.2.2 Binding multiple CSVs from a folder\nSometimes you will have a list of multiple CSV’s that are the same in structure that just represent different dates of saving. In R it is relatively simple to loop through a directory and append all the files. There are multiple ways of doing this but here is one of the most concise ways I have come across.\n\n## directory\nloc &lt;- \"/Users/Research/Dropbox/Rscripts/listExample\"\n\n# gets a list of files in directoru\nfiles &lt;- list.files(path = loc, pattern = \"\\\\.csv$\",full.names = T)\n\n## loops through and binds them together. \ncombined_df &lt;- rbindlist(lapply(files, fread))\n\nunique(combined_df$FIXED_ID)\n\n[1] 106730101 106730102 106730103\n\nhead(combined_df[,.(MATCH_DATE,GROUP_ROUND_NO,HOME_SQUAD,AWAY_SQUAD,\n                    PERIOD,STATISTIC_CODE,FULLNAME)])|&gt;\n  htmlTable::htmlTable()\n\n\n\n\n\nMATCH_DATE\nGROUP_ROUND_NO\nHOME_SQUAD\nAWAY_SQUAD\nPERIOD\nSTATISTIC_CODE\nFULLNAME\n\n\n\n\n1\n28-FEB-19\n1\nCarlton\nEssendon\n1\nMTCHI\n\n\n\n2\n28-FEB-19\n1\nCarlton\nEssendon\n1\nPERST\n\n\n\n3\n28-FEB-19\n1\nCarlton\nEssendon\n1\nCEBO\n\n\n\n4\n28-FEB-19\n1\nCarlton\nEssendon\n1\nCEBO\n\n\n\n5\n28-FEB-19\n1\nCarlton\nEssendon\n1\nCBVS\nAndrew Phillips\n\n\n6\n28-FEB-19\n1\nCarlton\nEssendon\n1\nCBVS\nZac Clarke"
  },
  {
    "objectID": "module1.html#simple-data-cleaning-procedures",
    "href": "module1.html#simple-data-cleaning-procedures",
    "title": "2  Module 1",
    "section": "2.3 Simple data cleaning procedures",
    "text": "2.3 Simple data cleaning procedures\nIn this example we will be using data1 and playpos data.frames. We will explore how to filter, add variables (mutate) , join data bases and summaries. Below I will tidyverse packages to do this but you could also do everything below using the data.table package (my personal favorite) .\n\n# Lets have a look at column names from both \ncolnames(data1)\n\n [1] \"V1\"             \"MATCH_ID\"       \"GAME_ID\"        \"SEASON_ID\"     \n [5] \"GROUP_ROUND_NO\" \"VENUE_NAME\"     \"PERSON_ID\"      \"FULLNAME\"      \n [9] \"SQUAD_NAME\"     \"OPP_SQUAD_NAME\" \"SQUAD_MARGIN\"   \"variable\"      \n[13] \"value\"         \n\n\n\ncolnames(playpos)\n\n[1] \"FULLNAME\"  \"SEASON_ID\" \"PERSON_ID\" \"Position\" \n\n\nQuick way to get additional information about your data.\n\nstr(data1)\n\nClasses 'data.table' and 'data.frame':  2656638 obs. of  13 variables:\n $ V1            : int  1 2 3 4 5 6 7 8 9 10 ...\n $ MATCH_ID      : int  266569840 266569840 266569840 266569840 266569840 266569840 266569840 266569840 266569840 266569840 ...\n $ GAME_ID       : chr  \"R0123\" \"R0123\" \"R0123\" \"R0123\" ...\n $ SEASON_ID     : int  2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 ...\n $ GROUP_ROUND_NO: int  1 1 1 1 1 1 1 1 1 1 ...\n $ VENUE_NAME    : chr  \"MCG\" \"MCG\" \"MCG\" \"MCG\" ...\n $ PERSON_ID     : int  250395 270146 270896 280819 290627 290847 293813 294036 294592 294674 ...\n $ FULLNAME      : chr  \"Jack Riewoldt\" \"Ed Curnow\" \"Trent Cotchin\" \"Dylan Grimes\" ...\n $ SQUAD_NAME    : chr  \"Richmond\" \"Carlton\" \"Richmond\" \"Richmond\" ...\n $ OPP_SQUAD_NAME: chr  \"Carlton\" \"Richmond\" \"Carlton\" \"Carlton\" ...\n $ SQUAD_MARGIN  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ variable      : chr  \"BALL_UP_CLEARANCE\" \"BALL_UP_CLEARANCE\" \"BALL_UP_CLEARANCE\" \"BALL_UP_CLEARANCE\" ...\n $ value         : num  1 1 3 0 2 0 0 2 1 0 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n\nThe above can be a useful problem solver if you are struggling with code, as sometimes things that you might expect to be saved as a integer may be saved as a character. If you then tried to average a column that is considered to be characters you would run into trouble.\n\n2.3.1 Joins\nWe can notice that both data.frames have columns names called PERSON_ID, what we would like to do is join these databases by SEASON_ID & PERSON_ID so that we could have player position joined with the data1 database. To do this we are going to use a left_join() function from dplyr package, this package is loaded when you load Tidyverse .\nA quick note on joins. There are many different types of joins, Left, Right, Inner, Outer and Cross joins. I struggle to remember what they all represent namely because I have only ever really had to use Left joins and the occasional cross join. Below is an example of both a Left and Cross join. See https://r4ds.had.co.nz/relational-data.html#understanding-joins for more information regarding other join methods.\n\n## Cross join: I am going to use the data.table CJ function here just because it is already loaded and it is faster than tidyverse equivalent\n\nlist1 &lt;-seq(as.Date(\"2023-11-20\"), as.Date(\"2024-09-28\"), by=\"days\")\nlist2 &lt;- c(\"A\",\"B\",\"C\",\"D\")\n\nhead(CJ(list1,list2),12)|&gt;htmlTable::htmlTable()\n\n\n\n\n\nlist1\nlist2\n\n\n\n\n1\n2023-11-20\nA\n\n\n2\n2023-11-20\nB\n\n\n3\n2023-11-20\nC\n\n\n4\n2023-11-20\nD\n\n\n5\n2023-11-21\nA\n\n\n6\n2023-11-21\nB\n\n\n7\n2023-11-21\nC\n\n\n8\n2023-11-21\nD\n\n\n9\n2023-11-22\nA\n\n\n10\n2023-11-22\nB\n\n\n11\n2023-11-22\nC\n\n\n12\n2023-11-22\nD\n\n\n\n\n\n\n## now for a left join example\n## simple rule for left join, large merges with small, in our case data1 with playpos\n\ndata2 = left_join(data1,playpos, by =c(\"SEASON_ID\",\"PERSON_ID\",\"FULLNAME\"))\n\n#names(data2)\n\nhead(data2)|&gt;htmlTable::htmlTable()\n\n\n\n\n\nV1\nMATCH_ID\nGAME_ID\nSEASON_ID\nGROUP_ROUND_NO\nVENUE_NAME\nPERSON_ID\nFULLNAME\nSQUAD_NAME\nOPP_SQUAD_NAME\nSQUAD_MARGIN\nvariable\nvalue\nPosition\n\n\n\n\n1\n1\n266569840\nR0123\n2023\n1\nMCG\n250395\nJack Riewoldt\nRichmond\nCarlton\n0\nBALL_UP_CLEARANCE\n1\nKey Fwd\n\n\n2\n2\n266569840\nR0123\n2023\n1\nMCG\n270146\nEd Curnow\nCarlton\nRichmond\n0\nBALL_UP_CLEARANCE\n1\nMid Fwd\n\n\n3\n3\n266569840\nR0123\n2023\n1\nMCG\n270896\nTrent Cotchin\nRichmond\nCarlton\n0\nBALL_UP_CLEARANCE\n3\nMid Fwd\n\n\n4\n4\n266569840\nR0123\n2023\n1\nMCG\n280819\nDylan Grimes\nRichmond\nCarlton\n0\nBALL_UP_CLEARANCE\n0\nKey Def\n\n\n5\n5\n266569840\nR0123\n2023\n1\nMCG\n290627\nDion Prestia\nRichmond\nCarlton\n0\nBALL_UP_CLEARANCE\n2\nMid\n\n\n6\n6\n266569840\nR0123\n2023\n1\nMCG\n290847\nDustin Martin\nRichmond\nCarlton\n0\nBALL_UP_CLEARANCE\n0\nGen Fwd\n\n\n\n\n\n\n\n2.3.2 Selecting and filtering\nOkay , lets say after inspection of the data.frame we feel like some columns are redundant or we simply just want to move some columns around we will achieve this using the select verb. Additionally, lets say we want to just have the data frame represent certain positions and key variables (game statistics) we think are important, this can be achieved using the filter verb.\n\n# Firstly lets get a list of column names \nnames(data2)\n\n [1] \"V1\"             \"MATCH_ID\"       \"GAME_ID\"        \"SEASON_ID\"     \n [5] \"GROUP_ROUND_NO\" \"VENUE_NAME\"     \"PERSON_ID\"      \"FULLNAME\"      \n [9] \"SQUAD_NAME\"     \"OPP_SQUAD_NAME\" \"SQUAD_MARGIN\"   \"variable\"      \n[13] \"value\"          \"Position\"      \n\n\n\n# Lets have a look at what positions exist within the Position column\nunique(data2$Position)\n\n[1] \"Key Fwd\" \"Mid Fwd\" \"Key Def\" \"Mid\"     \"Gen Fwd\" \"Wing\"    \"Gen Def\"\n[8] \"Ruck\"    NA       \n\n\n\n# finally lets explore what statisitcs are within the variable column\nhead(unique(data2$variable),30)\n\n [1] \"BALL_UP_CLEARANCE\"         \"BALL_UP_FIRST_POSSESSION\" \n [3] \"BALL_UP_HITOUT\"            \"BALL_UP_HITOUT_SHARKED\"   \n [5] \"BAULKED\"                   \"BEHIND\"                   \n [7] \"BROKEN_TACKLE\"             \"BU_HITOUT_TO_ADVANTAGE\"   \n [9] \"CB_FIRST_POSSESSION\"       \"CB_HITOUT_SHARKED\"        \n[11] \"CB_HITOUT_TO_ADVANTAGE\"    \"CENTRE_BOUNCE_CLEARANCE\"  \n[13] \"CENTRE_BOUNCE_HITOUT\"      \"CLANGER\"                  \n[15] \"CLANGER_HANDBALL\"          \"CLANGER_KICK\"             \n[17] \"CLEARANCE\"                 \"CONTESTED_KNOCK_ON\"       \n[19] \"CONTESTED_MARK\"            \"CONTESTED_MARK_FROM_OPP\"  \n[21] \"CONTESTED_MARK_FROM_TEAM\"  \"CONTESTED_POSSESSION\"     \n[23] \"CONTESTED_POSSESSION_POST\" \"CONTESTED_POSSESSION_PRE\" \n[25] \"CRUMB\"                     \"DISPOSAL\"                 \n[27] \"DISPOSAL_POST\"             \"DISPOSAL_PRE\"             \n[29] \"EFFECTIVE_DISPOSAL\"        \"EFFECTIVE_HANDBALL\"       \n\n\n\n## select by number or name\ndata2 = data2|&gt;\n   select(4,2,5:9,14,10:13)|&gt;\n# If you want to remove a column you can do the below\n# select(!c(\"PERSON_ID\"))|&gt; this is an example how you may remove a specific column\n#   filter(Position %in% c(\"Mid Fwd\",\"Mid\"))|&gt;\n   filter(variable %in% c(\"CLEARANCE\",\"CONTESTED_POSSESSION\",\n                          \"EFFECTIVE_DISPOSAL\",\"TURNOVER\",\n                          \"TOTAL_GAINED_METRES\",\"PLY_PRESS_PTS\"))\n\n# Check variables\nunique(data2$variable)\n\n[1] \"CLEARANCE\"            \"CONTESTED_POSSESSION\" \"EFFECTIVE_DISPOSAL\"  \n[4] \"TOTAL_GAINED_METRES\"  \"TURNOVER\"             \"PLY_PRESS_PTS\"       \n\n\n\n\n2.3.3 Adding calculated columns\nLets add a calculated column to the data frame using the mutate function. The column we are going to add is going to convert the score differential into a binary win loss where 0 represents a loss and 1 a win.\n\ndata2 &lt;- data2|&gt;mutate(WL = ifelse(SQUAD_MARGIN&gt;0,1,0))\n\n# if we wanted to have draw we could simply do \n\ndata2 &lt;- data2|&gt;mutate(WLD = ifelse(SQUAD_MARGIN&gt;0,1,\n                                    ifelse(SQUAD_MARGIN==0,0,-1)))\n\n# We don't have to break it up as we did above we could simply add two columns at once\ndata2 &lt;- data2|&gt;mutate(WL = ifelse(SQUAD_MARGIN&gt;0,1,0),\n                       WLD = ifelse(SQUAD_MARGIN&gt;0,1,\n                                    ifelse(SQUAD_MARGIN==0,0,-1)))\n\ndata2|&gt;\n  tail(10)|&gt;\n  htmlTable()\n\n\n\n\n\nSEASON_ID\nMATCH_ID\nGROUP_ROUND_NO\nVENUE_NAME\nPERSON_ID\nFULLNAME\nSQUAD_NAME\nPosition\nOPP_SQUAD_NAME\nSQUAD_MARGIN\nvariable\nvalue\nWL\nWLD\n\n\n\n\n1\n2024\n149741648\n7\nMCG\n1013133\nBraeden Campbell\nSydney Swans\nWing\nHawthorn\n76\nPLY_PRESS_PTS\n3.75\n1\n1\n\n\n2\n2024\n149741648\n7\nMCG\n1013230\nLogan McDonald\nSydney Swans\nKey Fwd\nHawthorn\n76\nPLY_PRESS_PTS\n22.8\n1\n1\n\n\n3\n2024\n149741648\n7\nMCG\n1013409\nJames Jordon\nSydney Swans\nMid Fwd\nHawthorn\n76\nPLY_PRESS_PTS\n29.1\n1\n1\n\n\n4\n2024\n149741648\n7\nMCG\n1017091\nJai Serong\nHawthorn\nKey Def\nSydney Swans\n-76\nPLY_PRESS_PTS\n1.2\n0\n-1\n\n\n5\n2024\n149741648\n7\nMCG\n1017094\nConnor Macdonald\nHawthorn\nGen Fwd\nSydney Swans\n-76\nPLY_PRESS_PTS\n25.65\n0\n-1\n\n\n6\n2024\n149741648\n7\nMCG\n1018016\nSeamus Mitchell\nHawthorn\nGen Def\nSydney Swans\n-76\nPLY_PRESS_PTS\n9.15\n0\n-1\n\n\n7\n2024\n149741648\n7\nMCG\n1020895\nJai Newcombe\nHawthorn\nMid\nSydney Swans\n-76\nPLY_PRESS_PTS\n26.55\n0\n-1\n\n\n8\n2024\n149741648\n7\nMCG\n1023482\nCam Mackenzie\nHawthorn\nMid\nSydney Swans\n-76\nPLY_PRESS_PTS\n43.5\n0\n-1\n\n\n9\n2024\n149741648\n7\nMCG\n1027935\nJosh Weddle\nHawthorn\nKey Def\nSydney Swans\n-76\nPLY_PRESS_PTS\n20.55\n0\n-1\n\n\n10\n2024\n149741648\n7\nMCG\n1027965\nMax Ramsden\nHawthorn\nKey Fwd\nSydney Swans\n-76\nPLY_PRESS_PTS\n9.6\n0\n-1\n\n\n\n\n\n\n\n2.3.4 Adding calculated columns by group\n\ndata2 &lt;- data2|&gt;\n  group_by(variable,Position)|&gt;\n  # if you wanted to specific by position you could do the below\n  #group_by(SEASON_ID,Position,variable)|&gt;\n  mutate(avgByVar = round(mean(value),2))|&gt;\n  ungroup()|&gt;\n  mutate(diff = round(value - avgByVar,2))\n  \nhtmlTable(head(data2,10))\n\n\n\n\n\nSEASON_ID\nMATCH_ID\nGROUP_ROUND_NO\nVENUE_NAME\nPERSON_ID\nFULLNAME\nSQUAD_NAME\nPosition\nOPP_SQUAD_NAME\nSQUAD_MARGIN\nvariable\nvalue\nWL\nWLD\navgByVar\ndiff\n\n\n\n\n1\n2023\n266569840\n1\nMCG\n250395\nJack Riewoldt\nRichmond\nKey Fwd\nCarlton\n0\nCLEARANCE\n1\n0\n0\n0.47\n0.53\n\n\n2\n2023\n266569840\n1\nMCG\n270146\nEd Curnow\nCarlton\nMid Fwd\nRichmond\n0\nCLEARANCE\n1\n0\n0\n2.08\n-1.08\n\n\n3\n2023\n266569840\n1\nMCG\n270896\nTrent Cotchin\nRichmond\nMid Fwd\nCarlton\n0\nCLEARANCE\n4\n0\n0\n2.08\n1.92\n\n\n4\n2023\n266569840\n1\nMCG\n280819\nDylan Grimes\nRichmond\nKey Def\nCarlton\n0\nCLEARANCE\n0\n0\n0\n0.2\n-0.2\n\n\n5\n2023\n266569840\n1\nMCG\n290627\nDion Prestia\nRichmond\nMid\nCarlton\n0\nCLEARANCE\n5\n0\n0\n4.56\n0.44\n\n\n6\n2023\n266569840\n1\nMCG\n290847\nDustin Martin\nRichmond\nGen Fwd\nCarlton\n0\nCLEARANCE\n1\n0\n0\n0.87\n0.13\n\n\n7\n2023\n266569840\n1\nMCG\n293813\nTom Lynch\nRichmond\nKey Fwd\nCarlton\n0\nCLEARANCE\n2\n0\n0\n0.47\n1.53\n\n\n8\n2023\n266569840\n1\nMCG\n294036\nGeorge Hewett\nCarlton\nMid\nRichmond\n0\nCLEARANCE\n8\n0\n0\n4.56\n3.44\n\n\n9\n2023\n266569840\n1\nMCG\n294592\nKamdyn McIntosh\nRichmond\nWing\nCarlton\n0\nCLEARANCE\n1\n0\n0\n1.48\n-0.48\n\n\n10\n2023\n266569840\n1\nMCG\n294674\nNick Vlastuin\nRichmond\nGen Def\nCarlton\n0\nCLEARANCE\n0\n0\n0\n0.75\n-0.75\n\n\n\n\n\n\n\n2.3.5 Summarizing data (creating pivot tables)\nOkay the next section is going to go over how we can create pivot tables using the summarise function from the dplyr package which is loaded when you load tidyverse . I will do a couple of different examples of data summaries you may be interested in making using the data-set at hand. I will also show examples of plots as this can be a quick way to just double check your aggregation procedures\n\nlibrary(tidytext)\n\ndata2|&gt;\n  filter(SEASON_ID==2023)|&gt;\n  group_by(SEASON_ID,SQUAD_NAME,GROUP_ROUND_NO,variable)%&gt;%\n  summarise(sum = sum(value))%&gt;%\n  ungroup()|&gt;\n  group_by(SEASON_ID,SQUAD_NAME,variable)%&gt;%\n  summarise(season_avg = mean(sum),\n            season_sd  = sd(sum),\n            season_max = max(sum),\n            season_sum = sum(sum))|&gt;\n    ungroup()|&gt;\n  #arrange(variable, desc(season_avg))|&gt;\n  # Reorder SQUAD_NAME based on season_avg, within each variable facet\n  mutate(SQUAD_NAME_RO = reorder_within(SQUAD_NAME, season_sum, variable))|&gt;\n  ggplot(aes(season_sum,SQUAD_NAME_RO,col=as.factor(SEASON_ID)))+\n  geom_point()+\n  facet_wrap(~variable, scales = \"free\", labeller = label_wrap_gen(width = 10)) +\n  scale_y_reordered() +  # Necessary to apply the custom ordering\n  theme_bw() +\n  theme(legend.position = \"top\",\n        axis.text = element_text(size = 6))\n\n`summarise()` has grouped output by 'SEASON_ID', 'SQUAD_NAME',\n'GROUP_ROUND_NO'. You can override using the `.groups` argument.\n`summarise()` has grouped output by 'SEASON_ID', 'SQUAD_NAME'. You can override\nusing the `.groups` argument.\n\n\n\n\n\nYou don’t need to necessarily know what the code below is doing for now but It is just going to shorten the club names.\nIn this next example lets have a quick look at the average stats across key variables as a function of score differential and or Win vs Loss.\n\ndata2|&gt;\n  filter(SEASON_ID==2023)|&gt;\n  group_by(SQUAD_NAME,GROUP_ROUND_NO,variable,WL)|&gt;\n  summarise(sum = sum(value))|&gt;\n  ggplot(aes(as.factor(WL),sum))+\n  geom_jitter()+\n  geom_boxplot()+\n  theme_bw()+\n  facet_wrap(~variable,scales = \"free\")\n\n`summarise()` has grouped output by 'SQUAD_NAME', 'GROUP_ROUND_NO', 'variable'.\nYou can override using the `.groups` argument.\n\n\n\n\n\n\ndata2|&gt;\n  filter(SEASON_ID==2023)|&gt;\n  group_by(SQUAD_NAME,GROUP_ROUND_NO,variable,SQUAD_MARGIN)|&gt;\n  summarise(sum = sum(value))|&gt;\n  ggplot(aes(SQUAD_MARGIN,sum, col = SQUAD_NAME))+\n  geom_jitter(col=\"gray80\",alpha=.3)+\n  stat_smooth(method = \"lm\",se=F)+\n  theme_bw()+\n  theme(legend.position=\"top\")+\n  facet_wrap(~variable,scales = \"free\")\n\n`summarise()` has grouped output by 'SQUAD_NAME', 'GROUP_ROUND_NO', 'variable'.\nYou can override using the `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nLets delve a little deeper in to CONTESTED_POSSESSION\n\ndata2|&gt;\n  filter(SEASON_ID==2023)|&gt;\n  group_by(SQUAD_NAME,GROUP_ROUND_NO,variable,SQUAD_MARGIN)|&gt;\n  summarise(sum = sum(value))|&gt;\n  filter(variable==\"CONTESTED_POSSESSION\")|&gt;\n  ggplot(aes(SQUAD_MARGIN,sum, col = SQUAD_NAME))+\n  geom_jitter(col=\"gray80\",alpha=.3)+\n  stat_smooth(method = \"lm\",se=F)+\n  theme_bw()+\n  facet_wrap(~variable,scales = \"free\")\n\n`summarise()` has grouped output by 'SQUAD_NAME', 'GROUP_ROUND_NO', 'variable'.\nYou can override using the `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "module1.html#running-your-first-model",
    "href": "module1.html#running-your-first-model",
    "title": "2  Module 1",
    "section": "2.4 Running your first model",
    "text": "2.4 Running your first model\nOkay we are going to run our first statistical model. The simple linear model we will use in this case isn’t technically appropriate to use for the data set at hand but we will improve on this over the coming months. Before we can run the model we need to change the shape of the data-frame from its “long” format to a “wider” format. We can do that using the spread function.\nPersonally, I prefer using dcast.dafwrifwta.table function for this but I am trying to be consistent within the tidyverse for you here.\n\nwide_TV = data2|&gt;\n  filter(SEASON_ID==2023)|&gt;\n # filter(Position%in%c(\"Mid\",\"Mid Fwd\"))|&gt;\n  group_by(SQUAD_NAME,GROUP_ROUND_NO,variable,SQUAD_MARGIN)|&gt;\n  summarise(sum = sum(value))|&gt;\n  spread(key = c(\"variable\"),value = sum)\n\n`summarise()` has grouped output by 'SQUAD_NAME', 'GROUP_ROUND_NO', 'variable'.\nYou can override using the `.groups` argument.\n\n## the data.table method i prefer\nwide_dt = setDT(data2)[SEASON_ID==2023,#&Position%in%c(\"Mid\",\"Mid Fwd\"),\n         ][, .(sum = sum(value)),\n              by=.(SQUAD_NAME,GROUP_ROUND_NO,variable,SQUAD_MARGIN)\n          ][,dcast.data.table(.SD,...~variable,value.var = \"sum\")\n          ]\n\n#wide_TV|&gt;head()|&gt;htmlTable()\nwide_dt|&gt;head()|&gt;htmlTable()\n\n\n\n\n\nSQUAD_NAME\nGROUP_ROUND_NO\nSQUAD_MARGIN\nCLEARANCE\nCONTESTED_POSSESSION\nEFFECTIVE_DISPOSAL\nPLY_PRESS_PTS\nTOTAL_GAINED_METRES\nTURNOVER\n\n\n\n\n1\nAdelaide Crows\n1\n-16\n33\n111\n242\n613.35\n6077.2\n56\n\n\n2\nAdelaide Crows\n2\n-32\n33\n139\n253\n601.2\n5950.1\n62\n\n\n3\nAdelaide Crows\n3\n31\n39\n146\n251\n577.8\n5882.4\n50\n\n\n4\nAdelaide Crows\n4\n39\n35\n151\n276\n592.8\n6096.5\n71\n\n\n5\nAdelaide Crows\n5\n56\n37\n156\n302\n626.25\n6373\n67\n\n\n6\nAdelaide Crows\n6\n3\n47\n145\n255\n686.1\n5902\n66\n\n\n\n\n\nLets run a multiple regression model\n\npacman::p_load(broom,equatiomatic,lme4,mgcv)\n\nmodel_lm &lt;- lm(SQUAD_MARGIN~CONTESTED_POSSESSION+EFFECTIVE_DISPOSAL+CLEARANCE+TOTAL_GAINED_METRES+TURNOVER,data = wide_dt)\n\nmodel_lmer &lt;- lmer(SQUAD_MARGIN~CONTESTED_POSSESSION+EFFECTIVE_DISPOSAL+CLEARANCE+TOTAL_GAINED_METRES+TURNOVER+\n  (CONTESTED_POSSESSION+CLEARANCE||SQUAD_NAME),data = wide_dt)\n\nboundary (singular) fit: see help('isSingular')\n\nwide_dt$SQUAD_NAME &lt;-as.factor(wide_dt$SQUAD_NAME)\n\nmodel_gam &lt;- mgcv::gam(SQUAD_MARGIN~s(CONTESTED_POSSESSION)+s(EFFECTIVE_DISPOSAL)+\n                      s(TURNOVER)+s(TOTAL_GAINED_METRES)+s(CLEARANCE)+\n                      s(SQUAD_NAME,bs=\"re\"),data = wide_dt)\n\n#equatiomatic::extract_eq(first_model)\n\n#equatiomatic::extract_eq(first_model, use_coefs = TRUE)\n\nBelow is a summary of the output\n\nbroom::tidy(model_lm,conf.int = T,conf.level = .95)|&gt;\n       mutate_if(is.numeric, round, 2)|&gt;\n  htmlTable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n1\n(Intercept)\n-326.69\n19.66\n-16.62\n0\n-365.33\n-288.06\n\n\n2\nCONTESTED_POSSESSION\n0.44\n0.12\n3.68\n0\n0.21\n0.68\n\n\n3\nEFFECTIVE_DISPOSAL\n0.27\n0.05\n5.62\n0\n0.17\n0.36\n\n\n4\nCLEARANCE\n0.85\n0.23\n3.62\n0\n0.39\n1.31\n\n\n5\nTOTAL_GAINED_METRES\n0.05\n0\n14.25\n0\n0.04\n0.05\n\n\n6\nTURNOVER\n-1.49\n0.15\n-10.2\n0\n-1.77\n-1.2\n\n\n\n\n\nMaybe we want to visualize the above\n\nbroom::tidy(model_lm,conf.int = T,conf.level = .95)|&gt;\n       mutate_if(is.numeric, round, 2)|&gt;\n       filter(term%in%c(\"TURNOVER\",\"TOTAL_GAINED_METRES\",\"CLEARANCE\",\n                        \"EFFECTIVE_DISPOSAL\",\"CONTESTED_POSSESSION\"))|&gt;\n       ggplot(aes(estimate,term,xmin=conf.low,xmax=conf.high))+\n       geom_pointrange()+\n       theme_bw()\n\n\n\n\nNow there are many issues with this first figure. Firstly, as they are all on different scales the figure above could potentially exaggerate the impact of turnover when compared to other variables such as metres gained. Also the p values and subsequent confidence intervals are incorrect as we have violated a couple of statistical assumptions with just using a basic linear model.\nPartial dependency plots offer a better way of assessing the impact of a predictor. Effectively, you can think of it as the predicted impact of changing a variable whilst holding other variables constant.\n\npacman::p_load(pdp)\npartial(model_gam, pred.var = c(\"TURNOVER\"), \n        plot = TRUE,plot.engine = \"ggplot\",ice=T,)+\n       geom_hline(yintercept = 0)+\n       theme_bw()\n\n\n\n\nWhat about metres gained\n\npartial(model_gam, pred.var = c(\"TOTAL_GAINED_METRES\"), \n        plot = TRUE,plot.engine = \"ggplot\",ice=T,)+\n       geom_hline(yintercept = 0)+\n       theme_bw()\n\n\n\n\n\npartial(model_gam, pred.var = c(\"TOTAL_GAINED_METRES\",\"TURNOVER\"), \n        plot = TRUE,\n        plot.engine = \"ggplot\",\n        chull = T)+\n       geom_hline(yintercept = 0)+\n       theme_bw()\n\n\n\n\nLets have a look at how well our model performed\n\n## calculate predicted model values from data obtained along with some other summary #stats\nwide_dt &lt;- wide_dt|&gt;\n           mutate(predicted = predict(model_lm),\n                  error = SQUAD_MARGIN - predicted,\n                  absError = abs(error),\n                  squError = error^2)\n\nLets have a look at how well or model can do at predicting SQUAD_MARGIN\n\n ggplot(wide_dt,aes(SQUAD_MARGIN,predicted))+\n   geom_point()+\n#   geom_smooth(method = \"lm\",se=F)+\n   geom_abline(intercept = 0,slope = 1,col=\"red\",linetype=\"dashed\")+\n   theme_bw()\n\n\n\n\nOkay, visually the performance looks pretty bad, ideally most of the dots would fit along the red line. It looks like the model is struggling to pick up the magnitude of wins and loses correctly. Lets numerically summarize this.\n\n## mean absolute error\nround(mean(wide_dt$absError),1)\n\n[1] 21.1\n\n## Root mean squred error\nround(sqrt(mean(wide_dt$squError)),1)\n\n[1] 26.7\n\n\nOkay, so we can say that using a simple multiple regression model on just mid and mid fwd data across the variables we looked at is not doing a great job at predicting SQUAD_MARGIN."
  },
  {
    "objectID": "module1.html#practice-exercises",
    "href": "module1.html#practice-exercises",
    "title": "2  Module 1",
    "section": "2.5 Practice exercises",
    "text": "2.5 Practice exercises\n\nDownload R and Quarto https://quarto.org/docs/download/ if you haven’t already.\nLoad some data into R and attempt some basic data manipulation\nAttempt to build a basic plot\nStart to think about questions you might have regarding data you have access to.\nLook to annoy Isaac atleast once over the next two weeks with a problem you might be having."
  },
  {
    "objectID": "module1.html#additional-resources",
    "href": "module1.html#additional-resources",
    "title": "2  Module 1",
    "section": "2.6 Additional resources",
    "text": "2.6 Additional resources\nplotting in R using GGPLOT: https://ggplot2-book.org/\nFree R for data science resource: https://r4ds.had.co.nz/introduction.html\nFree ISLR resource : https://www.statlearning.com/\nWhilst, we wont use tidymodels too much I would encourage you to be across it https://www.tidymodels.org/start/ as it is a very powerful modular way for machine learning in R.\nyoutube:\nhttps://www.youtube.com/@TidyX_screencast\nhttps://www.youtube.com/@JuliaSilge/videos\nand of course CHATGPT.\nI will continue to update this as I remember more of the resources I have come across."
  },
  {
    "objectID": "module2.html#recap-from-previous-week",
    "href": "module2.html#recap-from-previous-week",
    "title": "3  Module 2",
    "section": "3.1 Recap from previous week",
    "text": "3.1 Recap from previous week\nLets load some data first. This time to make it easier lets see if you can load data from my github.\nOur current outline for the next couple of weeks looks like the below.\n\nIntroduction into R programming language and getting started with some descriptive statistics. (last week)\nUnderstanding key terms; Exploratory analysis, Supervised vs Unsupervised problems, regression vs classification, prediction vs association.\nA quick note on distributions - Supervised regression and classification problems using a generalized linear model framework. (Hypothesis testing) (Next module)\n\n\n3.1.1 Loading packages and data\nWe will load some data that we used from the previous session.\n\npacman::p_load(data.table,tidyverse,htmlTable,factoextra,cluster,GGally)\n\n\nurlfile=\"https://raw.githubusercontent.com/R2mu/GWS_DSPR/main/data/mod2data.csv\"\n\ndata1 &lt;- fread(urlfile)\n\nLets just remind ourselves of the dataset we are using\n\nstr(data1)\n\nClasses 'data.table' and 'data.frame':  9936 obs. of  44 variables:\n $ SEASON_ID                : int  2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 ...\n $ MATCH_ID                 : int  266569840 266569840 266569840 266569840 266569840 266569840 266569840 266569840 266569840 266569840 ...\n $ GROUP_ROUND_NO           : int  1 1 1 1 1 1 1 1 1 1 ...\n $ VENUE_NAME               : chr  \"MCG\" \"MCG\" \"MCG\" \"MCG\" ...\n $ PERSON_ID                : int  250395 270146 270896 280819 290627 290847 293813 294036 294592 294674 ...\n $ FULLNAME                 : chr  \"Jack Riewoldt\" \"Ed Curnow\" \"Trent Cotchin\" \"Dylan Grimes\" ...\n $ SQUAD_NAME               : chr  \"Richmond\" \"Carlton\" \"Richmond\" \"Richmond\" ...\n $ Position                 : chr  \"Key Fwd\" \"Mid Fwd\" \"Mid Fwd\" \"Key Def\" ...\n $ OPP_SQUAD_NAME           : chr  \"Carlton\" \"Richmond\" \"Carlton\" \"Carlton\" ...\n $ SQUAD_MARGIN             : int  0 0 0 0 0 0 0 0 0 0 ...\n $ WL                       : int  0 0 0 0 0 0 0 0 0 0 ...\n $ WLD                      : int  0 0 0 0 0 0 0 0 0 0 ...\n $ BEHIND                   : int  0 0 0 0 1 0 3 0 0 1 ...\n $ CLEARANCE                : int  1 1 4 0 5 1 2 8 1 0 ...\n $ CONTESTED_MARK           : int  4 0 0 1 0 2 4 0 0 0 ...\n $ CONTESTED_POSSESSION     : int  10 2 11 4 13 10 9 14 4 2 ...\n $ CONTESTED_POSSESSION_POST: int  9 1 4 4 5 8 7 4 3 2 ...\n $ CONTESTED_POSSESSION_PRE : int  1 1 7 0 8 2 2 10 1 0 ...\n $ DISPOSAL                 : int  12 14 18 12 23 23 10 28 17 11 ...\n $ EFFECTIVE_DISPOSAL       : int  10 8 12 11 17 13 6 20 12 9 ...\n $ EFFECTIVE_HANDBALL       : int  3 2 7 4 10 6 1 14 4 2 ...\n $ EFFECTIVE_KICK           : int  7 6 5 7 7 7 5 6 8 7 ...\n $ GOAL                     : int  1 0 0 0 0 1 3 0 0 0 ...\n $ HANDBALL                 : int  3 2 9 4 13 8 1 18 6 2 ...\n $ HARD_BALL_GET            : int  0 1 3 1 1 4 2 3 0 1 ...\n $ HITOUT                   : int  1 0 0 0 0 0 1 0 0 0 ...\n $ IN50_KICK                : int  3 4 3 0 2 7 0 1 3 1 ...\n $ INSIDE_50                : int  4 4 3 0 2 8 1 2 3 2 ...\n $ INTERCEPT                : int  3 1 3 6 5 1 0 2 2 4 ...\n $ KICK                     : int  9 12 9 8 10 15 9 10 11 9 ...\n $ LONG_KICK                : int  0 1 3 2 4 3 1 0 3 3 ...\n $ MARK                     : int  6 6 3 5 4 6 6 5 5 4 ...\n $ MARK_ON_LEAD             : int  1 0 1 0 0 1 1 0 0 1 ...\n $ METRES_GAINED_EFF        : int  139 182 178 89 158 230 118 39 139 156 ...\n $ MISSED_TACKLE            : int  0 0 1 1 1 0 0 0 0 0 ...\n $ PLY_PRESS_PTS            : num  39.3 17.4 33.5 12 25.5 ...\n $ POINTS                   : int  6 0 0 0 1 6 21 0 0 1 ...\n $ RATING                   : num  18.2 3.9 8.2 6.4 7.9 13.7 13.2 12.5 2.3 4.2 ...\n $ SMOTHER                  : int  1 0 1 0 1 0 0 0 2 0 ...\n $ SPOIL                    : int  0 0 1 3 0 0 3 0 0 4 ...\n $ TACKLE                   : int  5 4 3 2 1 0 0 4 3 2 ...\n $ TOTAL_GAINED_METRES      : num  198 300 264 111 316 ...\n $ TURNOVER                 : int  3 5 5 1 4 7 2 2 3 1 ...\n $ UNCONTESTED_MARK         : int  2 6 3 4 4 4 2 5 5 4 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\nhead(data1)|&gt;htmlTable()\n\n\n\n\n\nSEASON_ID\nMATCH_ID\nGROUP_ROUND_NO\nVENUE_NAME\nPERSON_ID\nFULLNAME\nSQUAD_NAME\nPosition\nOPP_SQUAD_NAME\nSQUAD_MARGIN\nWL\nWLD\nBEHIND\nCLEARANCE\nCONTESTED_MARK\nCONTESTED_POSSESSION\nCONTESTED_POSSESSION_POST\nCONTESTED_POSSESSION_PRE\nDISPOSAL\nEFFECTIVE_DISPOSAL\nEFFECTIVE_HANDBALL\nEFFECTIVE_KICK\nGOAL\nHANDBALL\nHARD_BALL_GET\nHITOUT\nIN50_KICK\nINSIDE_50\nINTERCEPT\nKICK\nLONG_KICK\nMARK\nMARK_ON_LEAD\nMETRES_GAINED_EFF\nMISSED_TACKLE\nPLY_PRESS_PTS\nPOINTS\nRATING\nSMOTHER\nSPOIL\nTACKLE\nTOTAL_GAINED_METRES\nTURNOVER\nUNCONTESTED_MARK\n\n\n\n\n1\n2023\n266569840\n1\nMCG\n250395\nJack Riewoldt\nRichmond\nKey Fwd\nCarlton\n0\n0\n0\n0\n1\n4\n10\n9\n1\n12\n10\n3\n7\n1\n3\n0\n1\n3\n4\n3\n9\n0\n6\n1\n139\n0\n39.3\n6\n18.2\n1\n0\n5\n197.6\n3\n2\n\n\n2\n2023\n266569840\n1\nMCG\n270146\nEd Curnow\nCarlton\nMid Fwd\nRichmond\n0\n0\n0\n0\n1\n0\n2\n1\n1\n14\n8\n2\n6\n0\n2\n1\n0\n4\n4\n1\n12\n1\n6\n0\n182\n0\n17.4\n0\n3.9\n0\n0\n4\n300.3\n5\n6\n\n\n3\n2023\n266569840\n1\nMCG\n270896\nTrent Cotchin\nRichmond\nMid Fwd\nCarlton\n0\n0\n0\n0\n4\n0\n11\n4\n7\n18\n12\n7\n5\n0\n9\n3\n0\n3\n3\n3\n9\n3\n3\n1\n178\n1\n33.45\n0\n8.2\n1\n1\n3\n263.7\n5\n3\n\n\n4\n2023\n266569840\n1\nMCG\n280819\nDylan Grimes\nRichmond\nKey Def\nCarlton\n0\n0\n0\n0\n0\n1\n4\n4\n0\n12\n11\n4\n7\n0\n4\n1\n0\n0\n0\n6\n8\n2\n5\n0\n89\n1\n12\n0\n6.4\n0\n3\n2\n111\n1\n4\n\n\n5\n2023\n266569840\n1\nMCG\n290627\nDion Prestia\nRichmond\nMid\nCarlton\n0\n0\n0\n1\n5\n0\n13\n5\n8\n23\n17\n10\n7\n0\n13\n1\n0\n2\n2\n5\n10\n4\n4\n0\n158\n1\n25.5\n1\n7.9\n1\n0\n1\n316.3\n4\n4\n\n\n6\n2023\n266569840\n1\nMCG\n290847\nDustin Martin\nRichmond\nGen Fwd\nCarlton\n0\n0\n0\n0\n1\n2\n10\n8\n2\n23\n13\n6\n7\n1\n8\n4\n0\n7\n8\n1\n15\n3\n6\n1\n230\n0\n25.5\n6\n13.7\n0\n0\n0\n432.1\n7\n4\n\n\n\n\n\n\n\n3.1.2 Quick summary\nLets do some quick summaries in a similar fashion to what we did last time. To start of with I am going to have a look at some position summaries by win vs loss across key statistics.\nFirstly to make the analysis a little simpler I am going to make the dataframe into what is called a long format. Now to be fair it is possible to summarize keeping the data in its current wide format but being confident in manipulating data.frames from wide to long and vice versa is very useful for data analysis in general.\nBefore I do that however, lets just check something\n\nunique(data1$Position)\n\n[1] \"Key Fwd\" \"Mid Fwd\" \"Key Def\" \"Mid\"     \"Gen Fwd\" \"Wing\"    \"Gen Def\"\n[8] \"Ruck\"    \"\"       \n\n\nYou may notice a “” is returned this represents a blank value, which ideally there should not be any. Lets have a look a little deeper\n\ndata1|&gt;\n  filter(Position==\"\")|&gt;\n  select(FULLNAME,PERSON_ID,GROUP_ROUND_NO)\n\n        FULLNAME PERSON_ID GROUP_ROUND_NO\n 1: Willie Rioli    296225              2\n 2: Willie Rioli    296225              5\n 3: Willie Rioli    296225              7\n 4: Willie Rioli    296225              8\n 5: Willie Rioli    296225             11\n 6: Willie Rioli    296225             13\n 7: Willie Rioli    296225             19\n 8: Willie Rioli    296225             21\n 9: Willie Rioli    296225             22\n10: Willie Rioli    296225             23\n11: Willie Rioli    296225             24\n12: Willie Rioli    296225             25\n13: Willie Rioli    296225             26\n\n\nOkay this is because he was referred to as “Junior Rioli” that year which means ideally I need to fix the initial join i did from the previous week to just be PERSON_ID and SEASON_ID and not include FULLNAME.\nI know however, that he played as “Gen Fwd” that year so lets quickly update that in our database.\n\ndata1 &lt;- data1|&gt;\n         mutate(Position=ifelse(SEASON_ID==2023&PERSON_ID==296225,\n                                \"Gen Fwd\",Position))\n\nWe can check out work with\n\nunique(data1$Position)\n\n[1] \"Key Fwd\" \"Mid Fwd\" \"Key Def\" \"Mid\"     \"Gen Fwd\" \"Wing\"    \"Gen Def\"\n[8] \"Ruck\"   \n\n\nOkay lets now look to melt the data and drop some columns for the sake of it.\n\ndata.frame(names(data1))\n\n                names.data1.\n1                  SEASON_ID\n2                   MATCH_ID\n3             GROUP_ROUND_NO\n4                 VENUE_NAME\n5                  PERSON_ID\n6                   FULLNAME\n7                 SQUAD_NAME\n8                   Position\n9             OPP_SQUAD_NAME\n10              SQUAD_MARGIN\n11                        WL\n12                       WLD\n13                    BEHIND\n14                 CLEARANCE\n15            CONTESTED_MARK\n16      CONTESTED_POSSESSION\n17 CONTESTED_POSSESSION_POST\n18  CONTESTED_POSSESSION_PRE\n19                  DISPOSAL\n20        EFFECTIVE_DISPOSAL\n21        EFFECTIVE_HANDBALL\n22            EFFECTIVE_KICK\n23                      GOAL\n24                  HANDBALL\n25             HARD_BALL_GET\n26                    HITOUT\n27                 IN50_KICK\n28                 INSIDE_50\n29                 INTERCEPT\n30                      KICK\n31                 LONG_KICK\n32                      MARK\n33              MARK_ON_LEAD\n34         METRES_GAINED_EFF\n35             MISSED_TACKLE\n36             PLY_PRESS_PTS\n37                    POINTS\n38                    RATING\n39                   SMOTHER\n40                     SPOIL\n41                    TACKLE\n42       TOTAL_GAINED_METRES\n43                  TURNOVER\n44          UNCONTESTED_MARK\n\ndtLong = data1|&gt;\n        melt(id.vars = 1:12)|&gt;\n  ## going to remove some columns \n  filter(!variable %in%c(\"POINTS\",\"GOAL\",\"BEHIND\"))\n\nLets now create a quick summary of the stats. Now to reiterate what I mentioned last week, this is where R can be really useful. Effectively, we need to create multiple levels of aggregation to get to the level we want and in excel for example this would require multiple pivot tables.\n\ndtlongSum  &lt;- dtLong|&gt;\n  group_by(GROUP_ROUND_NO,SQUAD_NAME,Position,variable,WL)|&gt;\n  summarise(sum = round(sum(value,na.rm = T),2))|&gt;\n  # the above effectively summarised to the position level\n  ungroup()|&gt;\n  group_by(Position, variable,WL)|&gt;\n  summarise(mu = round(mean(sum),2),\n            sd = round(sd(sum),2))\n\n`summarise()` has grouped output by 'GROUP_ROUND_NO', 'SQUAD_NAME', 'Position',\n'variable'. You can override using the `.groups` argument.\n`summarise()` has grouped output by 'Position', 'variable'. You can override\nusing the `.groups` argument.\n\n # The above then averages the statistics via WL for each position over the season so effectively removes GROUP level data\n\n\n## As I was building it I would use the below as an example of checking as I go  \n#filter(SQUAD_NAME==\"Adelaide Crows\"&variable==\"CLEARANCE\"&Position==\"Mid\")\n\nhead(dtlongSum)|&gt;htmlTable()\n\n\n\n\n\nPosition\nvariable\nWL\nmu\nsd\n\n\n\n\n1\nGen Def\nCLEARANCE\n0\n3.53\n2.47\n\n\n2\nGen Def\nCLEARANCE\n1\n3.95\n2.72\n\n\n3\nGen Def\nCONTESTED_MARK\n0\n1.6\n1.49\n\n\n4\nGen Def\nCONTESTED_MARK\n1\n1.57\n1.26\n\n\n5\nGen Def\nCONTESTED_POSSESSION\n0\n24.42\n7.5\n\n\n6\nGen Def\nCONTESTED_POSSESSION\n1\n24.28\n7.26\n\n\n\n\n\nThe above is still quite a long data.frame with 464 rows which might not be the easiest way to see the differences so lets re shape it again to see if it helps.\n\ndcast.data.table(setDT(dtlongSum),Position+variable~WL,value.var = \"mu\")|&gt;\n  filter(variable==\"DISPOSAL\")|&gt;\n  mutate(difference = round(`1`-`0`,1))|&gt;\n  htmlTable()\n\n\n\n\n\nPosition\nvariable\n0\n1\ndifference\n\n\n\n\n1\nGen Def\nDISPOSAL\n89.28\n89.18\n-0.1\n\n\n2\nGen Fwd\nDISPOSAL\n51.44\n55.79\n4.4\n\n\n3\nKey Def\nDISPOSAL\n30.98\n34.39\n3.4\n\n\n4\nKey Fwd\nDISPOSAL\n23.05\n26.11\n3.1\n\n\n5\nMid\nDISPOSAL\n83.1\n87.57\n4.5\n\n\n6\nMid Fwd\nDISPOSAL\n28.22\n30.66\n2.4\n\n\n7\nRuck\nDISPOSAL\n17.06\n17.94\n0.9\n\n\n8\nWing\nDISPOSAL\n38.56\n38.76\n0.2\n\n\n\n\n\nMaybe we want to plot the differences\n\ndtLong|&gt;\n  group_by(MATCH_ID,SQUAD_NAME,WL,Position,variable)|&gt;\n  summarise(sum = sum(value))|&gt;\n  filter(variable%in%c(\"DISPOSAL\",\"CLEARANCE\"))|&gt;\n  #filter(Position%in%c(\"Mid\",\"Mid Fwd\",\"Wing\"))|&gt;\n  ggplot(aes(as.factor(WL),sum))+\n  geom_jitter(col=\"gray80\")+\n  geom_boxplot()+\n  stat_summary(fun.data = \"mean_sdl\",\n               geom = \"pointrange\",\n               fun.args = list(mult=1),col=\"black\")+\n  facet_grid(vars(variable), vars(Position),scales = \"free_y\")+\n  theme_bw()\n\n`summarise()` has grouped output by 'MATCH_ID', 'SQUAD_NAME', 'WL', 'Position'.\nYou can override using the `.groups` argument.\n\n\n\n\n\n\n\n3.1.3 Important note\nYou guys are the experts here in being able to double check what numbers to expect. I want to strongly reiterate how much of coding is just checking your work as you go, are you getting numbers you expect? As I mentioned the beauty of R being modular is it easily allows for you to continuously check as you build.\n\n\n3.1.4 Final exploration\nLets have a look at the global relationships across a range of variables.\n\nsum2 = dtLong|&gt;\n  group_by(GROUP_ROUND_NO,SQUAD_NAME,variable)|&gt;\n  summarise(sum = round(sum(value),2))|&gt;\n  pivot_wider(values_from = sum,\n              names_from = variable)|&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'GROUP_ROUND_NO', 'SQUAD_NAME'. You can\noverride using the `.groups` argument.\n\nsum2|&gt;head()|&gt;htmlTable()\n\n\n\n\n\nGROUP_ROUND_NO\nSQUAD_NAME\nCLEARANCE\nCONTESTED_MARK\nCONTESTED_POSSESSION\nCONTESTED_POSSESSION_POST\nCONTESTED_POSSESSION_PRE\nDISPOSAL\nEFFECTIVE_DISPOSAL\nEFFECTIVE_HANDBALL\nEFFECTIVE_KICK\nHANDBALL\nHARD_BALL_GET\nHITOUT\nIN50_KICK\nINSIDE_50\nINTERCEPT\nKICK\nLONG_KICK\nMARK\nMARK_ON_LEAD\nMETRES_GAINED_EFF\nMISSED_TACKLE\nPLY_PRESS_PTS\nRATING\nSMOTHER\nSPOIL\nTACKLE\nTOTAL_GAINED_METRES\nTURNOVER\nUNCONTESTED_MARK\n\n\n\n\n1\n1\nAdelaide Crows\n33\n9\n111\n77\n34\n318\n242\n89\n153\n101\n26\n40\n39\n52\n62\n217\n55\n102\n6\n3992\n4\n613.35\n178.6\n6\n30\n40\n6077.2\n56\n93\n\n\n2\n1\nBrisbane Lions\n38\n4\n116\n55\n61\n264\n181\n72\n109\n102\n33\n41\n35\n40\n52\n162\n62\n52\n6\n3879\n10\n650.4\n161.2\n7\n37\n46\n5309.8\n66\n48\n\n\n3\n1\nCarlton\n32\n18\n148\n100\n48\n341\n241\n92\n149\n120\n32\n26\n33\n45\n79\n221\n67\n97\n0\n4119\n10\n580.65\n186.2\n8\n29\n55\n5963.4\n75\n79\n\n\n4\n1\nCollingwood\n43\n10\n135\n79\n56\n372\n286\n120\n166\n143\n18\n45\n52\n62\n62\n229\n62\n105\n7\n4611\n8\n594.45\n250.5\n8\n32\n52\n6293\n61\n95\n\n\n5\n1\nEssendon\n32\n11\n128\n83\n45\n414\n322\n142\n180\n159\n28\n20\n54\n66\n63\n255\n45\n131\n6\n4607\n6\n521.25\n264.1\n4\n30\n37\n6993.8\n59\n120\n\n\n6\n1\nFremantle\n28\n14\n146\n102\n44\n437\n334\n145\n189\n179\n30\n45\n56\n65\n92\n258\n63\n136\n6\n4615\n4\n633.3\n200.2\n11\n24\n53\n6581.1\n82\n122\n\n\n\n\n\n\ncols &lt;- c(\"METRES_GAINED_EFF\", \"PLY_PRESS_PTS\",\"KICK\",\"HANDBALL\",\"MARK\",\"CONTESTED_POSSESSION\",\"TACKLE\")\n\nforPairs &lt;- sum2|&gt;\n           select(!1:2)|&gt;\n           select(cols)\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(cols)\n\n  # Now:\n  data %&gt;% select(all_of(cols))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\nggpairs(forPairs)+theme_light()"
  },
  {
    "objectID": "module2.html#exploratory-and-descriptive-analysis",
    "href": "module2.html#exploratory-and-descriptive-analysis",
    "title": "3  Module 2",
    "section": "3.2 Exploratory and Descriptive analysis",
    "text": "3.2 Exploratory and Descriptive analysis\nThe above is an example of both what we would consider EXPLORATORY and DESCRIPTIVE analysis. These analysis techniques are usually first steps we take when analysing data.\nExploratory analysis can be really useful for thing such as\n\nIdentifying outliers or anomalies in the data (very important)\nObserve if the problem is tractable/solvable (very important)\nAssess distributional assumptions of the data (more on this next week)\nVisual assessment of relationships or patterns that may exist within the data\n\nWhere as descriptive analysis refers to summarizing the main variables of interest. You are not necessarily looking to infer a relationship or hypothesis per se but just are reporting relevant summary statistics e.g. mean, max, min, range, standard deviation, median, sum, proportion,count relating to a question at hand.\n” The average disposal count for mid fielders was ….. for 2023 and …. for 2024.”\n” The proportion of time player x played up forward was …. vs ….. played down back”"
  },
  {
    "objectID": "module2.html#a-quick-disclaimer-regarding-exploratory-analysis",
    "href": "module2.html#a-quick-disclaimer-regarding-exploratory-analysis",
    "title": "3  Module 2",
    "section": "3.3 A quick disclaimer regarding exploratory analysis",
    "text": "3.3 A quick disclaimer regarding exploratory analysis\nWhile exploratory analysis is a powerful tool that is used almost subconsciously in our daily lives, it is important to exercise caution and maintain mental error control. The human brain is notoriously good at finding and justifying relationships in data that may genuinely just be noise. To highlight this problem, an additional chapter called “Multiple testing” was added to the second version of the ISLR book i recommend the previous module.\nHypothesis testing is one type of tool that aims to help reduce the risk of being misled by noise. Ultimately, however, you bear a significant responsibility to develop a hypothesis or causal framework that justifies why you think the observed trend may indeed be valid.\nWhilst, developing causal models can quickly become an extensive process a simply starting point is developing a Directed Acyclic Graph (DAG). A DAG is a graphical representation of a hypothetical model outlining causal effects. Outlined below is a quick example of how one may develop a dag.\nWe want to model the relationships between the following variables:\n\nParental Education (PE)\nStudent’s Motivation (SM)\nTime Spent Studying (TS)\nAttendance (A)\nAcademic Performance (AP)\n\nBelow we are going graphical represent relationships that we believe to impact these variables.\n\n\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n\n\n\n\n\n\n3.3.1 DAG Structure:\nFor the above figure we would interpret it like so:\n\nParental Education (PE) affects both Student’s Motivation (SM) and Time Spent Studying (TS).\nStudent’s Motivation (SM) influences Time Spent Studying (TS).\nTime Spent Studying (TS) and Attendance (A) both affect Academic Performance (AP).\nParental Education (PE) also directly affects Academic Performance (AP).\n\nWhile this may initially seem trivial, a well-thought-out DAG can help identify situations where causal analysis approaches can be explored using observational data—an area long thought to be exclusive to randomized controlled trials. See this paper by Dr Judd Kalkhoven an academic from WSU who gives some examples of causal modelling from an athlete injury perspective https://link.springer.com/article/10.1007/s40279-024-02008-1."
  },
  {
    "objectID": "module2.html#supervised-vs-unsupervised-problems",
    "href": "module2.html#supervised-vs-unsupervised-problems",
    "title": "3  Module 2",
    "section": "3.4 Supervised vs Unsupervised problems",
    "text": "3.4 Supervised vs Unsupervised problems\n\n3.4.1 Supervised Learning Overview\nIn our previous session, we explored our ability to predict score differential using a range of statistics. The simple linear model we examined was an example of a supervised regression problem. The easiest way to identify whether something can be thought of as a supervised problem is by determining whether you have a clear outcome of interest that you want to predict. If the answer is yes, then it is a supervised problem.\n\n\n3.4.2 Types of Supervised Problems\nIn reality, most of the problems/questions you encounter will relate to predicting some quantity, whether it be an absolute number such as score differential (a regression problem), win vs. loss probability (a classification problem), or predicting the next action type, e.g., kick, handball, tackle (a multinomial classification problem).\n\nPrediction of a number = Regression problem\nPrediction of a class/category = Classification problem\n\n\n\n3.4.3 Choosing a Model\nThere are often numerous models to answer the same regression or classification problem, and there will usually be trade-offs between certain models that you need to explore. A useful first step is asking yourself, “Is it important that I understand how the model made the prediction it did, or is it just important that the model can predict accurately?” This question can help you narrow down the scope of models you may use.\n\nThe above picture is taken from ISLR book I recommended (https://www.statlearning.com/) and it highlights some of these tradeoffs. Over the coming weeks we will explore models that exist along this continuum but I want you to also think of the x axis as dataset size required."
  },
  {
    "objectID": "module2.html#unsupervised-problems",
    "href": "module2.html#unsupervised-problems",
    "title": "3  Module 2",
    "section": "3.5 Unsupervised Problems",
    "text": "3.5 Unsupervised Problems\nLet’s say, for example, that you don’t have a specific dependent/outcome variable that you are interested in predicting. Instead, you are more interested in exploring whether there are any interesting patterns, clusters, or groupings that exist within the data. You are not too concerned with how they are structured, just whether there is a structure. This would be considered an unsupervised learning problem.\n\n3.5.1 Common Algorithms for Unsupervised Learning\n\nClustering Algorithms:\n\nAim to find multidimensional similarities between variables, grouping data points into clusters based on their features.\nExamples: K-means clustering, hierarchical clustering, DBSCAN, GMM.\n\nPrincipal Component Analysis (PCA):\n\nAims to find components that can best explain the variance between variables, reducing the dimensionality of the data while retaining most of the variance.\nUseful for data visualization and simplifying complex datasets.\n\nAssociation Rules / Market Basket Analysis:\n\nAim to find associations between items or transactions, identifying how frequently certain items or events occur together.\nExample: Market basket analysis can reveal that customers who buy bread often buy butter as well.\n\n\n\n\n3.5.2 Summary\nUnsupervised learning is about discovering the inherent structure within your data without predefined labels or outcomes. It’s particularly useful when you want to explore and understand the underlying patterns or relationships in your dataset.\nBelow is a quick example where we are using a variant of the kmeans clustering algorithm.\n\n## select some columns to analyse. I do suggest to do select only variables you are interested in. e.g. don;t just put everything in and hope for the best.\n\ncols &lt;- c(\"METRES_GAINED_EFF\", \"PLY_PRESS_PTS\",\"KICK\",\"HANDBALL\",\"MARK\",\"CONTESTED_POSSESSION\",\"count\")\n\n### create a quick summary of all players from 2023\n\nsmalldf &lt;- data1|&gt;\n  group_by(FULLNAME,Position)|&gt;\n  mutate(count = length(FULLNAME))|&gt;\n  group_by(FULLNAME,Position)|&gt;\n  ## the below is a way where you can summarise in the wide format\n  summarise(across(all_of(cols), mean))|&gt;\n  ungroup()|&gt;\n  ## going to remove players with less than 5 games\n  filter(!count&lt;=5)|&gt;\n  # dont need the count column after so going to remove\n  select(!count)\n\n`summarise()` has grouped output by 'FULLNAME'. You can override using the\n`.groups` argument.\n\n## outside the scope of but we need to remove players name from being a column variable to being a row now. \nsmallerdf &lt;- smalldf|&gt;\n             column_to_rownames(\"FULLNAME\")|&gt;\n             select(!Position)\n\n## We are going to compute a slightly more complicated kmeans algo \n# the reasoning is a little complicated but it just better matches our data. \n\n### compute gowers distance. \n# this calcualtes how far away each player is from each other across all statistics and represents as one number. \ndissimilarity_matrix &lt;- daisy(smallerdf,\n                               metric = \"gower\")\n\n## convert to a matrix\ngower_mat &lt;- as.matrix(dissimilarity_matrix)\n\n\n### find number of clusters within group. I know ahead of time that 3 is an okay number but usually you may need to explore a number of cluster sizes. \n\n\n## below is the clustering algo we will use\nset.seed(123) # For reproducibility\npam_result &lt;- pam(gower_mat, k = 3)\n\n## join computed cluster back on the inital dataframe\nsmalldf &lt;- smalldf|&gt;mutate(cluster = pam_result$clustering)\n\n## lets plot the results \nfviz_cluster(pam_result,geom = \"text\",\n               ellipse.type = \"convex\",\n               show.clust.cent = TRUE,\n               ggtheme = theme_classic(),\n               labelsize = 8)\n\n\n\n\nLets get a feel for what differentiates the clusters.\n\nclusterLong =setDT(smalldf)|&gt;melt.data.table(id.vars = c(\"FULLNAME\",\"Position\",\"cluster\"))\n\n\nggplot(clusterLong,aes(value,as.factor(cluster)))+\n  geom_jitter(aes(col=Position))+\n  stat_summary(fun.data = \"mean_sdl\",\n               geom = \"crossbar\",\n               fun.args = list(mult=1),col=\"black\")+\n  facet_wrap(~variable,scales = \"free\",ncol = 3)+\n  theme_light()+\n  theme(legend.position = \"top\")\n\n\n\n\nFinally, a question you may have from this which players are most similar to another player in a multidimensional case.\nFirstly, I need to create a helper function to do this. I know this may seem scary but I was able to do this within a couple of minutes of searching on google and using chatGPT.\n\n# You don;t need to necessarily understand what this function does but it allows us to find the most similar players based off the multidimensioal calculation we did before.\n\nfind_most_similar &lt;- function(df, gower_matrix, individual, n = 1) {\n  # Find the index of the selected individual\n  individual_index &lt;- which(df$FULLNAME == individual)\n  \n  # Get the distances for the selected individual\n  distances &lt;- gower_matrix[individual_index, ]\n  \n  # Set the distance to itself as Inf to exclude it from the nearest neighbors\n  distances[individual_index] &lt;- Inf\n  \n  # Find the indices of the n smallest distances\n  nearest_indices &lt;- order(distances)[1:n]\n  \n  # Include the selected individual in the result\n  all_indices &lt;- c(individual_index, nearest_indices)\n  \n  # Return the rows of the selected individual and nearest individuals\n  return(df[all_indices, ])\n}\n\nRun the helper function\n\nfind_most_similar(smalldf, gower_mat, 'Toby Greene', n = 3)|&gt;\n  mutate(across(where(is.numeric), round, 1))|&gt;htmlTable()\n\n\n\n\n\nFULLNAME\nPosition\nMETRES_GAINED_EFF\nPLY_PRESS_PTS\nKICK\nHANDBALL\nMARK\nCONTESTED_POSSESSION\ncluster\n\n\n\n\n1\nToby Greene\nGen Fwd\n217.6\n30.4\n12\n5.7\n4.4\n7.7\n2\n\n\n2\nChristian Salem\nGen Def\n212.2\n31.6\n12.5\n6.6\n3.9\n5.8\n2\n\n\n3\nSteele Sidebottom\nWing\n227.2\n32.8\n12.8\n8\n4.2\n6.9\n2\n\n\n4\nBen Ainsworth\nGen Fwd\n203\n26.9\n10.9\n6.8\n4.8\n6.5\n2\n\n\n\n\n\nAlways good to cross reference if you think these results make sense.\nWe will talk about some other use cases of unsupervised problems in later modules."
  },
  {
    "objectID": "module2.html#practice-exercises-module-2",
    "href": "module2.html#practice-exercises-module-2",
    "title": "3  Module 2",
    "section": "3.6 Practice exercises module 2",
    "text": "3.6 Practice exercises module 2\n\nThink about questions you may have from datasets you have used. What would you classify your question as? e.g. ( supervised, unsupervised, regression or classification)\nCould you defend or represent your problem as a DAG?\nCan you think of any questions that might be appropriate for an unsupervised learning problem?"
  },
  {
    "objectID": "module2.html#resources.",
    "href": "module2.html#resources.",
    "title": "3  Module 2",
    "section": "3.7 Resources.",
    "text": "3.7 Resources.\nISLR https://www.statlearning.com/"
  },
  {
    "objectID": "module3.html#recap-from-last-week",
    "href": "module3.html#recap-from-last-week",
    "title": "4  Module 3",
    "section": "4.1 Recap from last week",
    "text": "4.1 Recap from last week\n\nWhat is the difference between supervised and unsupervised learning?\nIf I was to try predict a category or a class, what type of prediction problem would this be considered?\nWhat are the benefits of developing a mini DAG or casual structure?"
  },
  {
    "objectID": "module3.html#goals-for-this-week.",
    "href": "module3.html#goals-for-this-week.",
    "title": "4  Module 3",
    "section": "4.2 Goals for this week.",
    "text": "4.2 Goals for this week.\n\nIntroduction into R programming language and getting started with some descriptive statistics.\nUnderstanding key terms; Exploratory analysis, Supervised vs Unsupervised problems, regression vs classification, prediction vs association.\nA quick note on distributions - Supervised regression and classification problems using a generalized linear model framework. (Hypothesis testing)\nMachine learning approach to Supervised learning\n\nThe below is an exert from a book that I have stolen that I think does a great job a summarizing the goal of statistical modelling.\n“Almost every statistical analysis begins with some kind of statistical model. A statistical model generally takes the form of a probability distribution that attempts to quantify the uncertainty that comes with observing a new response. The model is intended to represent the unknown phenomenon that governs the observation process. At the same time, the model needs to be convenient to work with mathematically, so that inference procedures such as confidence intervals and hypothesis tests can be developed. Selecting a model is typically a compromise between two competing goals: providing a more detailed approximation to the process that generates the data and providing inference procedures that are easy to use.”\npg1, Analysis of Categorical data with R Christopher R. Bilder and Thomas M Loughin. 2015.\nThis book also seems to be freely available from a google search.\nhttp://ndl.ethernet.edu.et/bitstream/123456789/28010/1/Christopher%20R.%20Bilder_2015.pdf"
  },
  {
    "objectID": "module3.html#load-data-and-packages",
    "href": "module3.html#load-data-and-packages",
    "title": "4  Module 3",
    "section": "4.3 Load data and packages",
    "text": "4.3 Load data and packages\n\npacman::p_load(data.table,tidyverse,htmlTable,ggforce,ggpubr,ggeffects,lme4,emmeans,mgcv,performance,nnet)\n\n\nurlfile=\"https://raw.githubusercontent.com/R2mu/GWS_DSPR/main/data/mod2data.csv\"\n\ndata1 &lt;- fread(urlfile)\n\nurlfile2=\"https://raw.githubusercontent.com/R2mu/GWS_DSPR/main/data/shotdata.csv\"\n\ndata2 &lt;- fread(urlfile2)\ndata2 &lt;- data2|&gt;\n  rename(Rotated.xStd=shots.details.locationRotated.xStd,\n         Rotated.yStd=shots.details.locationRotated.yStd)"
  },
  {
    "objectID": "module3.html#basic-probability-and-distributions",
    "href": "module3.html#basic-probability-and-distributions",
    "title": "4  Module 3",
    "section": "4.4 Basic probability and distributions",
    "text": "4.4 Basic probability and distributions\nWe are going to explore the idea of modelling a probability distribution today. Now there are many different varieties of probability distributions that exist, but we will just explore a couple of them that I from my experience will cover the largest chunk of questions you may be interested in asking in sport.\n1) The normal distribution, often referred to as the bell curve, is appropriate for many situations involving continuous data. It is characterized by its symmetric shape, where most of the observations cluster around the mean, and the probabilities for values taper off equally on both sides. This distribution is particularly suitable for variables such as score differential, meters gained, and other player statistics with high means.\n\n\n\n\n\n2) Binomial Distribution: The binomial distribution is ideal for modeling the number of successes in a fixed number of independent trials, where each trial has two possible outcomes (success or failure) and a constant probability of success. It is particularly useful for scenarios where you are counting the number of times an event occurs, such as the number of goals scored by a player in a series of attempts, or the number of games won in a season. The binomial distribution is defined by two parameters: the number of trials (n) and the probability of success (p). It helps in situations where the outcomes are discrete and the trials are independent, allowing for the calculation of probabilities for different numbers of successes.\nLets say for example you wanted the model probability of Toby Greene kicking 4 goals from 5 attempts in a game.\n\nx = 4 # represents number of GOALS \nn = 5 # From 5 attempts \np = 0.57 # with a baseline probability per attempt of 0.57\nround(dbinom(x,n,p),2)\n\n[1] 0.23\n\n\nThis means that, under these conditions, there is a 23% chance that Toby will achieve 4 successful goals in 5 attempts.\nIf we wanted to plot all the outcomes e.g 1 from 5, 2 from 5 etc etc it would look like the below\n\nbinom_prob = dbinom(0:5,5,0.57)\n\nplot(0:5, binom_prob, type = \"h\", lwd = 2, col = \"blue\",\n     xlab = \"Number of Goals\", ylab = \"Probability\",\n     main = \"Binomial Distribution of number of goals from 5 attempts\")\npoints(0:5, binom_prob, pch = 16, col = \"blue\")\n\n\n\n\n3) The Poisson distribution: is appropriate for modelling the number of events occurring within a fixed interval of time or space, particularly when these events are rare or infrequent. It is often used for count data where the mean number of occurrences is low, such as the number of goals scored in a single match by a player, the number of customer arrivals in an hour, or the number of system failures in a day. The Poisson distribution is defined by a single parameter (λ), which represents both the mean and the variance of the distribution. In my experience this assumption regarding the mean and variance is often violated so other parameters can be introduced to the assumption to deal with it.\n\ng1= ggplot(data1,aes(TACKLE))+\n   geom_histogram(col=\"white\")\n\ng2 =ggplot(data1,aes(CONTESTED_POSSESSION))+\n   geom_histogram(col=\"white\",binwidth = 1)+\n  scale_x_continuous(limits = c(0,20))\n\nggarrange(g1,g2)"
  },
  {
    "objectID": "module3.html#generalized-linear-models",
    "href": "module3.html#generalized-linear-models",
    "title": "4  Module 3",
    "section": "4.5 Generalized Linear models",
    "text": "4.5 Generalized Linear models\n\n4.5.1 Gaussian example\nNow the we’ve had a look at some distributions we are going to have a look at some methods for modelling different distributions. In week 1 we modeled how well a simple linear regression model could predict score differential. Mathematically, a linear regression model can be thought of as saying a prediction of Score differential is Normally distributed with some mean \\(\\mu\\) and standard deviation \\(\\sigma\\) .\n\\(\\text{Score differential} \\sim \\text{Normal}(\\mu_i,\\sigma)\\)\nWhere the mean can be described by a linear equation\n\\(\\mu_i = \\alpha+\\beta_1 X_1+\\beta2 X_2\\).\nNow this might look a little scary but this effectively just y = mx+b that you have learnt in high school, except in this case \\(X_1\\) might represent inside 50s and \\(X_2\\) might represent disposals. Since we have two variables this is a multiple regression model.\nThe figure below hopefully helps make this idea that a prediction can be thought of as a sample from the normal distribution with a conditional effect based on the predictor variable\n\n\n\n4.5.2 Binomial example.\nLets go back to our binomial probability example we were talking about before. We might say something like the probability of scoring a goal is binomially distributed with some probability \\(p_i\\).\n\\(\\text{Goal success} \\sim \\text{Binomial}(1,p_i)\\)\nThe issue we have here is that \\(p_i\\) which represents the probability success, is obviously going to be impacted by things such as distance from goal, angle from goal and whether it was during play or a set shot. So what we are interested in doing is estimated how this probability \\(p_i\\) will change as a function of those variables, which we do in the same manner we estimated the mean in the linear regression model above.\n\\(logit(p_i) = \\alpha +\\beta_1 X_1+\\beta2 X_2\\)\nYou can think of \\(B_1\\) as maybe distance from goal and \\(B_2\\) as angle from goal possibly. The other thing you may notice is that the \\(p_1\\) has been wrapped in a logit function. This is one example of the super power of generalized linear models. Effectively, we are able to keep the same notation as we would for a simple linear model, but we transform this linear combination to make sure that the result stays between 0 and 1 or Miss and GOAL in our example. This logit (logistic) function is why you will often here modelling 0 and 1s as logistic regression.\n\n\n4.5.3 Poisson Example\nIf we were modeling a count or something that fits a Poisson distribution, the transformation would be:\n\\(log(\\lambda) = \\alpha +\\beta_1 X_1+\\beta2 X_2\\)\nThis time \\(log(\\lambda)\\) would transform the linear predictors to be constrained to match the poisson distribution. It should be noted that Poisson distributions can be a little more tricky to deal with, as there are some other parameters to consider tuning which relate to dispersion etc. These are more advanced topics and outside the scope of this practical."
  },
  {
    "objectID": "module3.html#binomial-example-xg-model",
    "href": "module3.html#binomial-example-xg-model",
    "title": "4  Module 3",
    "section": "4.6 Binomial Example / xG model",
    "text": "4.6 Binomial Example / xG model\nOkay so now we have got past the theory, lets delve into some examples. Our first example is going to be developing our own expected Goals/Points models which can be just simply thought of as a binomial/logistic regression model. We will use information related to shot location and shots detail (General play, Set shot etc) to predict the probability of a goal being scored.\nFirst things first, lets do some quick data cleaning.\n\nshot_small &lt;- data2%&gt;%\n  filter(Rotated.xStd&gt;(-20))|&gt;\n  mutate(Shot_outcome = as.factor(ifelse(shots.result.code==\"G\",1,0)),\n         Champ_Xp_err = round(shots.result.points-shots.result.pointsExpected,3))\n\n\nggplot()+\n  geom_ellipse(aes(x0 = 0, y0 = 0, a = 80, b = 70, angle = 0))+\n  geom_hex(shot_small, \n           mapping= aes(x=Rotated.xStd,y=Rotated.yStd),\n           bins=30)+\n  xlim(c(-80,80))+\n  theme_minimal()\n\n\n\n\n\nggplot(shot_small,\n       aes(x=Rotated.xStd,\n           y=Rotated.yStd,col=shots.result.code))+\n  geom_point(alpha=.3)+\n  geom_density2d(col=\"gray90\")+\n  theme_minimal()\n\n\n\n\nLets have a quick look at how well the champion data expected points model does.\n\nMAE = mean(abs(shot_small$Champ_Xp_err))\nggplot(shot_small,aes(abs(Champ_Xp_err)))+\n  #geom_histogram(col=\"white\")+\n  ggtitle(paste(\"Campion xG error =\", round(MAE,2)))+\n  geom_density(fill=\"gray80\")+\n  theme_minimal()\n\n\n\n\nBefore we look to build our first model. Lets do some simple exploratory statistics of some of the parameters we will likely end up including in our model.\n\nshot_small|&gt;ggplot(aes(y=80-Rotated.xStd,x=Shot_outcome,\n                       col = shots.details.type))+\n  geom_jitter(height = .1,alpha=.1)+\n  geom_boxplot(width=.4)+\n  coord_flip()+\n  facet_wrap(~shots.details.type,ncol = 1)+\n  theme_light()+\n  theme(legend.position = \"bottom\")\n\n\n\n\nNot the best looking plot, but in general we can a small impact of the closer we are to the goals in x_direction the more likely we are to score a goal. Lets try model this along with the y component.\n\nm1 &lt;- glm(Shot_outcome~Rotated.xStd+Rotated.yStd,\n          data = shot_small,family = binomial)\ns =summary(m1)\n\nround(s$coefficients,3)\n\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    -1.197      0.081 -14.701        0\nRotated.xStd    0.022      0.002  14.196        0\nRotated.yStd    0.004      0.001   4.949        0\n\n\nNow lets spend a little bit of time thinking about what the above means. We will focus our time on the Estimate column. Now a logistic regression model returns coefficients (Estimate) on the log odds scale. So currently, if we focus on Rotated.xStd with its estimate of 0.022, this would mean as we get 1 metre closer to the goal holding all other variables constant our log odds of success increases by 0.022. If you still have no idea what that means, that’s okay—you are in the right place. It is often easier to understand if we convert log-odds to what are called odds ratios. We will do this below\n\ndata.frame(round(exp(coef(m1)),3))\n\n             round.exp.coef.m1....3.\n(Intercept)                    0.302\nRotated.xStd                   1.022\nRotated.yStd                   1.004\n\n\nNow we can say the odds of scoring a goal increase by 2.2% for every meter closer to the goal we get, holding all other variables equal. Keen-eyed readers might notice that the odds ratio and log-odds in this case return effectively identical numbers. Although it’s outside the scope of this module, for small values of the coefficients, both the log-odds and odds ratios will be similar.\nHowever, both log-odds and odds ratios still have limitations in that they only express the relative difference between two scenarios. They do not provide information about how this impacts the absolute risk. This is a much bigger problem in datasets with imbalanced outcomes, such as low rates of injuries versus not injured. For instance, we might say the odds of injury increase by 50% under some condition, but this might only represent an absolute risk increase from 3% to 4.5%.\nThe easiest way around all this complexity is just to plot or return the values on the probability scale. Lets do this below.\n\nxstd &lt;- predict_response(m1, \"Rotated.xStd\")\nystd &lt;- predict_response(m1, \"Rotated.yStd\")\np1   &lt;- plot(xstd,show_data = T,jitter = 0.1)\np2   &lt;- plot(ystd,show_data = T,jitter = 0.1)\n\nggarrange(p1,p2,common.legend = T)\n\n\n\n\nWe can see the effects above but if you wanted to be more explicit about contrasting the impacts, you could do something like the below\n\nem1 =emmeans(m1,~Rotated.xStd,\n          at = list(Rotated.xStd = c(25,50)),\n          type = \"response\")\n\nprint(em1)\n\n Rotated.xStd  prob      SE  df asymp.LCL asymp.UCL\n           25 0.343 0.01012 Inf     0.323     0.363\n           50 0.474 0.00486 Inf     0.464     0.483\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n\nem1 |&gt; \n regrid()|&gt;\n  pairs()|&gt;\n  data.frame()|&gt;\n  select(contrast,estimate)\n\n                         contrast   estimate\n1 Rotated.xStd25 - Rotated.xStd50 -0.1312767"
  },
  {
    "objectID": "module3.html#assess-our-first-xg-model",
    "href": "module3.html#assess-our-first-xg-model",
    "title": "4  Module 3",
    "section": "4.7 Assess our first xG model",
    "text": "4.7 Assess our first xG model\nTo do this we are going to predict our model response to the data.frame and multiple that number by 6.\n\nshot_small &lt;- shot_small|&gt;\n  mutate(xG_m1 = predict(m1,type=\"response\")*6,\n         m1_Xp_err = round(shots.result.points-xG_m1,3))\n\n\nMAE_champ  = mean(abs(shot_small$Champ_Xp_err))\nMAE_m1     = mean(abs(shot_small$m1_Xp_err))\nggplot(shot_small,aes(abs(Champ_Xp_err)))+\n  #geom_histogram(col=\"white\")+\n  ggtitle(paste(\"MAE_champ =\",round(MAE_champ,2),\"MAE_m1=\",round(MAE_m1,2)))+\n  geom_density(fill=\"gray80\",alpha=.3)+\n  geom_density(fill=\"blue\",alpha=.3,\n               shot_small, mapping=aes(abs(m1_Xp_err)))+\n  theme_minimal()\n\n\n\n\nOkay so we are slightly better than just flipping a coin on each shot at goal but champion is still a better model at the moment. Lets try one more model and see how we go.\n\nshot_small$shots.player.displayName&lt;-as.factor(shot_small$shots.player.displayName)\nshot_small$shots.details.type&lt;-as.factor(shot_small$shots.details.type)\n\nm2 &lt;- bam(Shot_outcome~te(Rotated.xStd,Rotated.yStd,by=shots.details.type)+\n                    s(shots.player.displayName,bs=\"re\"),\n           family = binomial,\n           nthreads=8,\n           discrete = T,\n           data = shot_small ,method = \"fREML\")\n\n\nlibrary(earth)\n\nWarning: package 'earth' was built under R version 4.3.3\n\n\nLoading required package: Formula\n\n\nLoading required package: plotmo\n\n\nWarning: package 'plotmo' was built under R version 4.3.3\n\n\nLoading required package: plotrix\n\n\nWarning: package 'plotrix' was built under R version 4.3.2\n\nm3 = earth::earth(Shot_outcome~Rotated.xStd+Rotated.yStd+shots.details.type,\n             degree=2, glm=list(family=binomial),data = shot_small)\n\nplot(m3, which = 1)\n\n\n\npdp::partial(m3, pred.var = \"Rotated.xStd\",prob =TRUE,\n             grid.resolution = 10,) %&gt;% autoplot()\n\n\n\n\nWe won’t bother looking at the coefs for now lets just see if we are getting better performance.\n\nshot_small &lt;- shot_small|&gt;\n  mutate(xG_m2 = predict(m2,type=\"response\")*6,\n         xG_m3 = predict(m3,type=\"response\")*6,\n         m2_Xp_err = round(shots.result.points-xG_m2,3),\n         m3_Xp_err = round(shots.result.points-xG_m3,3))\n\nMAE_m2     = mean(abs(shot_small$m2_Xp_err))\nMAE_m3     = mean(abs(shot_small$m3_Xp_err))\n\nggplot(shot_small,aes(abs(Champ_Xp_err)))+\n  #geom_histogram(col=\"white\")+\n  ggtitle(paste(\"MAE_champ =\",round(MAE_champ,2),\"MAE_m1=\",round(MAE_m1,2),\n                \"MAE_m2=\",round(MAE_m2,2),\n                \"MAE_m3=\",round(MAE_m3,2)))+\n  geom_density(fill=\"gray80\",alpha=.3)+\n  geom_density(fill=\"blue\",alpha=.3,\n               shot_small, mapping=aes(abs(m1_Xp_err)))+\n    geom_density(fill=\"red\",alpha=.3,\n               shot_small, mapping=aes(abs(m2_Xp_err)))+\n      geom_density(fill=\"yellow\",alpha=.3,\n               shot_small, mapping=aes(abs(m3_Xp_err)))+\n  theme_minimal()\n\n\n\n\n\nsum(abs(shot_small$Champ_Xp_err))-\nsum(abs(shot_small$m2_Xp_err))\n\n[1] 889.98\n\n\nSo over the 2013 season our GAM model better identified up 889 points when compared to the champion code.\nLets see if we can better visualize what our final model thinks\n\npar(mfrow=c(1,2),cex=.9)\n\nvis.gam(m2, view = c(\"Rotated.xStd\", \"Rotated.yStd\"),\n        cond=list(shots.details.type=\"Set Shot Regular\"),\n        plot.type = \"contour\", \n        color = \"heat\",too.far = 0.1,\n        lwd=2,labcex = 0.8,\n        contour.col  = \"black\",nCol = 20,\n        type = \"response\",main =\"Set Shot Regular\")\n\nvis.gam(m2, view = c(\"Rotated.xStd\", \"Rotated.yStd\"),\n        cond=list(shots.details.type=\"Set Shot Snap\"),\n        plot.type = \"contour\", \n        color = \"heat\",too.far = 0.1,\n        lwd=2,labcex = 0.8,\n        contour.col  = \"black\",nCol = 20,\n        type = \"response\",main =\"Set Shot Snap\")\n\n\n\npar(mfrow=c(1,1))"
  },
  {
    "objectID": "module3.html#circling-back",
    "href": "module3.html#circling-back",
    "title": "4  Module 3",
    "section": "4.8 Circling back",
    "text": "4.8 Circling back\nAt the beginning, you may remember my reference to developing a model that not only describes the data-generating process well but also has good properties that can be tested via hypothesis testing. I have deliberately focused more on using visuals to identify the magnitude of the effects we have observed rather than delving deeply into hypothesis testing. While hypothesis testing is a valuable resource, using it successfully requires at least a basic understanding of which assumptions are valid for a given model and which are not.\nFor example, our initial model did not perform well because it was not specified correctly; we failed to account for the fact that there were many repeat observations among players. This violates the independence assumption of GLMs and affects the calculation of standard errors and p-values.\nIn practice, you would typically first identify the probability family appropriate for your response variable, such as binomial, Poisson, or Gaussian. Next, you would select models capable of handling those probability distributions and then check the assumptions underlying those models. Some assumptions can be identified as broken just by understanding your dataset, while others can only be tested after you have built the model.\n\n4.8.1 Key Assumptions to Consider\n\nIndependence: For many models, including GLMs, observations are assumed to be independent. Violations of this assumption, such as repeated measures on the same subjects, need to be accounted for.\nNormality of Residuals: For linear models, residuals are often assumed to be normally distributed. This can be checked using diagnostic plots.\nHomoscedasticity: Constant variance of residuals is another common assumption. Heteroscedasticity can affect the efficiency of estimates and the validity of hypothesis tests.\n\n\n\n4.8.2 Practical Steps\n\nIdentify the Probability Family:\n\nDetermine whether your response variable follows a binomial, Poisson, Gaussian, or another distribution.\n\nModel Selection:\n\nChoose models that are appropriate for the identified probability distribution.\n\nCheck Assumptions:\n\nSome assumptions, such as independence, can be known to be violated from understanding the dataset. For example, if the data includes repeated measures, this should be accounted for in the model.\nOther assumptions, such as normality of residuals and homoscedasticity, can be tested after the model is built using diagnostic plots and tests.\n\nIterative Process:\n\nModel building is an iterative process. Start with a simple model, check assumptions, refine the model, and repeat as necessary.\n\n\nBy following these steps and being mindful of model assumptions, you can build more robust models and make more reliable inferences from your data\nThe below is a simple example of the subtle difference that can occur when specifying a model\n\nshot_small$dis_char &lt;- as.character(shot_small$shots.player.displayName)\n\nm0 &lt;- glm(Shot_outcome ~ shots.details.type ,data = shot_small,family = binomial)\n\nm1_glmer =glmer(Shot_outcome ~ shots.details.type +(1| dis_char),data = shot_small,family = binomial)\n\ns0 &lt;- summary(m0)\nsglmer &lt;-summary(m1_glmer)\n\nround(s0$coefficients,3)\n\n                                    Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                           -0.088      0.045  -1.958    0.050\nshots.details.typeGeneral Play Snap   -0.348      0.058  -5.966    0.000\nshots.details.typeGround Kick         -0.101      0.142  -0.709    0.478\nshots.details.typeSet Shot Regular     0.167      0.053   3.122    0.002\nshots.details.typeSet Shot Snap        0.481      0.088   5.495    0.000\n\nround(sglmer$coefficients,3)\n\n                                    Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                           -0.096      0.047  -2.047    0.041\nshots.details.typeGeneral Play Snap   -0.363      0.059  -6.131    0.000\nshots.details.typeGround Kick         -0.135      0.144  -0.943    0.346\nshots.details.typeSet Shot Regular     0.135      0.055   2.465    0.014\nshots.details.typeSet Shot Snap        0.459      0.089   5.126    0.000"
  },
  {
    "objectID": "module3.html#final-models",
    "href": "module3.html#final-models",
    "title": "4  Module 3",
    "section": "4.9 Final Models",
    "text": "4.9 Final Models\n\n4.9.1 Poisson model\nEarlier we introduced the Poisson family regression for counts. Lets quickly explore trying to model TACKLES as a function of HARD_BALL_GET.\n\nmean(data1$TACKLE)\n\n[1] 2.66093\n\nvar(data1$TACKLE)\n\n[1] 5.012247\n\ncount_model= glm(TACKLE~CONTESTED_POSSESSION,data1,\n                 family = \"poisson\")\n\nsummary(count_model)\n\n\nCall:\nglm(formula = TACKLE ~ CONTESTED_POSSESSION, family = \"poisson\", \n    data = data1)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          0.456261   0.012017   37.97   &lt;2e-16 ***\nCONTESTED_POSSESSION 0.080189   0.001439   55.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 19097  on 9935  degrees of freedom\nResidual deviance: 16299  on 9934  degrees of freedom\nAIC: 40427\n\nNumber of Fisher Scoring iterations: 5\n\npred_count &lt;- predict_response(count_model)\n\nplot(pred_count,show_data = T,jitter = T)\n\n\n\ncheck_model(count_model)\n\nNot enough model terms in the conditional part of the model to check for\n  multicollinearity.\n\n\n\n\n\n\n\n4.9.2 Multinominial\nLets say you were actually interested in modelling GOAL, BEHIND and MISS. Not just GOAL, MISS as we were before. You could do that using what is called a multinominal regression. This is extension of the GLM frame work but we have to use a different package to do this.\n\nmnommod&lt;- multinom(shots.result.code~Rotated.xStd+Rotated.yStd,data = data2)\n\n# weights:  12 (6 variable)\ninitial  value 11962.789211 \niter  10 value 11098.696107\niter  10 value 11098.696107\niter  10 value 11098.696107\nfinal  value 11098.696107 \nconverged\n\nggeffect(mnommod, terms = \"Rotated.xStd\") |&gt;\n    plot()"
  },
  {
    "objectID": "module3.html#what-we-didnt-get-time-to-explore",
    "href": "module3.html#what-we-didnt-get-time-to-explore",
    "title": "4  Module 3",
    "section": "4.10 What we didn’t get time to explore",
    "text": "4.10 What we didn’t get time to explore\n\nGaussian regression\nPoisson regression\nBinomial regression (classification)\nFractional binomial regression\nQuasibinomial regression\nMultinomial classification\nGamma regression\nOrdinal regression\nNegative Binomial regression\nTweedie distribution"
  },
  {
    "objectID": "module3.html#next-module",
    "href": "module3.html#next-module",
    "title": "4  Module 3",
    "section": "4.11 Next module",
    "text": "4.11 Next module\nWe are going to skip a lot of the math and look to say who really cares about all the technical stuff, lets just see how well we can predict something."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "GWS Data Science and Problem Recognition",
    "section": "",
    "text": "Free R for data science resource: https://r4ds.had.co.nz/introduction.html\nPlotting in R using ggplot2 https://ggplot2-book.org/\n\nFree R for data science resource: https://r4ds.had.co.nz/introduction.html\nFree ISLR resource https://www.statlearning.com/\nHands on machine learning https://bradleyboehmke.github.io/HOML/\nTidymodels: https://www.tidymodels.org/start/\nVisual learning:\nhttps://mlu-explain.github.io/\nhttp://www.r2d3.us/\n\nYoutube:\nhttps://www.youtube.com/@TidyX_screencast\nhttps://www.youtube.com/@JuliaSilge/videos"
  },
  {
    "objectID": "module4.html#recap-from-last-week",
    "href": "module4.html#recap-from-last-week",
    "title": "5  module4",
    "section": "5.1 Recap from last week",
    "text": "5.1 Recap from last week\nWe explored our ability to predict shot success compared to what Champion provides. We primarily did this through the lens of the generalized linear model (GLM) framework. The key strength of the GLM framework is its ability to represent models in a “linear” form even when estimating the effect on a binary variable (goal, miss), a count variable (number of disposals), or a range of other types of outcome distributions that don’t follow a normal distribution.\nAs the GLM framework is still linear, we can interpret the coefficients using standard hypothesis testing procedures (e.g., p-values). Additionally, with a bit of extra effort, we can extend the GLM framework further to allow for non-linear effects (we did this using the GAM model) and also situations where we violate the independence assumption. This final point can be achieved using mixed effects models, where you effectively let the model know that some observations will be clustered within players or teams, etc.\nIn research settings, you can see how the above type of modeling procedure can be quite attractive. We can make inferences about effects within a relatively flexible framework, provided that certain assumptions about the modeling procedure are met"
  },
  {
    "objectID": "module4.html#goals-for-this-week.",
    "href": "module4.html#goals-for-this-week.",
    "title": "5  Module 4",
    "section": "5.2 Goals for this week.",
    "text": "5.2 Goals for this week.\nOur current outline for the next out weeks looks like the below.\n\nIntroduction into R programming language and getting started with some descriptive statistics.\nUnderstanding key terms; Exploratory analysis, Supervised vs Unsupervised problems, regression vs classification, prediction vs association.\nA quick note on distributions - Supervised regression and classification problems using a generalized linear model framework. (Hypothesis testing)\nMachine learning approach to Supervised learning\nMethods of assessing model accuracy\nBias - variance trade-off, Feature selection, hyper-parameter tuning. (this may get blended in with 5)\nUnsupervised learning strategies (cluster analysis, PCA, market basket analysis)\nCreating a machine learning pipeline"
  },
  {
    "objectID": "module4.html#machine-learning-approach-to-supervised-learning",
    "href": "module4.html#machine-learning-approach-to-supervised-learning",
    "title": "5  Module 4",
    "section": "5.3 Machine learning approach to supervised learning",
    "text": "5.3 Machine learning approach to supervised learning\n\n5.3.1 Some quick context\nWhat makes a “machine learning” model?\nGreat question!! With no definitive answer. Technically, the GLM framework above could be considered a machine learning model, and indeed, many papers have referred to things such as logistic regression as machine learning.\nFrom an operational perspective, I like to think of it more along the lines of models that either focus on making reliable inferences about the variables used to predict an outcome (e.g., goal or miss, score differential) or models where the focus is more on the prediction itself, with less concern about how the prediction was made. (This is a very generalized statement)\nPut differently, some models will significantly relax their assumptions to find clever ways to better fit the data. This relaxation of assumptions makes it harder for us to know exactly what has happened under the hood. For example, in a linear model, I effectively tell the model that I assume the effect to be “linear,” which constrains the model to this rule. This makes it easier for us to interpret but not very flexible.\nLast week, when we used the GAM model to fit the shot data, we were effectively fitting a model that allowed us to look for non-linear effects. The easiest way to learn about these potential non-linear effects was to plot them*. Now, imagine a situation where we believe there might be interactions between different variables, some of which may be linear or non-linear, and we don’t really know in advance. It would be beneficial to have models that can potentially identify these interactions for us, and some of those models are what we are going to explore today\n*Note it is still possible to do hypothesis testing on the non-linear effects with GAMS but plotting effects is really the only way to understand the non-linear effects.\n\n\n5.3.2 What is an interaction effect?\n\n\n\n\n\nWe can notice here some different effects, but lets say the effects might be even more complicated.\n\n\n\n\n\nNow imagine that whether the effect was linear, non-linear or linear dependent on other variables as well. Hopefully, you can see how quickly this can get quite complex to model with a simple linear model.\nFor example see this example below taken from https://link.springer.com/article/10.3758/s13428-024-02389-1\nWhere they are looking at factors that impact science ability over time. These factors include SES (Socio Economic Status), GMOTOR (Gross motor skills) and INTERN (internalization problems)."
  },
  {
    "objectID": "module4.html#enough-talking-lets-do-some-machine-learning",
    "href": "module4.html#enough-talking-lets-do-some-machine-learning",
    "title": "5  Module 4",
    "section": "5.4 Enough talking lets do some machine learning",
    "text": "5.4 Enough talking lets do some machine learning\nToday the main models we are going to explore are the Decision Tree and Random Forest Algorithm. Lets have a quick look at where these algorithms sit on the conceptual model map from the ISLR book.\n\nThe decision tree is perhaps considered one of the most interpretable machine learning algorithms in that you can just traverse a tree to get explanations of expected values (more on this in a second). As always, there is no free lunch and decision trees often are extremely sensitive to initial conditions and can vary a lot with small perturbations in the data. This link visualizers what I am talking about https://mlu-explain.github.io/decision-tree/.\nThis subsequently, reduces there performance on hold out data set and makes decision trees not necessarily the best option from a predictive standpoint.\nWe will go over how to use them anyway, as there may be an occasion where the difference in performance between a more complicated model and a decision tree is trivial enough that you choose to go with a model that is more easily interpreted. Or If you are relative confident that the key variables are the same between models, you may use the decision tree as a point of illustration but know that in the back ground you are using a better performing model."
  },
  {
    "objectID": "module4.html#load-data-and-packages",
    "href": "module4.html#load-data-and-packages",
    "title": "5  Module 4",
    "section": "5.5 Load data and packages",
    "text": "5.5 Load data and packages\n\npacman::p_load(data.table,tidyverse,caret,mgcv,ggtext,\n               rpart,rpart.plot,pdp,party,ranger,glue,patchwork)\n\nurlfile=\"https://raw.githubusercontent.com/R2mu/GWS_DSPR/main/data/mod2data.csv\"\n\ndata1 &lt;- fread(urlfile)\n\nurlfile2=\"https://raw.githubusercontent.com/R2mu/GWS_DSPR/main/data/shotdata.csv\"\n\ndata2 &lt;- fread(urlfile2)\ndata2 &lt;- data2|&gt;\n  rename(Rotated.xStd=shots.details.locationRotated.xStd,\n         Rotated.yStd=shots.details.locationRotated.yStd)\n\nshot_small &lt;- data2%&gt;%\n  filter(Rotated.xStd&gt;(-20))|&gt;\n  mutate(Shot_outcome = as.factor(ifelse(shots.result.code==\"G\",1,0)),\n         Champ_Xp_err = round(shots.result.points-shots.result.pointsExpected,3))\n\nshot_small$shots.player.displayName&lt;-as.factor(shot_small$shots.player.displayName)\nshot_small$shots.details.type&lt;-as.factor(shot_small$shots.details.type)\n\nWe will continue to focus on what we explored last week and see if these machine learning models can help do a better job than what we achieved last week. Lets quickly recap where we got to last week.\n\n## simple logistic regression model\nm1 &lt;- glm(Shot_outcome~Rotated.xStd+Rotated.yStd,\n          data = shot_small,\n          family = binomial)\n\n## \nm2 &lt;- bam(Shot_outcome~te(Rotated.xStd,Rotated.yStd,by=shots.details.type)+\n                    s(shots.player.displayName,bs=\"re\"),\n           family = binomial,\n           nthreads=8,\n           discrete = T,\n           data = shot_small ,method = \"fREML\")\n\n\nshot_small &lt;- shot_small|&gt;\n  mutate(xG_m1 = predict(m1,type=\"response\")*6,\n         xG_m2 = predict(m2,type=\"response\")*6,\n         glm_Xp_err = round(shots.result.points-xG_m1,3),\n         gam_Xp_err = round(shots.result.points-xG_m2,3))\n\n\nMAE_champ   = round(mean(abs(shot_small$Champ_Xp_err)),2)\nMAE_glm     = round(mean(abs(shot_small$glm_Xp_err)),2)\nMAE_gam     = round(mean(abs(shot_small$gam_Xp_err)),2)\n\nggplot(shot_small,aes(abs(Champ_Xp_err))) +\n  geom_density(size = 3,fill=\"#0072B2\",alpha=.3,col=NA) +\n    geom_density(fill=\"#D55E00\",alpha=.3,\n               shot_small, mapping=aes(abs(glm_Xp_err)))+\n    geom_density(fill=\"#009E73\",alpha=.3,\n               shot_small, mapping=aes(abs(gam_Xp_err)))+\n  labs(\n    title = glue(\"**Model comparison**&lt;br&gt;\",\n                 \"&lt;span style='color:#0072B2;'&gt;Champ: {MAE_champ}&lt;/span&gt;, \",\n                 \"&lt;span style='color:#D55E00;'&gt;GLM: {MAE_glm}&lt;/span&gt;, and \",\n                 \"&lt;span style='color:#009E73;'&gt;GAM: {MAE_gam}&lt;/span&gt;\")\n  )+\n  theme_minimal() +\n  theme(\n    plot.title = element_markdown(lineheight = 1.1),\n    legend.text = element_markdown(size = 11)\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "module4.html#decision-tree",
    "href": "module4.html#decision-tree",
    "title": "5  Module 4",
    "section": "5.6 Decision tree",
    "text": "5.6 Decision tree\nLets create our first decision tree.\n\nm3_dt &lt;- rpart(\n  formula = Shot_outcome~Rotated.xStd+Rotated.yStd+ shots.details.type,\n  data    = shot_small,\n  method  = \"class\"\n)\n\nsummary(m3_dt)\n\nCall:\nrpart(formula = Shot_outcome ~ Rotated.xStd + Rotated.yStd + \n    shots.details.type, data = shot_small, method = \"class\")\n  n= 10886 \n\n          CP nsplit rel error    xerror        xstd\n1 0.08892714      0 1.0000000 1.0000000 0.009968954\n2 0.04016064      2 0.8221457 0.8328552 0.009775323\n3 0.03155479      3 0.7819851 0.8024479 0.009711314\n4 0.01099637      4 0.7504303 0.7588449 0.009603362\n5 0.01000000      6 0.7284376 0.7481354 0.009573871\n\nVariable importance\n      Rotated.yStd       Rotated.xStd shots.details.type \n                46                 36                 18 \n\nNode number 1: 10886 observations,    complexity param=0.08892714\n  predicted class=0  expected loss=0.4803417  P(node) =1\n    class counts:  5657  5229\n   probabilities: 0.520 0.480 \n  left son=2 (2261 obs) right son=3 (8625 obs)\n  Primary splits:\n      Rotated.yStd       &lt; -21.05 to the left,  improve=139.95050, (0 missing)\n      Rotated.xStd       &lt; 65.05  to the left,  improve= 77.49282, (0 missing)\n      shots.details.type splits as  RLLRR,      improve= 65.45643, (0 missing)\n\nNode number 2: 2261 observations\n  predicted class=0  expected loss=0.3237506  P(node) =0.207698\n    class counts:  1529   732\n   probabilities: 0.676 0.324 \n\nNode number 3: 8625 observations,    complexity param=0.08892714\n  predicted class=1  expected loss=0.4786087  P(node) =0.792302\n    class counts:  4128  4497\n   probabilities: 0.479 0.521 \n  left son=6 (2575 obs) right son=7 (6050 obs)\n  Primary splits:\n      Rotated.yStd       &lt; 17.55  to the right, improve=124.69680, (0 missing)\n      Rotated.xStd       &lt; 57.75  to the left,  improve= 97.15160, (0 missing)\n      shots.details.type splits as  RLLRR,      improve= 88.77016, (0 missing)\n\nNode number 6: 2575 observations\n  predicted class=0  expected loss=0.391068  P(node) =0.2365423\n    class counts:  1568  1007\n   probabilities: 0.609 0.391 \n\nNode number 7: 6050 observations,    complexity param=0.04016064\n  predicted class=1  expected loss=0.4231405  P(node) =0.5557597\n    class counts:  2560  3490\n   probabilities: 0.423 0.577 \n  left son=14 (2274 obs) right son=15 (3776 obs)\n  Primary splits:\n      shots.details.type splits as  RLLRR,      improve=110.3041, (0 missing)\n      Rotated.xStd       &lt; 51.45  to the left,  improve=106.6849, (0 missing)\n      Rotated.yStd       &lt; -11.75 to the left,  improve= 36.7377, (0 missing)\n  Surrogate splits:\n      Rotated.xStd &lt; 14.5   to the left,  agree=0.627, adj=0.008, (0 split)\n\nNode number 14: 2274 observations,    complexity param=0.01099637\n  predicted class=0  expected loss=0.4538259  P(node) =0.2088922\n    class counts:  1242  1032\n   probabilities: 0.546 0.454 \n  left son=28 (721 obs) right son=29 (1553 obs)\n  Primary splits:\n      Rotated.xStd       &lt; 51.45  to the left,  improve=27.450270, (0 missing)\n      Rotated.yStd       &lt; -12.55 to the left,  improve=16.799070, (0 missing)\n      shots.details.type splits as  -RL--,      improve= 2.128917, (0 missing)\n  Surrogate splits:\n      Rotated.yStd &lt; 17.35  to the right, agree=0.683, adj=0.001, (0 split)\n\nNode number 15: 3776 observations,    complexity param=0.03155479\n  predicted class=1  expected loss=0.3490466  P(node) =0.3468675\n    class counts:  1318  2458\n   probabilities: 0.349 0.651 \n  left son=30 (1349 obs) right son=31 (2427 obs)\n  Primary splits:\n      Rotated.xStd       &lt; 41.05  to the left,  improve=188.85380, (0 missing)\n      Rotated.yStd       &lt; -11.75 to the left,  improve= 21.76219, (0 missing)\n      shots.details.type splits as  L--RR,      improve= 18.69419, (0 missing)\n\nNode number 28: 721 observations\n  predicted class=0  expected loss=0.3398058  P(node) =0.06623186\n    class counts:   476   245\n   probabilities: 0.660 0.340 \n\nNode number 29: 1553 observations,    complexity param=0.01099637\n  predicted class=1  expected loss=0.4932389  P(node) =0.1426603\n    class counts:   766   787\n   probabilities: 0.493 0.507 \n  left son=58 (308 obs) right son=59 (1245 obs)\n  Primary splits:\n      Rotated.yStd       &lt; -12.55 to the left,  improve=19.513420, (0 missing)\n      shots.details.type splits as  -RL--,      improve=12.585550, (0 missing)\n      Rotated.xStd       &lt; 75.55  to the right, improve= 1.830883, (0 missing)\n\nNode number 30: 1349 observations\n  predicted class=0  expected loss=0.4388436  P(node) =0.1239206\n    class counts:   757   592\n   probabilities: 0.561 0.439 \n\nNode number 31: 2427 observations\n  predicted class=1  expected loss=0.2311496  P(node) =0.2229469\n    class counts:   561  1866\n   probabilities: 0.231 0.769 \n\nNode number 58: 308 observations\n  predicted class=0  expected loss=0.3474026  P(node) =0.02829322\n    class counts:   201   107\n   probabilities: 0.653 0.347 \n\nNode number 59: 1245 observations\n  predicted class=1  expected loss=0.4538153  P(node) =0.1143671\n    class counts:   565   680\n   probabilities: 0.454 0.546 \n\n\nIt is possible to read through this, but its just a lot easier to plot it instead. Here are two options.\n\nrpart.plot(m3_dt)\n\n\n\n\nThis option make take a minute or two to run.\n\npdp_result_dt&lt;- partial(m3_dt,\n        pred.var = c(\"Rotated.xStd\",\"Rotated.yStd\",\"shots.details.type\"), \n        prob = T,\n        which.class = 2)\n\nggplot(pdp_result_dt, aes(x = Rotated.xStd, y = Rotated.yStd, fill = yhat)) +\n  geom_tile() +\n  #geom_contour(aes(z = yhat), color = \"white\") +\n  scale_fill_viridis_c(name = \"Probability of Goal\") +\n  facet_wrap(~ shots.details.type,ncol = 3) +\n  theme_minimal() +\n  labs(title = \"Partial Dependence Plot by shot type\",\n       subtitle = \"Probability of goal\")\n\n\n\n\nHopefully the idea of how the decision tree creates Decision boundaries is clear from the photo above.\nLets see how it performed even though I already know it probably won;t beat our GAM model.\n\nshot_small &lt;- shot_small|&gt;\n  mutate(xG_m3 = predict(m3_dt,type=\"prob\")[,2]*6,\n         DT_Xp_err = round(shots.result.points-xG_m3,3))\n\nMAE_DT      = round(mean(abs(shot_small$DT_Xp_err)),2)\n\nggplot(shot_small,aes(abs(Champ_Xp_err))) +\n  geom_density(size = 3,fill=\"#0072B2\",alpha=.3,col=NA) +\n    geom_density(fill=\"#D55E00\",alpha=.3,\n               shot_small, mapping=aes(abs(glm_Xp_err)))+\n    geom_density(fill=\"#009E73\",alpha=.3,\n               shot_small, mapping=aes(abs(gam_Xp_err)))+\n      geom_density(fill=\"#CC79A7\",alpha=.3,\n               shot_small, mapping=aes(abs(DT_Xp_err)))+\n  labs(\n    title = glue(\"**Model comparison**&lt;br&gt;\",\n                 \"&lt;span style='color:#0072B2;'&gt;Champ: {MAE_champ}&lt;/span&gt;, \",\n                 \"&lt;span style='color:#D55E00;'&gt;GLM: {MAE_glm}&lt;/span&gt;, and \",\n                 \"&lt;span style='color:#009E73;'&gt;GAM: {MAE_gam}&lt;/span&gt;, and \",\n                 \"&lt;span style='color:#CC79A7;'&gt;DT: {MAE_DT}&lt;/span&gt;\")\n  )+\n  theme_minimal() +\n  theme(\n    plot.title = element_markdown(lineheight = 1.1),\n    legend.text = element_markdown(size = 11)\n  )\n\n\n\n\nSo we can see it did better than our GLM model but worse than the champ model and GAM model. Now to be fair to the decision tree model there is some additional tuning we could do to it but the above does sort of align with a general summary of DT’s. In that they are most likely never going to have best in house predictive performance despite having a high degree of interpretability.\nAs mentioned earlier decision trees are very sensitive to initial conditions and can vary a lot from small deviations in data. https://mlu-explain.github.io/decision-tree/."
  },
  {
    "objectID": "module4.html#random-forest",
    "href": "module4.html#random-forest",
    "title": "5  Module 4",
    "section": "5.7 Random forest",
    "text": "5.7 Random forest\nA simple solution around the above probelm is to not build one decision tree but many decision tress (100s or 1000s) and take an aggregation of all the decision trees to give a pooled estimate. This approach is called bagging, with a special case of the bagging method called the Random forest algorithm. We will explore how a random forest algorithm will work on our data as it tends to have good out of the bag performance on tabular data but know that there other methods such as bagged trees and boosting methods which can also perform well.\n\n\nm4_rf &lt;- ranger(\n  formula = Shot_outcome ~ Rotated.xStd + Rotated.yStd + shots.details.type,\n  data = shot_small,\n  num.trees = 300,  # Number of trees in the forest\n  mtry = NULL,      # Number of variables to possibly split at in each node\n  importance = 'permutation',  # Calculate variable importance\n  probability = TRUE  # For classification, set to TRUE to get probabilities\n)\n\nLets see how it performed\n\nshot_small &lt;- shot_small|&gt;\n  mutate(xG_m4 = predict(m4_rf,data=shot_small,type = \"response\")$predictions[,2]*6,\n         RF_Xp_err = round(shots.result.points-xG_m4,3))\n\n\nMAE_RF      = round(mean(abs(shot_small$RF_Xp_err)),2)\n\ng1 =ggplot(shot_small,aes(abs(Champ_Xp_err))) +\n  geom_density(size = 3,fill=\"#0072B2\",alpha=.3,col=NA) +\n    geom_density(fill=\"#D55E00\",alpha=.3,\n               shot_small, mapping=aes(abs(glm_Xp_err)))+\n    geom_density(fill=\"#009E73\",alpha=.3,\n               shot_small, mapping=aes(abs(gam_Xp_err)))+\n      geom_density(fill=\"#CC79A7\",alpha=.3,\n               shot_small, mapping=aes(abs(DT_Xp_err)))+\n       geom_density(fill=\"#E69F00\",alpha=.3,\n               shot_small, mapping=aes(abs(RF_Xp_err)))+\n  labs(\n    title = glue(\"**Model comparison**&lt;br&gt;\",\n                 \"&lt;span style='color:#0072B2;'&gt;Champ: {MAE_champ}&lt;/span&gt;, \",\n                 \"&lt;span style='color:#D55E00;'&gt;GLM: {MAE_glm}&lt;/span&gt;, \",\n                 \"&lt;span style='color:#009E73;'&gt;GAM: {MAE_gam}&lt;/span&gt;, \",\n                 \"&lt;span style='color:#CC79A7;'&gt;DT: {MAE_DT}&lt;/span&gt;, \",\n                 \"&lt;span style='color:#E69F00;'&gt;RF: {MAE_RF}&lt;/span&gt;\")\n  )+\n  theme_minimal() +\n  theme(\n    plot.title = element_markdown(lineheight = 1.1),\n    legend.text = element_markdown(size = 11)\n  )\n\n\ng2 =ggplot(shot_small,aes(abs(Champ_Xp_err))) +\n    geom_density(fill=\"#009E73\",alpha=.3,\n               shot_small, mapping=aes(abs(gam_Xp_err)))+\n       geom_density(fill=\"#E69F00\",alpha=.3,\n               shot_small, mapping=aes(abs(RF_Xp_err)))+\n  labs(title = glue(\n                 \"&lt;span style='color:#009E73;'&gt;GAM: {MAE_gam}&lt;/span&gt;, \",\n                 \"&lt;span style='color:#E69F00;'&gt;RF: {MAE_RF}&lt;/span&gt;\"))+\n  theme_minimal() +\n  theme(\n    plot.title = element_markdown(lineheight = 1.1),\n    legend.text = element_markdown(size = 11)\n  )\n\n\nfinres &lt;- data.frame(models = c(\"champ\",\"glm\",\"gam\",\"DT\",\"RF\"),\n                    MAE = c(mean(abs(shot_small$Champ_Xp_err)),MAE_glm,MAE_gam,\n                             MAE_DT,MAE_RF))\n\ng3= ggplot(finres,aes(x=models,y=MAE,col=models))+\n  geom_point()+\n  scale_colour_manual(values = c(\"champ\"=\"#0072B2\",\n                                 \"glm\" = \"#D55E00\",\n                                 \"gam\" = \"#009E73\",\n                                 \"DT\" = \"#CC79A7\",\n                                 \"RF\"=\"#E69F00\"))+\n   theme_minimal()\n(g1|g3)/g2"
  },
  {
    "objectID": "module4.html#interpreting-the-rf",
    "href": "module4.html#interpreting-the-rf",
    "title": "5  Module 4",
    "section": "5.8 Interpreting the RF?",
    "text": "5.8 Interpreting the RF?\nIn this situation it seems to be just beating our GAM model as mentioned earlier one of the strengths of RF is that they have very good out of the box performance and whilst there are parameters that can be tuned usually fine tuning those parameters only result in trivial increases.\nFor a detailed understanding of those parameters have a read of the chapter in the ISLR book https://www.statlearning.com/ or this chapter from book the Hands on Machine learning with R https://bradleyboehmke.github.io/HOML/random-forest.html.\nAs always there is no free lunch and a model that was perceived as easily interpretable in the decision tree now requires a different slightly more complex method for assessing which features are influencing the model.\n\ndata.frame(m4_rf$variable.importance)|&gt;\n  rownames_to_column(var=\"variables\")|&gt;\n  rename(\"importance\"=m4_rf.variable.importance)|&gt;\n  ggplot(aes(x=importance,y=variables))+\n  geom_col()+\n  theme_bw()\n\n\n\n\nNow, what exactly the word “importance” means for a Random Forest can be a tad complicated. There are two common ways it can be represented: impurity and permutation. In general, the key variables will often be the same between the two approaches (impurity and permutation).\nI’ve gone with the permutation-based method here because it has some nice characteristics:\n\nIt’s often more reliable, especially for complex datasets.\nIt’s less impacted by high cardinality features or correlations between variables.\nIt’s calculated after the model is trained, looking at how predictions change.\n\nIn contrast, the impurity method measures importance during training based on how features split the trees.\nSo to simply describe the plot above:\n\nBigger numbers mean more importance.\nThis importance is calculated by seeing how much the model accuracy decreases when data in a variable is randomly shuffled. If randomly shuffling the data in a column decreases the model accuracy a lot, this suggests the variable has high importance.\n\nRemember, these importance measures help us understand which features the model relies on most, but they can be misleading in that they don;t necessarily tell you the practical relevance of the effect observed. For that we need partial dependency plots.\nIf you decide to run this just note this can take some time to run. e.g. on my decently powered comp this can take 10 minutes.\n\nsample_data &lt;- shot_small[sample(nrow(shot_small), 400), ]\n\npdp_result&lt;- partial(m4_rf,\n        pred.var = c(\"Rotated.xStd\",\"Rotated.yStd\",\"shots.details.type\"), \n        prob = T,\n        which.class = 2,\n        train = sample_data)\n\nggplot(pdp_result, aes(x = Rotated.xStd, y = Rotated.yStd, fill = yhat)) +\n  geom_tile() +\n  #geom_contour(aes(z = yhat), color = \"white\") +\n  scale_fill_viridis_c(name = \"Probability of Goal\") +\n  facet_wrap(~ shots.details.type,ncol = 3) +\n  theme_minimal() +\n  labs(title = \"Partial Dependence Plot by shot type\",\n       subtitle = \"Probability of goal\")\n\n\n\n\n\nggplot(setDT(pdp_result)[shots.details.type==\"Set Shot Regular\",],\n       aes(x = Rotated.xStd, y = Rotated.yStd, fill = yhat)) +\n  geom_tile() +\n  #geom_contour(aes(z = yhat), color = \"white\") +\n  scale_fill_viridis_c(name = \"Probability of Goal\") +\n  facet_wrap(~ shots.details.type,ncol = 3) +\n  theme_minimal() +\n  labs(title = \"Partial Dependence Plot by shot type\",\n       subtitle = \"Probability of goal\")\n\n\n\n\nLets compare that to the GAM model.\n\nvis.gam(m2, view = c(\"Rotated.xStd\", \"Rotated.yStd\"),\n        cond=list(shots.details.type=\"Set Shot Regular\"),\n        plot.type = \"contour\",\n       color = \"heat\",\n        too.far = 0.1,\n        lwd=2,\n        #labcex = 0.8,\n        contour.col  = \"black\",nCol = 20,\n        type = \"response\",main =\"Set Shot Regular\")\n\n\n\n\nPretty similar but you may be able to see that the GAM model is smoother while the RF is more segmented. This represents the decision boundaries that happen when creating a decision tree."
  },
  {
    "objectID": "module4.html#so-just-use-random-forest",
    "href": "module4.html#so-just-use-random-forest",
    "title": "5  Module 4",
    "section": "5.9 So just use Random Forest?",
    "text": "5.9 So just use Random Forest?\nWell the Random forest is a very good algorithm but we initially used it on a very big data set and whilst it has lots of procedures in place to limit over-fitting it definitely is not immune to it. Over-fitting refers to when machine learning models start to try and effectively model/capture noise in data. This can cause problems when trying to make future predictions that won;t necessarily follow those same rules learnt.\nLets see if our RF is struggling with that?\n\nlibrary(rsample)\n\nset.seed(123)\n\n## lets make our dataset smaller with only 3000 observations\nshot_smaller &lt;- shot_small[sample(nrow(shot_small), 3000), ]\n\n### we are going to randomly select 75% for training and leave 25% for testing\ntrain_test_split.Z  &lt;- initial_split(shot_smaller,  prop = 3/4) \n\n### here is our 75% split for training\nTRAIN.z &lt;- training(train_test_split.Z\n                    )[,c(\"shots.player.displayName\",\"shots.details.type\"):=\n                        lapply(.SD,as.factor),\n                      .SDcols=c(\"shots.player.displayName\",\"shots.details.type\")]\n\n## helper function for later\ntrainNames = unique(TRAIN.z$shots.player.displayName)\n\n## here is our 25% spliit of data for testing\nTEST.z  &lt;- testing(train_test_split.Z\n                   )[,c(\"shots.player.displayName\",\"shots.details.type\"):=\n                        lapply(.SD,as.factor),\n                      .SDcols=c(\"shots.player.displayName\",\"shots.details.type\")\n                     ][,key:= ifelse(shots.player.displayName%in%trainNames,1,0)]\n\n\nTEST.z = TEST.z[key==1,]\n\n\nm2_gam_TRAIN &lt;- bam(\n           Shot_outcome~te(Rotated.xStd,Rotated.yStd,by=shots.details.type)+\n                    s(shots.player.displayName,bs=\"re\"),\n           family = binomial,\n           nthreads=8,\n           discrete = T,\n           data = TRAIN.z ,\n           method = \"fREML\")\n\n\nm4_rf_TRAIN &lt;- ranger(\n  formula = Shot_outcome ~ Rotated.xStd + Rotated.yStd + shots.details.type,\n  data = TRAIN.z,\n  num.trees = 500,  # Number of trees in the forest\n  mtry = NULL,      # Number of variables to possibly split at in each node\n  importance = 'permutation',  # Calculate variable importance\n  probability = TRUE  # For classification, set to TRUE to get probabilities\n)\n\n\nTEST.z = TEST.z|&gt;\n   mutate(\n    # For GAM model\n    xG_gam = predict(m2_gam_TRAIN, newdata = cur_data(),type=\"response\")*6,\n    gam_Xp_err = round(shots.result.points - xG_gam, 3),\n    \n    # For Random Forest model\n    xG_rf = predict(m4_rf_TRAIN, data = cur_data(), type = \"response\")$predictions[,2] * 6,\n    RF_Xp_err = round(shots.result.points - xG_rf, 3)\n  )\n\nMAE_gam_test = round(mean(abs(TEST.z$gam_Xp_err)),2)\nMAE_RF_test  = round(mean(abs(TEST.z$RF_Xp_err)),2)\n\ng5= ggplot(TEST.z,aes(abs(Champ_Xp_err))) +\n    geom_density(fill=\"#009E73\",alpha=.3,\n               TEST.z, mapping=aes(abs(gam_Xp_err)))+\n       geom_density(fill=\"#E69F00\",alpha=.3,\n               TEST.z, mapping=aes(abs(RF_Xp_err)))+\n  labs(title = glue(\"&lt;b&gt;Test data&lt;/b&gt;&lt;br&gt;\n                 &lt;span style='color:#009E73;'&gt;GAM: {MAE_gam_test}&lt;/span&gt;,\n                 &lt;span style='color:#E69F00;'&gt;RF: {MAE_RF_test}&lt;/span&gt;\"))+\n  theme_minimal() +\n  theme(\n    plot.title = element_markdown(lineheight = 1.1),\n    legend.text = element_markdown(size = 11)\n  )\n\ng2|g5\n\n\n\n\nWhat do you notice?"
  },
  {
    "objectID": "module4.html#wrapping-up",
    "href": "module4.html#wrapping-up",
    "title": "5  Module 4",
    "section": "5.12 Wrapping up",
    "text": "5.12 Wrapping up\nWe have explored some commonly use machine learning algorithms today that can be used for both regression (predicting a number) and classification (predicting a class). There are many other algorithms that could be really useful and are discussed in some of the books I have referenced these include but not limited to.\n\nMARS : Multi-Adaptive Regression Splines\nBART: Bayesian Additive Regression Trees\nGPboost : allows for combining tree-boosting and mixed effects models."
  },
  {
    "objectID": "module4.html#next-week",
    "href": "module4.html#next-week",
    "title": "5  Module 4",
    "section": "5.13 Next Week",
    "text": "5.13 Next Week\nWe will explore in more detail over model performance metrics, model tuning and techniques for assessing model validity,reliability and calibration."
  },
  {
    "objectID": "module4.html#recap-from-last-module",
    "href": "module4.html#recap-from-last-module",
    "title": "5  Module 4",
    "section": "5.1 Recap from last module",
    "text": "5.1 Recap from last module\nWe explored our ability to predict shot success compared to what Champion provides. We primarily did this through the lens of the generalized linear model (GLM) framework. The key strength of the GLM framework is its ability to represent models in a “linear” form even when estimating the effect on a binary variable (goal, miss), a count variable (number of disposals), or a range of other types of outcome distributions that don’t follow a normal distribution.\nAs the GLM framework is still linear, we can interpret the coefficients using standard hypothesis testing procedures (e.g., p-values). Additionally, with a bit of extra effort, we can extend the GLM framework further to allow for non-linear effects (we did this using the GAM model) and also situations where we violate the independence assumption. This final point can be achieved using mixed effects models, where you effectively let the model know that some observations will be clustered within players or teams, etc.\nIn research settings, you can see how the above type of modeling procedure can be quite attractive. We can make inferences about effects within a relatively flexible framework, provided that certain assumptions about the modeling procedure are met"
  },
  {
    "objectID": "module4.html#so-just-use-the-random-forest",
    "href": "module4.html#so-just-use-the-random-forest",
    "title": "5  Module 4",
    "section": "5.9 So just use the Random Forest?",
    "text": "5.9 So just use the Random Forest?\nWell the Random forest is a very good algorithm but we initially used it on a very big data set and whilst it has lots of procedures in place to limit over-fitting it definitely is not immune to it. Over-fitting refers to when machine learning models start to try and effectively model/capture noise in data. This can cause problems when trying to make future predictions that won;t necessarily follow those same rules learnt.\nLets see if our RF is struggling with that?\n\nlibrary(rsample)\n\nset.seed(123)\n\n## lets make our dataset smaller with only 3000 observations\nshot_smaller &lt;- shot_small[sample(nrow(shot_small), 3000), ]\n\n### we are going to randomly select 75% for training and leave 25% for testing\ntrain_test_split.Z  &lt;- initial_split(shot_smaller,  prop = 3/4) \n\n### here is our 75% split for training\nTRAIN.z &lt;- training(train_test_split.Z\n                    )[,c(\"shots.player.displayName\",\"shots.details.type\"):=\n                        lapply(.SD,as.factor),\n                      .SDcols=c(\"shots.player.displayName\",\"shots.details.type\")]\n\n## helper function for later\ntrainNames = unique(TRAIN.z$shots.player.displayName)\n\n## here is our 25% spliit of data for testing\nTEST.z  &lt;- testing(train_test_split.Z\n                   )[,c(\"shots.player.displayName\",\"shots.details.type\"):=\n                        lapply(.SD,as.factor),\n                      .SDcols=c(\"shots.player.displayName\",\"shots.details.type\")\n                     ][,key:= ifelse(shots.player.displayName%in%trainNames,1,0)]\n\n\nTEST.z = TEST.z[key==1,]\n\n\nm2_gam_TRAIN &lt;- bam(\n           Shot_outcome~te(Rotated.xStd,Rotated.yStd,by=shots.details.type)+\n                    s(shots.player.displayName,bs=\"re\"),\n           family = binomial,\n           nthreads=8,\n           discrete = T,\n           data = TRAIN.z ,\n           method = \"fREML\")\n\n\nm4_rf_TRAIN &lt;- ranger(\n  formula = Shot_outcome ~ Rotated.xStd + Rotated.yStd + shots.details.type,\n  data = TRAIN.z,\n  num.trees = 500,  # Number of trees in the forest\n  mtry = NULL,      # Number of variables to possibly split at in each node\n  importance = 'permutation',  # Calculate variable importance\n  probability = TRUE  # For classification, set to TRUE to get probabilities\n)\n\n\nTEST.z = TEST.z|&gt;\n   mutate(\n    # For GAM model\n    xG_gam = predict(m2_gam_TRAIN, newdata = cur_data(),type=\"response\")*6,\n    gam_Xp_err = round(shots.result.points - xG_gam, 3),\n    \n    # For Random Forest model\n    xG_rf = predict(m4_rf_TRAIN, data = cur_data(), type = \"response\")$predictions[,2] * 6,\n    RF_Xp_err = round(shots.result.points - xG_rf, 3)\n  )\n\nMAE_gam_test = round(mean(abs(TEST.z$gam_Xp_err)),2)\nMAE_RF_test  = round(mean(abs(TEST.z$RF_Xp_err)),2)\n\ng5= ggplot(TEST.z,aes(abs(Champ_Xp_err))) +\n    geom_density(fill=\"#009E73\",alpha=.3,\n               TEST.z, mapping=aes(abs(gam_Xp_err)))+\n       geom_density(fill=\"#E69F00\",alpha=.3,\n               TEST.z, mapping=aes(abs(RF_Xp_err)))+\n  labs(title = glue(\"&lt;b&gt;Test data&lt;/b&gt;&lt;br&gt;\n                 &lt;span style='color:#009E73;'&gt;GAM: {MAE_gam_test}&lt;/span&gt;,\n                 &lt;span style='color:#E69F00;'&gt;RF: {MAE_RF_test}&lt;/span&gt;\"))+\n  theme_minimal() +\n  theme(\n    plot.title = element_markdown(lineheight = 1.1),\n    legend.text = element_markdown(size = 11)\n  )\n\ng2|g5\n\n\n\n\nWhat do you notice?"
  },
  {
    "objectID": "module4.html#extreme-case-xgboost",
    "href": "module4.html#extreme-case-xgboost",
    "title": "5  Module 4",
    "section": "5.10 Extreme case (XGBoost)",
    "text": "5.10 Extreme case (XGBoost)\n\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(Matrix)\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n# Split data into training and testing sets\nvar&lt;-Shot_outcome ~ Rotated.xStd + Rotated.yStd + shots.details.type\n\ntraining.samples &lt;- shot_smaller$Shot_outcome %&gt;%\n  createDataPartition(p = 0.75, list = FALSE)\n\ntrain.data       &lt;- shot_smaller[training.samples, ]\ntrain.data$Shot_outcome &lt;- as.numeric(as.character(train.data$Shot_outcome))\n  \ntest.data        &lt;- shot_smaller[-training.samples, ]\ntest.data$Shot_outcome &lt;- as.numeric(as.character(test.data$Shot_outcome))\n\nsm &lt;- sparse.model.matrix(var,  data = train.data)\ntd &lt;- sparse.model.matrix(var,  data = test.data)\n\n\nparams &lt;- list(\n  objective = \"binary:logistic\",\n  max_depth = 8,               # Deep trees\n  eta = 0.25,                    # High learning rate\n  subsample = 1,                # Use all data for each tree\n  colsample_bytree = 1,         # Use all features for each tree\n  min_child_weight = 5,         # Allow very specific splits\n#  scale_pos_weight = 1,         # Balance positive and negative weights\n  nthread = 8\n)\n\n# Train the XGBoost model\nxgb_model &lt;- xgboost(\n  sm,\n  label = train.data$Shot_outcome,\n  params = params,\n  nrounds = 1000,               # Many boosting rounds\n  verbose = 0\n)\n\n\ntrain_pred &lt;- predict(xgb_model,sm)\ntest_pred &lt;- predict(xgb_model, td)\n\ntrain.data &lt;- train.data|&gt;\n  mutate(xG_pred = train_pred*6,\n         xG_Xp_err = round(shots.result.points-xG_pred,3),\n         xG_abs_err = abs(xG_Xp_err),\n         split = \"Train\")\n\nMAE_xG_train = round(mean(abs(train.data$xG_Xp_err)),2)\n\ntest.data &lt;- test.data|&gt;\n  mutate(xG_pred = test_pred*6,\n         xG_Xp_err = round(shots.result.points-xG_pred,3),\n         xG_abs_err = abs(xG_Xp_err),\n         split = \"Test\")\n\nMAE_xG_test  = round(mean(abs(test.data$xG_Xp_err)),2)\n\ncomb &lt;- rbind(train.data,test.data)\n\n\n\n\nggplot(comb,aes(xG_abs_err,fill=split))+\n  geom_density(alpha=.5)+\n   labs(title = glue(\"&lt;b&gt;xgBoost on data splits&lt;/b&gt;&lt;br&gt;\"))+\n            #     &lt;span style='color:#48cdd1;'&gt;train: {MAE_xG_train}&lt;/span&gt;,\n            #     &lt;span style='color:#fa9f99;'&gt;test: {MAE_xG_test}&lt;/span&gt;\"))+\n  theme_minimal() +\n  geom_vline(xintercept = c(MAE_xG_train),col=\"#48cdd1\",linetype=\"dashed\")+\n  geom_vline(xintercept = c(MAE_xG_test),col=\"#fa9f99\",linetype=\"dashed\")+\n  annotate(\"label\", x = MAE_xG_test, y = 1.1, label = MAE_xG_test,\n           fill =\"#fa9f99\" )+\n  annotate(\"label\", x = MAE_xG_train, y = 1.1, label = MAE_xG_train,\n           fill =\"#48cdd1\")+\n  theme(\n    plot.title = element_markdown(lineheight = 1.1),\n    legend.text = element_markdown(size = 11)\n  )"
  },
  {
    "objectID": "module4.html#being-fairer-on-xgboost",
    "href": "module4.html#being-fairer-on-xgboost",
    "title": "5  Module 4",
    "section": "5.11 Being fairer on XGboost",
    "text": "5.11 Being fairer on XGboost\nXGboost is a highly per formative algorithm but it does require some tuning to really extract the best performance. Above I used settings that I knew were likely to over-fit the data so lets say how it goes when I use a range of different tuning metrics please note for the sake of time I haven;t done a full grid search of parameter values. As if you were to do 5 fold CV on a full grid search this could take up to 30 minutes in some cases.\n\ntrain.data$Shot_outcome_F &lt;-as.factor(train.data$Shot_outcome)\n\nxgb_tree &lt;- train(Shot_outcome_F~Rotated.xStd+Rotated.yStd+shots.details.type,\n                 data = train.data,\n                 method = \"xgbTree\",\n                 objective = \"binary:logistic\",\n                 trControl = trainControl(method = \"cv\",\n                                          number = 5,\nrepeats = 2,\n                                          verboseIter = F),\n                 tuneGrid = expand.grid(nrounds = c(300,500,800),\n                                        eta =     c(0.01,0.05,0.1,0.2),\n                                        max_depth = c(3,4,5),\n                                        colsample_bytree = c(.9),\n                                        subsample = c(.7),\n                                        gamma = c(.2),\n                                        min_child_weight = c(15)))\n\n[21:33:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:59] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:33:59] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:01] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:01] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:03] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:03] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:06] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:06] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:08] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:08] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:09] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:09] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:11] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:11] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:12] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:12] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:14] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:14] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:34:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:35:00] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:35:00] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:35:01] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:35:01] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:35:03] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:35:03] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:35:05] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:35:05] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:35:06] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:35:06] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:35:08] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:35:08] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:35:10] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:35:10] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:35:11] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:35:11] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:35:12] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:35:12] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:35:14] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[21:35:14] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n\n\nPlease note the above took about 25 minutes to run on my computer which is decently powered.\n\n## THE BEST TUNE\n\nxgb_tree$bestTune\n\n  nrounds max_depth  eta gamma colsample_bytree min_child_weight subsample\n8     500         5 0.01   0.2              0.9               15       0.7\n\n\n\nres &lt;- xgb_tree$results\n\nggplot(res,aes(nrounds,Accuracy,col=as.factor(eta)))+\n  geom_line()+\n  geom_point()+\n  facet_wrap(~max_depth)+\n  theme_minimal()\n\n\n\n\n\nxgb_fin &lt;- train(Shot_outcome_F~Rotated.xStd+Rotated.yStd+shots.details.type,\n                  data = train.data,\n                  method = \"xgbTree\",\n                  objective = \"binary:logistic\",\n          #        trControl = trainControl(method = \"cv\",\n           #                                number = 5,\n                                           #repeats = 2,\n          #                                 verboseIter = F),\n                  tuneGrid = expand.grid(nrounds = c(500),         \n                                         eta =     c(0.01),        \n                                         max_depth = c(4),            \n                                         colsample_bytree = c(.9),   \n                                         subsample = c(.5),         \n                                         gamma = c(.2),                     \n                                         min_child_weight = c(15))) \n\n\ntrain_pred_xg &lt;- predict(xgb_fin,train.data,type=\"prob\")[,2]\ntest_pred_xg &lt;- predict(xgb_fin, test.data,type=\"prob\")[,2]\n\ntest.data$Shot_outcome_F&lt;-as.factor(test.data$Shot_outcome)\n\n\ntrain.data_XGtune &lt;- train.data|&gt;\n  mutate(xG_tuned_pred = train_pred_xg*6,\n         xG_Xp_err = round(shots.result.points-xG_tuned_pred,3),\n         xG_abs_err = abs(xG_Xp_err),\n         split = \"Train\")\n\nMAE_xG_train_tune = round(mean(abs(train.data_XGtune$xG_Xp_err)),2)\n\ntest.data_XGtune &lt;- test.data|&gt;\n  mutate(xG_tuned_pred = test_pred_xg*6,\n         xG_Xp_err = round(shots.result.points-xG_tuned_pred,3),\n         xG_abs_err = abs(xG_Xp_err),\n         split = \"Test\")\n\n\n\nMAE_xG_test_tune  = round(mean(abs(test.data_XGtune$xG_Xp_err)),2)\n\n\ncomb_XGT &lt;- rbind(train.data_XGtune,test.data_XGtune)\n\nggplot(comb_XGT,aes(xG_abs_err,fill=split))+\n  geom_density(alpha=.5)+\n   labs(title = glue(\"&lt;b&gt;xgBoost results tuned with CV&lt;/b&gt;&lt;br&gt;\"))+\n            #     &lt;span style='color:#48cdd1;'&gt;train: {MAE_xG_train}&lt;/span&gt;,\n            #     &lt;span style='color:#fa9f99;'&gt;test: {MAE_xG_test}&lt;/span&gt;\"))+\n  theme_minimal() +\n  geom_vline(xintercept = c(MAE_xG_train_tune),col=\"#48cdd1\",linetype=\"dashed\")+\n  geom_vline(xintercept = c(MAE_xG_test_tune),col=\"#fa9f99\",linetype=\"dashed\")+\n\n  annotate(\"label\", x = MAE_xG_train_tune, y = 1.1, label = MAE_xG_train_tune,\n           fill =\"#48cdd1\")+\n    annotate(\"label\", x = MAE_xG_test_tune, y = 1.1, label = MAE_xG_test_tune,\n           fill =\"#fa9f99\" )+\n  theme(\n    plot.title = element_markdown(lineheight = 1.1),\n    legend.text = element_markdown(size = 11)\n  )\n\n\n\n\nOkay, with a better tune we see slightly better performance than the previous model but importantly we can probably fill a little more confident with out predictions as the Train and Test performance are closer so we can be a little less concerned about the model being over-fit to the training data. The consistency was improved because within the “train” function we implemented cross-fold validation. We will talk more about this next week."
  },
  {
    "objectID": "module5.html#recap-from-last-module",
    "href": "module5.html#recap-from-last-module",
    "title": "6  Module 5",
    "section": "6.2 Recap from last module",
    "text": "6.2 Recap from last module\nIn our last module, we explored commonly used machine learning algorithms, focusing on Decision Trees and Random Forests. We discussed how Decision Trees offer good interpretability but have limitations in performance. We then examined how Random Forests attempt to overcome these limitations, often achieving good out-of-the-bag performance with minimal tuning.\nHowever, we encountered a crucial insight: even powerful algorithms like Random Forests can sometimes overfit the data. We demonstrated this by comparing the Random Forest to a Generalized Additive Model (GAM), using the technique of splitting our data into training and test sets. This approach allowed us to train models on one subset of data and evaluate their performance on unseen data, revealing potential overfitting issues.\nThis week, we’ll delve deeper into strategies for mitigating overfitting risks and assessing model performance. We’ll explore:\n\nCross-validation: We briefly mentioned this technique last week as a method to help prevent overfitting. We’ll now examine it in more detail, understanding how it provides a more robust evaluation of model performance.\nModel Performance Metrics: We’ll cover common statistics used to assess model accuracy for both classification and regression problems. For binary classification (e.g., Goal/Miss), we’ll look at metrics such as accuracy, precision, recall, and F1-score. For regression problems with continuous outcomes, we’ll explore metrics like Root Mean Square Error (RMSE) and R-squared.\nBias-Variance relationship: Introduce the concept of the Bias variance relationship.\n\nTime permitting, we’ll tie these concepts together by exploring penalized regression, an algorithm that inherently incorporates feature selection and model tuning. This will provide a practical example of how these elements work together in advanced machine learning techniques.\nLearning Objectives: By the end of this module, you should be able to:\n\nUnderstand and apply cross-validation techniques\nCalculate and interpret common performance metrics for classification and regression models\nRecognize the importance of feature selection and apply basic selection techniques\nUnderstand the concept of model tuning and its role in improving model performance\n\nPlease note that assessing model performance is a vast area in statistics and machine learning. While we can’t cover all existing metrics and techniques, this module will equip you with a solid foundation in the most commonly used and practical approaches."
  },
  {
    "objectID": "module5.html#goals-for-this-week.",
    "href": "module5.html#goals-for-this-week.",
    "title": "6  Module 5",
    "section": "6.1 Goals for this week.",
    "text": "6.1 Goals for this week.\nOur current outline for the next out weeks looks like the below.\n\nIntroduction into R programming language and getting started with some descriptive statistics.\nUnderstanding key terms; Exploratory analysis, Supervised vs Unsupervised problems, regression vs classification, prediction vs association.\nA quick note on distributions - Supervised regression and classification problems using a generalized linear model framework. (Hypothesis testing)\nMachine learning approach to Supervised learning\nMethods of assessing model accuracy, feature selection and model tuning.\nUnsupervised learning strategies2 (market basket analysis, network analysis)\nCreating a machine learning pipeline"
  },
  {
    "objectID": "module5.html#load-data-and-r-packages",
    "href": "module5.html#load-data-and-r-packages",
    "title": "6  Module 5",
    "section": "6.3 Load data and R packages",
    "text": "6.3 Load data and R packages\n\n# https://gavinsimpson.github.io/gratia/reference/data_sim.html\n\npacman::p_load(data.table,tidyverse,caret,mgcv,ggtext,FactoMineR,factoextra,\n               pdp,ranger,glue,patchwork,glmnet,CalibrationCurves,DescTools)\n\nurlfile=\"https://raw.githubusercontent.com/R2mu/GWS_DSPR/main/data/mod5.csv\"\n\n\nmod5_dat &lt;- fread(urlfile)\n\nThis data set represents Team Data by QUARTER. Have a think about what R functions you could use to start to feel comfortable with the data set.\n\n## drop columns that obviously related to the outcome\nmod5_dat&lt;-mod5_dat[,!c(\"BEHIND\",\"POINTS\",\"GOAL\",\"SHOT_AT_GOAL\",\"HANDBALL_GAIN_METRES\",\n                       \"SCORE_INVOLVEMENT\",\"SCOREBOARD_IMPACT\",\"SCORE_LAUNCH\",\n                       \"INSIDE_50\")\n  ][, .SD, .SDcols = !patterns(\"PCNT\")\n  ][, .SD, .SDcols = !patterns(\"IN50\")]\n\n## drop columns with highly negatively skewed values \nis0 = function(x)is.numeric(x) &&  quantile(x, 0.25,na.rm =T) == 0\n\nm5_dat &lt;-copy(mod5_dat)[, which(sapply(mod5_dat, is0)) := NULL]\n\n##log transform to help normalize variables later on\nm5_log &lt;-copy(m5_dat)[, names(m5_dat)[8:94] := lapply(.SD, function(x) log(x + 1)), .SDcols = 8:94]\n\nm5_scaled &lt;- copy(m5_log)[, names(m5_log)[8:94] := lapply(.SD, scale), .SDcols = 8:94]\n\nBelow are some links behind the idea of what we are going to do.\nhttps://mlu-explain.github.io/train-test-validation/\nhttps://mlu-explain.github.io/cross-validation/"
  },
  {
    "objectID": "module5.html#data-splitting",
    "href": "module5.html#data-splitting",
    "title": "6  Module 5",
    "section": "6.4 Data splitting",
    "text": "6.4 Data splitting\nGiven the size of most of your data sets you should feel relatively confident in always creating a training and a test data set. Once you have created these data sets, it is important you put you only end up using the test data set at the end, once you have completed fine tuning your models. There are many R packages that will do data splitting for you but it is quite easy to do using base R.\n\nset.seed(123)  # for reproducibility\n\nsplit &lt;- 0.7 ## proportion of rows for training data set \n\ntrain &lt;- sample(nrow(m5_scaled), split * nrow(m5_scaled))  # randomly sample 70% of row indices from data set. \ntest  &lt;- setdiff(1:nrow(m5_scaled), train)\n# get remainder of 30% of rows not in training dataset. \n\ntrain_data &lt;- m5_scaled[train, ]# create training dataset\ntest_data  &lt;- m5_scaled[test, ]# create test dataset\n\nWe now have a data-set called train_data with 2066 rows and a data-set called test_data with 866 rows in it.\nAs I mentioned earlier, we will now effectively forget about the test data-set until later and just work with the training data-set."
  },
  {
    "objectID": "module5.html#assessing-continuous-responses",
    "href": "module5.html#assessing-continuous-responses",
    "title": "6  Module 5",
    "section": "6.5 Assessing continuous responses",
    "text": "6.5 Assessing continuous responses\nFor our first model we are going to assess a continuous outcome which is going to be quarter differential. The most common metrics used when assessing a models ability to describe a continuous outcome variable are\nMean Absolute Error (MAE) : This simply represents the\n\nmean(abs((actual - predicted)))\nMean Squared Error (MSE) : Very similar to above but instead of taking the absolute of the difference between variables we are going to square the distance. This will penalize poor predictions more than the MAE\n\nmean((actual - predicted)^2)\nRoot Mean Squared Error (RMSE) : Similar to above but we now just going to sqrt the MSE to get it back to the same level of the outcome measure.\nsqrt(mean((actual - predicted)^2)) or sqrt(MSE)\nR2: This represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It ranges from 0 to 1, where 1 indicates perfect prediction.\n1 - (sum((actual - predicted)^2) / sum((actual - mean(actual))^2))\nAdjusted R Squared:This is a modified version of R-squared that adjusts for the number of predictors in the model. It increases only if the new term improves the model more than would be expected by chance.\n1 - ((1 - R²) * (n - 1) / (n - p - 1))\nwhere n is the number of observations and p is the number of predictors.\nNotes:\n\nR-squared can be interpreted as the percentage of variance explained by the model.\nAdjusted R-squared is always lower than R-squared and can be negative.\nWhile R-squared always increases when adding more variables to the model (even if they’re not meaningful), Adjusted R-squared penalizes the addition of variables that don’t improve the model’s explanatory power.\n\nThese metrics provide different perspectives on model performance. MAE and RMSE are in the same units as the outcome variable, making them easily interpretable. R-squared and Adjusted R-squared give a sense of the model’s overall explanatory power. The choice of which metric to prioritize often depends on the specific needs of your analysis and the characteristics of your data.\nNow often you might calculate all of these on both the training data-set and test data-set, as it could give insight into possible over-fitting and also can give insight into the BIAS-VARIANCE trade off.\nhttps://mlu-explain.github.io/bias-variance/\nhttps://illustrated-machine-learning.github.io/#/machine-learning/bias-variance\n\n\nI encourage you to have a read through the websites above. But effectively all variables have some noise component of noise to them, as we continually add more and more variables we are effectively adding some noise to the model as well. Initially, important variables will reduce error, but we may get to a stage where we are just adding noise with no improvements in model accuracy."
  },
  {
    "objectID": "module5.html#simple-example",
    "href": "module5.html#simple-example",
    "title": "6  Module 5",
    "section": "6.6 Simple example",
    "text": "6.6 Simple example\n\nm5_small &lt;- train_data[,.(Diff.Qtr.Points,CLEARANCE,CONTESTED_POSSESSION,CONTESTED_MARK,CRUMB,EFFECTIVE_DISPOSAL,INTERCEPT,HITOUT,KICK_GAIN_METRES,KICK,MARK,LOOSE_BALL_GET,TACKLE,REBOUND_50,METRES_GAINED_EFF,PLY_PRESS_PTS,\nSPOIL,RATING,LONG_KICK,SHORT_KICK,FIRST_POSSESSION)]\n\nm5_small_tst &lt;- test_data[,.(Diff.Qtr.Points,CLEARANCE,CONTESTED_POSSESSION,CONTESTED_MARK,CRUMB,EFFECTIVE_DISPOSAL,INTERCEPT,HITOUT,KICK_GAIN_METRES,KICK,MARK,LOOSE_BALL_GET,TACKLE,REBOUND_50,METRES_GAINED_EFF,PLY_PRESS_PTS,\nSPOIL,RATING,LONG_KICK,SHORT_KICK,FIRST_POSSESSION)]\n\n\nset.seed(123)\n# Set up repeated k-fold cross-validation\ntrain.control &lt;- trainControl(method = \"cv\", number = 10)\n# Train the model\nstep.model &lt;- train(Diff.Qtr.Points ~., data = m5_small,\n                    method = \"leapBackward\", \n                    tuneGrid = data.frame(nvmax = 1:18),\n                    trControl = train.control\n                    )\n\n\n\nbwelim &lt;- data.table(step.model$results)\n\n\nggplot(bwelim,aes(nvmax,RMSE,ymin=RMSE-RMSESD,ymax=RMSE+RMSESD))+\n  geom_pointrange()+\n  theme_bw()\n\n\n\n#plot(step.model)\n\nFrom eyeballing we can that from around 8 variables in performance doesn’t really seem to change much. Lets have a look at the coefs. Of quite note, these coefficients have been scaled on log transformed data so they don’t have direct interpretation for absolute values, but you can still look interpret them as absolute magnitude of number being indicative of importance. .\n\nround(coef(step.model$finalModel, 8),2)\n\n       (Intercept) EFFECTIVE_DISPOSAL               KICK             TACKLE \n              0.18              -2.14               4.45               4.11 \n        REBOUND_50  METRES_GAINED_EFF      PLY_PRESS_PTS              SPOIL \n             -1.16               3.27              -5.11              -0.94 \n            RATING \n              9.25 \n\n\nYou may notice some weird directions for variables e.g. (“EFFECTIVE_DISPOSAL”,“PLY_PRESS_PTS”) , this likely due to the fact that some of the variables are highly correlated with each other, which can cause flipping of effects in simple regression problems. For example, see below. Whilst, the LM model has not really done anything wrong, its more a problem with us the user having to spend a lot of mental energy contextualizing what the effects actually mean. We will explore some other techniques latter (LASSO) that can help a little in situations where you might expect many variables to be highly correlated.\n\neg1 = ggplot(train_data,aes(TACKLE,PLY_PRESS_PTS))+geom_point()+stat_smooth(method = \"lm\")\neg2 = ggplot(train_data,aes(EFFECTIVE_DISPOSAL,METRES_GAINED_EFF))+geom_point()+stat_smooth(method = \"lm\")\n\neg0 &lt;- ggplot(train_data,aes(PLY_PRESS_PTS,Diff.Qtr.Points))+geom_point()+stat_smooth(method = \"lm\")\n\nggpubr::ggarrange(eg0,eg1,eg2,nrow = 1)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nOr it could there be another reason???\n\nnvar =step.model$finalModel$nvmax-1\n\nresults_df &lt;- data.frame(i = integer(), RMSE = numeric(),r2 = numeric(),adjr2 = numeric())\n\nfor (i in 1:nvar) {\n  \nip1 &lt;- i+1\n\nform1&lt;-as.formula(paste(\"Diff.Qtr.Points~\", \n                        paste(names(coef(step.model$finalModel,i)[2: ip1]),\n                              collapse = \"+\")))\n\nlmTest &lt;- lm(form1,data = train_data)\n\n\n\nrmse &lt;-round(caret::RMSE(predict(lmTest,test_data),test_data$Diff.Qtr.Points),2)\n\nresults_df &lt;- rbind(results_df, data.frame(i = i, RMSE = rmse,\n                                           r2 =summary(lmTest)$r.squared,\n                                           adjr2 = summary(lmTest)$adj.r.squared))\n\n}\n\n\ng1=ggplot()+\n  geom_pointrange(bwelim[nvmax&lt;=13,],mapping =aes(nvmax,RMSE,ymin=RMSE-RMSESD,ymax=RMSE+RMSESD),\n                  col=\"gray\")+\n  geom_point(results_df,mapping=aes(i,RMSE),col=\"red\")+\n  geom_path(results_df,mapping=aes(i,RMSE),col=\"red\")+\n  theme_bw()\n\ng2 = ggplot(results_df,aes(i,r2))+geom_path()+\n  geom_path(aes(i,adjr2),col=\"red\")+\n  theme_bw()\n\nggpubr::ggarrange(g1,g2)\n\n\n\n\nKey points above.\n1) Test performance still worse than cross-validated performance, but still follow a relatively similar shape, which is good to see and could be argued test performance is technically within CV performance range.\n2) A little harder to see but adjusted r2 (in red) gets lower as we add more values when compared to just the r2 value."
  },
  {
    "objectID": "module5.html#assessing-binomiallogistic-models",
    "href": "module5.html#assessing-binomiallogistic-models",
    "title": "6  Module 5",
    "section": "6.7 Assessing Binomial/logistic models",
    "text": "6.7 Assessing Binomial/logistic models\nWe will now explore how we go about assessing binary responses, e.g., yes vs. no, goal vs. miss. While some of the steps and statistics will be quite similar to those used for continuous outcomes, assessing binomial outcomes is usually more extensive. There are good reasons for this complexity:\n\nDecision-making: Many real-world situations, particularly in medicine, require individuals to make dichotomous decisions based on probabilistic information.\nRisk thresholds: Different individuals or contexts may have different thresholds for what constitutes an acceptable level of risk.\nCompeting risks: In many scenarios, the decision to act or not act comes with its own set of risks and benefits that need to be weighed against each other.\nProbability interpretation: Understanding and accurately interpreting probabilities can be challenging for many people.\n\nAs a result, statisticians and researchers have proposed many different metrics that aim to provide various perspectives on model performance. These metrics help contextualize the results, aiding people in ultimately making decisions based on what is fundamentally a probability output.\nSome key metrics we’ll explore include:\n\nAccuracy: The overall correct classification rate.\nSensitivity (Recall): The true positive rate; Proportion of actual positives correctly identified. Usually care about this when the cost of a false negative is high.\nSpecificity: The true negative rate; Proportion of negatives correctly identified.\nPrecision: The positive predictive value; Proportion of positive identification were actually correct. Usually care about this when the cost of a false positive is high.\nF1 Score: The harmonic mean of precision and recall.\nROC curve and AUC/ C statistic: A graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\nConfusion Matrix: A table layout of correct and incorrect classifications.\nBrier Score: A measure of the mean squared difference between the predicted probability and the actual outcome. Lower scores indicate better calibration of probability estimates.\nCalibration statistics: Measures how well the predicted probabilities of a model match the observed frequencies. This can include calibration plots, and other metrics that assess the reliability of probability estimates.\n\nEach of these metrics provides a different insight into model performance, and the choice of which to prioritize often depends on the specific context of the problem at hand. (There are other common statistics that are also suggested that aren’t mentioned above)\nhttps://illustrated-machine-learning.github.io/#/machine-learning/metrics\nhttps://www.researchgate.net/publication/344419928_Introduction_to_Machine_Learning_Neural_Networks_and_Deep_Learning"
  },
  {
    "objectID": "module5.html#binomial-example",
    "href": "module5.html#binomial-example",
    "title": "6  Module 5",
    "section": "6.8 Binomial example",
    "text": "6.8 Binomial example\nBelow we will use a penalized regression model to predict Wins vs Losses over a quarter. Penalized regression can be thought of as similar to a linear model, but with a penalty applied to the size of the coefficients. There are various ways this penalty can be applied, but here we’ll explore the L1 penalty, also known as LASSO (Least Absolute Shrinkage and Selection Operator) regression.\nThe LASSO penalty penalizes the absolute sum of all the coefficients. It has the added benefit of potentially shrinking parameters that don’t contribute significantly to model performance to exactly zero, which can aid in feature selection.\nMathematically, the LASSO objective function looks like this:\nLASSO Objective = (Sum of Squared Errors) + λ * (Sum of |β|)\nWhere:\n\n“Sum of Squared Errors” is the standard objective of ordinary least squares regression\nλ (lambda) is the tuning parameter that controls the strength of the penalty\n“Sum of |β|” is the sum of the absolute values of the coefficients\n\nThe first term in the equation is the usual sum of squared residuals that we seek to minimize in ordinary least squares regression. The second term, is the LASSO penalty.\nKey points:\n\nWhen λ = 0, the penalty term has no effect, and we get the same estimates as ordinary least squares.\nAs λ increases, the penalty becomes stronger, and more coefficients are shrunk towards zero.\nSome coefficients may be shrunk exactly to zero, effectively removing those predictors from the model.\nThe λ parameter is typically chosen through cross-validation to find the value that minimizes prediction error.\n\nLASSO is particularly useful when dealing with many predictors, especially when some may be correlated, as it performs both regularization (to prevent overfitting) and feature selection simultaneously.\nIn the literature, the LASSO (and other variants of penalized regression) tend to perform quite well in a range of scenarios, even when compared to more advanced technique.\n\nm5binom&lt;- m5_small[, QWL:=as.factor(ifelse(Diff.Qtr.Points&gt;0,1,0))\n                  ]#[,Diff.Qtr.Points:=NULL]\n\nm5binom_Tst&lt;- m5_small_tst[, QWL:=as.factor(ifelse(Diff.Qtr.Points&gt;0,1,0))\n                  ]#\n\n\nx     &lt;- as.matrix(m5binom[,2:21])\nx_tst &lt;- as.matrix(m5binom_Tst[,2:21])\ny     &lt;- m5binom$QWL\ny_tst &lt;- m5binom_Tst$QWL\n\nfit1 &lt;- cv.glmnet(x,y,family = \"binomial\",\n                  nfolds = 10,\n                  type.measure = \"auc\",\n                  alpha =1,\n                  keep = T,\n                  scale =F,intercept =F)\n\nplot(fit1)\n\n\n\nfin_fint  &lt;- glmnet(x,y,family = \"binomial\",\n                    lambda =fit1$lambda.1se,\n                    alpha =1,\n                    type.measure = \"auc\",\n                    scale =F,intercept = F)\n\n\n\ncoef(fit1, s = \"lambda.1se\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                               s1\n(Intercept)           .          \nCLEARANCE             .          \nCONTESTED_POSSESSION  .          \nCONTESTED_MARK        0.024550364\nCRUMB                 .          \nEFFECTIVE_DISPOSAL    .          \nINTERCEPT             .          \nHITOUT                .          \nKICK_GAIN_METRES      0.320804869\nKICK                  0.346023235\nMARK                  0.007240451\nLOOSE_BALL_GET       -0.032812210\nTACKLE                0.265107376\nREBOUND_50           -0.117526801\nMETRES_GAINED_EFF     0.199013932\nPLY_PRESS_PTS        -0.421942735\nSPOIL                -0.066702894\nRATING                1.314397272\nLONG_KICK             .          \nSHORT_KICK            .          \nFIRST_POSSESSION      .          \n\n\n\nassess.glmnet(fin_fint, newx = x_tst, newy = y_tst,family = \"binomial\")\n\n$deviance\n       s0 \n0.8440399 \nattr(,\"measure\")\n[1] \"Binomial Deviance\"\n\n$class\n       s0 \n0.2042889 \nattr(,\"measure\")\n[1] \"Misclassification Error\"\n\n$auc\n[1] 0.8921874\nattr(,\"measure\")\n[1] \"AUC\"\n\n$mse\n       s0 \n0.2722072 \nattr(,\"measure\")\n[1] \"Mean-Squared Error\"\n\n$mae\n       s0 \n0.5887983 \nattr(,\"measure\")\n[1] \"Mean Absolute Error\"\n\nrocs &lt;- roc.glmnet(fit1$fit.preval, newy = y)\n\nbest &lt;-fit1$index[\"1se\",]\nplot(rocs[[best]], type = \"l\")\ninvisible(sapply(rocs, lines, col=\"grey\"))\nlines(rocs[[best]], lwd = 2,col = \"red\")\nabline(a=0,b=1,lwd=1)\n\n\n\n\nTo understand the AUC measure of 0.89 it effectively means that if we randomly selected a goal or a miss there is a 0.89% chance that the model would give a higher probability of goal to the goal case. So in a manner this number represents how well our model does at discriminating between cases.\nThe AUC of the ROC curve is a useful statistic for evaluating model performance. However, in situations with high class imbalance, such as injury prediction where injuries are rare events, it can suggest overly optimistic model performance. This is particularly problematic because most injury models have relatively poor sensitivity (ability to correctly identify actual injuries). The AUC ROC might still appear high due to the model’s ability to correctly classify the numerous non-injury cases, even when its performance in detecting the rare injury cases is limited. This can lead to an overly optimistic interpretation of the model’s practical utility in identifying injuries. For cases like this, a precision-recall graph (which has a higher focus on the minority class) can be useful along with analyzing F1 scores, balanced Accuracy and Cohens Kappa).\nWe will use a helpful function from the Caret package which has many of these additional statistics. https://rdrr.io/cran/caret/man/confusionMatrix.html,\nWhilst, this is very helpful, be sure to read the documentation as it defaults to the first class being the positive class e.g. 0 being considered Goal where we obviously want the opposite. (This confused me for like 2 days for a study I did a while a go)\n\nconfusionMatrix(y_tst,\n                as.factor(predict(fin_fint,newx = x_tst,type = \"class\")),\n                positive = \"1\",\n                mode = \"everything\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 333 109\n         1  72 372\n                                          \n               Accuracy : 0.7957          \n                 95% CI : (0.7676, 0.8218)\n    No Information Rate : 0.5429          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.5913          \n                                          \n Mcnemar's Test P-Value : 0.007454        \n                                          \n            Sensitivity : 0.7734          \n            Specificity : 0.8222          \n         Pos Pred Value : 0.8378          \n         Neg Pred Value : 0.7534          \n              Precision : 0.8378          \n                 Recall : 0.7734          \n                     F1 : 0.8043          \n             Prevalence : 0.5429          \n         Detection Rate : 0.4199          \n   Detection Prevalence : 0.5011          \n      Balanced Accuracy : 0.7978          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nBrier Score\n\nBrierScore(predict(fin_fint,newx = x_tst,type = \"response\"),as.numeric(y_tst)-1)\n\n[1] 0.2943991\n\n\nCalibration Curve\n\ncalPerf = val.prob.ci.2(predict(fin_fint,newx = x_tst,type = \"response\"), as.numeric(y_tst)-1)"
  },
  {
    "objectID": "module5.html#feature-selection---l1",
    "href": "module5.html#feature-selection---l1",
    "title": "6  Module 5",
    "section": "6.9 Feature selection - L1",
    "text": "6.9 Feature selection - L1\n\nm5binom&lt;- m5_small[, QWL:=as.factor(ifelse(Diff.Qtr.Points&gt;0,1,0))\n                  ]#[,Diff.Qtr.Points:=NULL]\n\nm5binom_Tst&lt;- m5_small_tst[, QWL:=as.factor(ifelse(Diff.Qtr.Points&gt;0,1,0))\n                  ]#\n\n\nx     &lt;- as.matrix(m5binom[,2:21])\nx_tst &lt;- as.matrix(m5binom_Tst[,2:21])\ny     &lt;- m5binom$Diff.Qtr.Points\ny_tst &lt;- m5binom_Tst$Diff.Qtr.Points\n\nfit2 &lt;- cv.glmnet(x,y,\n                  #family = \"binomial\",\n                  nfolds = 10,\n                  type.measure = \"mae\",\n                  alpha =1,\n                  keep = T,\n                  scale =F,intercept =F)\n\nplot(fit2)\n\n\n\n#fin_fint  &lt;- glmnet(x,y,family = \"binomial\",\n#                    lambda =fit1$lambda.1se,\n#                    alpha =1,\n#                    type.measure = \"auc\",\n#                    scale =F,intercept = F)\n\n\ncoef(fit2, s = \"lambda.1se\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                              s1\n(Intercept)           .         \nCLEARANCE             .         \nCONTESTED_POSSESSION  .         \nCONTESTED_MARK        0.12882920\nCRUMB                 .         \nEFFECTIVE_DISPOSAL    .         \nINTERCEPT             .         \nHITOUT                .         \nKICK_GAIN_METRES      1.96561681\nKICK                  2.51050179\nMARK                  .         \nLOOSE_BALL_GET       -0.13742292\nTACKLE                2.94222473\nREBOUND_50           -1.06358316\nMETRES_GAINED_EFF     1.82694544\nPLY_PRESS_PTS        -3.76403791\nSPOIL                -0.67890923\nRATING                8.27345169\nLONG_KICK             .         \nSHORT_KICK            0.02990906\nFIRST_POSSESSION      .         \n\n\nWhen attempting to utilise feature selection algorithms, I usually suggest penalization methods such as LASSO (Least Absolute Shrinkage and Selection Operator). These techniques offer several advantages over alternative methods, particularly in handling multicollinearity and producing sparse, interpretable models. However, it’s important to acknowledge that penalization approaches are not without limitations.\nOne key consideration is that penalized models may yield coefficient estimates that differ from what we might expect based on univariate analyses. This divergence highlights the complex, multidimensional nature of relationships within our data and underscores the importance of a comprehensive approach to feature selection.\nTo enhance model interpretability and stability, it’s crucial to identify and potentially remove redundant variables or those that are nearly perfectly explained by a combination of other variables. Such variables can introduce confounding effects and obscure the interpretability of our model. This process often requires a balance between statistical techniques and domain expertise.\nTo address these challenges, we’ll explore unsupervised learning approaches to identify variables that are highly related in multidimensional space. Techniques such as Principal Component Analysis (PCA), t-SNE (t-Distributed Stochastic Neighbor Embedding), or UMAP (Uniform Manifold Approximation and Projection) can reveal non-obvious relationships between variables that might not be apparent from simple correlation matrices.\nThe data preprocessing steps we took earlier, including log transformation and scaling, are particularly relevant for these unsupervised methods. By transforming variables from different distributions to a roughly consistent log-normal distribution, we create a more suitable input for many unsupervised algorithms, which often assume normally distributed data.\nIt’s important to note that feature selection is typically an iterative process. We may cycle between penalization methods, unsupervised techniques, and domain knowledge-based selection to refine our feature set. Throughout this process, we’ll need to balance model complexity (number of features) with performance, considering the bias-variance tradeoff"
  },
  {
    "objectID": "module5.html#feature-selection---alternate-methods",
    "href": "module5.html#feature-selection---alternate-methods",
    "title": "6  Module 5",
    "section": "6.10 Feature selection - Alternate methods",
    "text": "6.10 Feature selection - Alternate methods\nProbably one of the most commonly used algorithms for reducing the amount of variables is principle component analysis (PCA). I won;t get into too much detail into the algorithm as there are many resources available online that do a great job at explaining what PCA is trying to achieve.\n\ndata.pca &lt;- princomp(x)\n\nfactoextra::fviz_eig(data.pca, addlabels = TRUE)\n\n\n\n\n\nround(data.pca$loadings[, 1:4],3)\n\n                     Comp.1 Comp.2 Comp.3 Comp.4\nCLEARANCE             0.064  0.331  0.415  0.097\nCONTESTED_POSSESSION  0.224  0.364  0.003  0.175\nCONTESTED_MARK        0.133 -0.067  0.047  0.097\nCRUMB                 0.150  0.151 -0.279  0.276\nEFFECTIVE_DISPOSAL    0.352 -0.130  0.094 -0.082\nINTERCEPT             0.206  0.097 -0.385 -0.023\nHITOUT               -0.045  0.221  0.237 -0.186\nKICK_GAIN_METRES      0.380  0.072 -0.077  0.021\nKICK                  0.381 -0.118  0.048 -0.119\nMARK                  0.239 -0.357  0.112 -0.173\nLOOSE_BALL_GET        0.166  0.309 -0.169  0.202\nTACKLE               -0.032  0.280  0.024 -0.575\nREBOUND_50           -0.038 -0.011 -0.386 -0.146\nMETRES_GAINED_EFF     0.348 -0.028 -0.040 -0.077\nPLY_PRESS_PTS        -0.055  0.257 -0.144 -0.594\nSPOIL                 0.016  0.176 -0.295 -0.005\nRATING                0.345  0.132  0.134 -0.029\nLONG_KICK             0.221  0.120 -0.120 -0.028\nSHORT_KICK            0.259 -0.317  0.162 -0.172\nFIRST_POSSESSION      0.031  0.324  0.408  0.076\n\n\n\nfactoextra::fviz_pca_var(data.pca, col.var = \"black\")\n\n\n\n\n\nfactoextra::fviz_cos2(data.pca, choice = \"var\", axes = 1)\n\n\n\n\n\nfactoextra::fviz_pca_var(data.pca, col.var = \"cos2\",\n            gradient.cols = c(\"black\", \"orange\", \"green\"),\n            repel = TRUE)\n\n\n\n\n\n6.10.0.1 Sparce PCA\nhttps://mixomicsteam.github.io/mixOmics-Vignette/index.html\n\nlibrary(mixOmics)\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\n\nLoaded mixOmics 6.26.0\nThank you for using mixOmics!\nTutorials: http://mixomics.org\nBookdown vignette: https://mixomicsteam.github.io/Bookdown\nQuestions, issues: Follow the prompts at http://mixomics.org/contact-us\nCite us:  citation('mixOmics')\n\n\n\nAttaching package: 'mixOmics'\n\n\nThe following objects are masked from 'package:caret':\n\n    nearZeroVar, plsda, splsda\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\ngrid.keepX &lt;- c(seq(5, 20, 5))\n\nset.seed(30) # For reproducibility\ntune.spca.result &lt;- tune.spca(x,\n                              ncomp = 5, \n                              folds = 5, \n                              test.keepX = grid.keepX, nrepeat = 10) \n\nplot(tune.spca.result)\n\n\n\n\n\nkeepX.select &lt;- tune.spca.result$choice.keepX[1:3]\n\nsparse_res &lt;-mixOmics::spca(x,\n                            ncomp = 3, \n                            #keepX=c(4,4,4),\n                            center = F,scale = F,\n                            keepX = keepX.select)\n\nplotVar(sparse_res,cex = 3) \n\n\n\n\n\nplotLoadings(sparse_res, comp = 1)\n\n\n\nplotLoadings(sparse_res, comp = 2)\n\n\n\nplotLoadings(sparse_res, comp = 3)\n\n\n\n#par(mfrow = c(1, 1))"
  },
  {
    "objectID": "module5.html#feature-selection---gam",
    "href": "module5.html#feature-selection---gam",
    "title": "6  Module 5",
    "section": "6.11 Feature selection - GAM",
    "text": "6.11 Feature selection - GAM\nTo finish of with we will also explore feature selection utilizing my modelling package, gams withing the MGCV package.\n\nm5_raw &lt;- m5_dat[,.(Diff.Qtr.Points,CLEARANCE,CONTESTED_POSSESSION,CONTESTED_MARK,CRUMB,EFFECTIVE_DISPOSAL,INTERCEPT,HITOUT,KICK_GAIN_METRES,KICK,MARK,LOOSE_BALL_GET,TACKLE,REBOUND_50,METRES_GAINED_EFF,PLY_PRESS_PTS,\nSPOIL,RATING,\nLONG_KICK,SHORT_KICK,FIRST_POSSESSION)]\n\npredictors &lt;- names(m5_raw[,2:20])\n\n\nsmooth_terms &lt;- paste0(\"s(\", predictors, \")\", collapse = \" + \")\n\n## could add random effects to this as well if you wanted eg\n# s(SQUAD_NAME,bs=\"re\")\n\nformula &lt;- reformulate(c(smooth_terms), response = \"Diff.Qtr.Points\")\n\n\n\nm1_gam &lt;- bam(formula, \n          data = m5_raw,\n          discrete = T,\n          select = T,\n          method = \"fREML\")\n\nsm_gam &lt;-summary(m1_gam)\n\n## only keey relatively high F values, could go based of pvalues as well\nround(sm_gam$s.table,2)|&gt;\n  data.frame()|&gt;\n  mutate(i=row_number())|&gt;\n  filter(F&gt;=1.5)|&gt;\n  dplyr::select(\"i\",\"edf\",\"F\",\"p.value\")|&gt;\n  htmlTable::htmlTable()\n\n\n\n\n\ni\nedf\nF\np.value\n\n\n\n\ns(EFFECTIVE_DISPOSAL)\n5\n2.17\n5.92\n0\n\n\ns(KICK_GAIN_METRES)\n8\n3.82\n7.38\n0\n\n\ns(KICK)\n9\n0.96\n3\n0\n\n\ns(LOOSE_BALL_GET)\n11\n2.71\n3\n0\n\n\ns(TACKLE)\n12\n1\n31.07\n0\n\n\ns(REBOUND_50)\n13\n2.95\n3.09\n0\n\n\ns(METRES_GAINED_EFF)\n14\n0.98\n4.92\n0\n\n\ns(PLY_PRESS_PTS)\n15\n3.36\n46.82\n0\n\n\ns(RATING)\n17\n3.04\n129.17\n0\n\n\ns(SHORT_KICK)\n19\n2.75\n1.82\n0\n\n\n\n\n#par(mar=c(1,1,1,1))\n\nplot(m1_gam,shade.col = \"gray70\",select = 14,shade = T)"
  },
  {
    "objectID": "module5.html#simple-gaussian-example",
    "href": "module5.html#simple-gaussian-example",
    "title": "6  Module 5",
    "section": "6.6 Simple Gaussian example",
    "text": "6.6 Simple Gaussian example\n\nm5_small &lt;- train_data[,.(Diff.Qtr.Points,CLEARANCE,CONTESTED_POSSESSION,CONTESTED_MARK,CRUMB,EFFECTIVE_DISPOSAL,INTERCEPT,HITOUT,KICK_GAIN_METRES,KICK,MARK,LOOSE_BALL_GET,TACKLE,REBOUND_50,METRES_GAINED_EFF,PLY_PRESS_PTS,\nSPOIL,RATING,LONG_KICK,SHORT_KICK,FIRST_POSSESSION)]\n\nm5_small_tst &lt;- test_data[,.(Diff.Qtr.Points,CLEARANCE,CONTESTED_POSSESSION,CONTESTED_MARK,CRUMB,EFFECTIVE_DISPOSAL,INTERCEPT,HITOUT,KICK_GAIN_METRES,KICK,MARK,LOOSE_BALL_GET,TACKLE,REBOUND_50,METRES_GAINED_EFF,PLY_PRESS_PTS,\nSPOIL,RATING,LONG_KICK,SHORT_KICK,FIRST_POSSESSION)]\n\n\nset.seed(123)\n# Set up repeated k-fold cross-validation\ntrain.control &lt;- trainControl(method = \"cv\", number = 10)\n# Train the model\nstep.model &lt;- train(Diff.Qtr.Points ~., data = m5_small,\n                    method = \"leapBackward\", \n                    tuneGrid = data.frame(nvmax = 1:18),\n                    trControl = train.control\n                    )\n\n\n\nbwelim &lt;- data.table(step.model$results)\n\n\nggplot(bwelim,aes(nvmax,RMSE,ymin=RMSE-RMSESD,ymax=RMSE+RMSESD))+\n  geom_pointrange()+\n  theme_bw()\n\n\n\n#plot(step.model)\n\nFrom eyeballing we can that from around 8 variables in performance doesn’t really seem to change much. Lets have a look at the coefs. Of quite note, these coefficients have been scaled on log transformed data so they don’t have direct interpretation for absolute values, but you can still look interpret them as absolute magnitude of number being indicative of importance. .\n\nround(coef(step.model$finalModel, 8),2)\n\n       (Intercept) EFFECTIVE_DISPOSAL               KICK             TACKLE \n              0.18              -2.14               4.45               4.11 \n        REBOUND_50  METRES_GAINED_EFF      PLY_PRESS_PTS              SPOIL \n             -1.16               3.27              -5.11              -0.94 \n            RATING \n              9.25 \n\n\nYou may notice some weird directions for variables e.g. (“EFFECTIVE_DISPOSAL”,“PLY_PRESS_PTS”) , this likely due to the fact that some of the variables are highly correlated with each other, which can cause flipping of effects in simple regression problems. For example, see below. Whilst, the LM model has not really done anything wrong, its more a problem with us the user having to spend a lot of mental energy contextualizing what the effects actually mean. We will explore some other techniques latter (LASSO) that can help a little in situations where you might expect many variables to be highly correlated.\n\neg1 = ggplot(train_data,aes(TACKLE,PLY_PRESS_PTS))+geom_point()+stat_smooth(method = \"lm\")\neg2 = ggplot(train_data,aes(EFFECTIVE_DISPOSAL,METRES_GAINED_EFF))+geom_point()+stat_smooth(method = \"lm\")\n\neg0 &lt;- ggplot(train_data,aes(PLY_PRESS_PTS,Diff.Qtr.Points))+geom_point()+stat_smooth(method = \"lm\")\n\nggpubr::ggarrange(eg0,eg1,eg2,nrow = 1)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nOr it could there be another reason???\n\nnvar =step.model$finalModel$nvmax-1\n\nresults_df &lt;- data.frame(i = integer(), RMSE = numeric(),r2 = numeric(),adjr2 = numeric())\n\nfor (i in 1:nvar) {\n  \nip1 &lt;- i+1\n\nform1&lt;-as.formula(paste(\"Diff.Qtr.Points~\", \n                        paste(names(coef(step.model$finalModel,i)[2: ip1]),\n                              collapse = \"+\")))\n\nlmTest &lt;- lm(form1,data = train_data)\n\n\n\nrmse &lt;-round(caret::RMSE(predict(lmTest,test_data),test_data$Diff.Qtr.Points),2)\n\nresults_df &lt;- rbind(results_df, data.frame(i = i, RMSE = rmse,\n                                           r2 =summary(lmTest)$r.squared,\n                                           adjr2 = summary(lmTest)$adj.r.squared))\n\n}\n\n\ng1=ggplot()+\n  geom_pointrange(bwelim[nvmax&lt;=nvar,],mapping =aes(nvmax,RMSE,ymin=RMSE-RMSESD,ymax=RMSE+RMSESD),\n                  col=\"gray\")+\n  geom_point(results_df,mapping=aes(i,RMSE),col=\"red\")+\n  geom_path(results_df,mapping=aes(i,RMSE),col=\"red\")+\n  theme_bw()\n\ng2 = ggplot(results_df,aes(i,r2))+geom_path()+\n  geom_path(aes(i,adjr2),col=\"red\")+\n  theme_bw()\n\nggpubr::ggarrange(g1,g2)\n\n\n\n\nKey points above.\n1) Test performance still worse than cross-validated performance, but still follow a relatively similar shape, which is good to see and could be argued test performance is technically within CV performance range.\n2) A little harder to see but adjusted r2 (in red) gets lower as we add more values when compared to just the r2 value.\n\n6.6.1 Quick note of CV\nWithin the code above, the model was tuned using cross validation. There are endless packages in R that will performance k-fold cross validation for you but below is just a quick overview of what is happening behind the scenes.\n\nnfolds = 5\n\n2066/nfolds ## data put aside for internal validation\n\n[1] 413.2\n\nround(2066-(2066/nfolds),0) ## data put aside for internal training\n\n[1] 1653\n\nfolds &lt;- createFolds(train_data$QtrWL,k = nfolds)\nsapply(folds, length)\n\nFold1 Fold2 Fold3 Fold4 Fold5 \n  413   413   414   413   413"
  }
]