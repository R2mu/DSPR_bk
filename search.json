[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GWS Data Science and Problem Recognition",
    "section": "",
    "text": "Preface\nThis is a Quarto book is designed as a brief introduction to data science and problem/pattern recognition. Look to copy code chunks that exist throughout the book and try run them in your r studio console.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html#why-r",
    "href": "intro.html#why-r",
    "title": "1  Overview",
    "section": "1.1 Why R?",
    "text": "1.1 Why R?\nComparing between programming languages whilst fun is really not a useful exercise. Long story short, there a three programming languages currently that saturate sports analytics. Python, R and SQL. We are using R within R studio, namely because\n\nSlightly more user friendly for non programmers hence, faster to prototype ideas\nThere are more resources available for common analytically techniques that we will use in sport.\nRstudio is a fantastic programming IDE, you are able to also write, SQL, Python, Javascript and more whilst working in an R session.\n\nBelow is a photo which I think encapsulates the goal of R as a programming language very nicely.\n\nThe above photo is taken from https://r4ds.had.co.nz/introduction.html which is a fantastic resource for learning R.\nUltimately whatever you choose, it is better to get really good at one language than get just okay at a couple. You will find once you learn one it will become easier each time you attempt to learn another. Below are some of the aspects that I would consider strengths of other programming languages, below is very much a subjective spiel of my experience so take it with a grain of salt.\nPython: Is the most popular programming language at the moment and there is good reason for it. As the creator of Shiny R Joe Cheng said, “Python is the second-best language for anything”(Initial quote from Dan Callahan). In particular, Python will have a lot more resources available for deep learning, developing servers and working with external hardware or IOT applications. While there are definitely some questions in sports performance that could utilize deep learning approaches these are exceptions and not the rule in my opinion.\nSQL: Is mainly thought of a database language in that it is largely responsible for calling databases that are stored on servers. In particular, SQL utilities key verbs such as From, Select, Filter, Inset, Group, Join, Summarize. One of the reasons why I am teaching you tidyverse is as you will see later, a lot of the key verbs discussed above are used within dplyr, so by learning tidyverse (dplyr in particular) you are developing your SQL knowledge.\nJulia: A newer scientific programming language that is aiming to have the speed of compiled languages such as C++ but the readability of languages like R and Python. I’ve played around with it a little and it is quite a nice language, in particular its ability to program CUDA for utilizing GPU and also how easy it makes it to parallelize code. That being said, I haven’t had a use case yet where it has made sense for me to use it over R or Python, additionally as it is a newer language the amount of available support and information is lacking when compared to other languages.\nJavascript: Love hate relationship with this language. I love that it is non event blocking which means you can run some pretty fast real time simulations with packages such as D3. It is just a hard language to learn added with\n\n1.1.1 A quick note on best coding practices\nThis is a classic case of do as I say not as I do. I have no doubt over the coming months there will be times when I don’t adhere to some of my suggestions. However, I have tried to put down things I wish I did when I was starting my R journey.\n\nComment more than necessary. This can be really useful if you get stuck with code not working and you want to get advice from something like CHATGPT. An example comment may be something like “aggregate value column by group and plot as horizontal bar chart sorted from highest to lowest”\nDon’t be afraid to be overly modular with your code. This will make more sense later on, but is is better to slowly build up steps making sure they are correct then building once massive function then working backwards trying to solve where it may not be working\nAttempt to be consistent with naming conventions\n“We should forget about small inefficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.” Donald Knuth. This is an example of advice I should listen to as I will often spend an hour trying to optimize something that took 15 seconds to run down to 5 seconds.\nTry to start with the end in mind. What do you want your report to look like? What analysis method do you think it will be most appropriate? Are there visuals that you want to use? This should help you then work backwards to the data you current have available.\nBe open minded. Unfortunately/fortunately with many data science problems there are many ways to get to the same outcome."
  },
  {
    "objectID": "module1.html#installing-and-loading-r-packages",
    "href": "module1.html#installing-and-loading-r-packages",
    "title": "2  Module 1",
    "section": "2.1 Installing and loading R packages",
    "text": "2.1 Installing and loading R packages\nIf you have never installed an R package there is a couple of options which I have outlined below :\n\n# \"#\" Will comment out R code and will just print text\n# as I already have pacman loaded I will comment it out.\n# the short cut to remove comments from code is ctrl+shift+c\n\n#install.packages(\"pacman\")\n\npacman::p_load(tidyverse,data.table,httr,jsonlite,htmlTable,sf,tidytext)"
  },
  {
    "objectID": "module1.html#loading-data-locally",
    "href": "module1.html#loading-data-locally",
    "title": "2  Module 1",
    "section": "2.2 Loading data locally",
    "text": "2.2 Loading data locally\nThere a couple of different ways to load data. As I want to attempt to future proof your learning we will use the fread() function from the data.table package. Many options for reading in csv;s exist but this by far is one of the fastest ways to load data in R.\nOption 1: if the data is located in your directory it is a sample as.\n\n# option 1: if the data is located in your directory it is a sample as.\n\ndata1   &lt;- fread(\"PlayerLong.csv\")\nplaypos &lt;- fread(\"PlayPos3.csv\")\n\nOption 2: Data is located elsewhere on your computer you will need to put the full path of where it is located. A simple way to get started with your path is with the getwd() function\n\ngetwd()\n\n[1] \"C:/Users/Research/Desktop/GWS_DSPR\"\n\n\n\n#data1 &lt;- fread(\"/Users/Research/Dropbox/Rscripts/PlayerLong.csv\")\n\nOption 3: Finally if you are feeling lazy or struggling to put the correct path in you file.choose();\n\n#data1 &lt;- fread(file.choose())\n\n\n2.2.1 Loading data from API\nWith the way technologies are heading, connecting to API’s to extract data are only going to become more common. Hence, I will give a quick example of how you may do that in R. Unfortunately, due differing data privacy and safety protocols, some API’s will require different methods of securing a “handshake”. Below is a simple example that does not require authentication.\n\n## get current location of ISS\nurl &lt;- \"https://api.wheretheiss.at/v1/satellites/25544\"\n\nworld_coordinates &lt;- map_data(\"world\") \n\n\n  df  &lt;- fromJSON(url,simplifyDataFrame = T)|&gt;data.frame()\n  \n  #dff &lt;- rbind(dff,df)\n  \n\n  print(ggplot(df,aes(longitude,latitude))+\n          geom_map( \n            data = world_coordinates, map = world_coordinates, \n            aes(long,lat, map_id = region) \n          )+\n          geom_point(col=\"orange\",size=2)+\n          theme_bw())\n\nWarning in geom_map(data = world_coordinates, map = world_coordinates,\naes(long, : Ignoring unknown aesthetics: x and y\n\n\n\n\n\n\n\n2.2.2 Binding multiple CSVs from a folder\nSometimes you will have a list of multiple CSV’s that are the same in structure that just represent different dates of saving. In R it is relatively simple to loop through a directory and append all the files. There are multiple ways of doing this but here is one of the most concise ways I have come across.\n\n## directory\nloc &lt;- \"/Users/Research/Dropbox/Rscripts/listExample\"\n\n# gets a list of files in directoru\nfiles &lt;- list.files(path = loc, pattern = \"\\\\.csv$\",full.names = T)\n\n## loops through and binds them together. \ncombined_df &lt;- rbindlist(lapply(files, fread))\n\nunique(combined_df$FIXED_ID)\n\n[1] 106730101 106730102 106730103\n\nhead(combined_df[,.(MATCH_DATE,GROUP_ROUND_NO,HOME_SQUAD,AWAY_SQUAD,\n                    PERIOD,STATISTIC_CODE,FULLNAME)])|&gt;\n  htmlTable::htmlTable()\n\n\n\n\n\nMATCH_DATE\nGROUP_ROUND_NO\nHOME_SQUAD\nAWAY_SQUAD\nPERIOD\nSTATISTIC_CODE\nFULLNAME\n\n\n\n\n1\n28-FEB-19\n1\nCarlton\nEssendon\n1\nMTCHI\n\n\n\n2\n28-FEB-19\n1\nCarlton\nEssendon\n1\nPERST\n\n\n\n3\n28-FEB-19\n1\nCarlton\nEssendon\n1\nCEBO\n\n\n\n4\n28-FEB-19\n1\nCarlton\nEssendon\n1\nCEBO\n\n\n\n5\n28-FEB-19\n1\nCarlton\nEssendon\n1\nCBVS\nAndrew Phillips\n\n\n6\n28-FEB-19\n1\nCarlton\nEssendon\n1\nCBVS\nZac Clarke"
  },
  {
    "objectID": "module1.html#simple-data-cleaning-procedures",
    "href": "module1.html#simple-data-cleaning-procedures",
    "title": "2  Module 1",
    "section": "2.3 Simple data cleaning procedures",
    "text": "2.3 Simple data cleaning procedures\nIn this example we will be using data1 and playpos data.frames. We will explore how to filter, add variables (mutate) , join data bases and summaries. Below I will tidyverse packages to do this but you could also do everything below using the data.table package (my personal favorite) .\n\n# Lets have a look at column names from both \ncolnames(data1)\n\n [1] \"V1\"             \"MATCH_ID\"       \"GAME_ID\"        \"SEASON_ID\"     \n [5] \"GROUP_ROUND_NO\" \"VENUE_NAME\"     \"PERSON_ID\"      \"FULLNAME\"      \n [9] \"SQUAD_NAME\"     \"OPP_SQUAD_NAME\" \"SQUAD_MARGIN\"   \"variable\"      \n[13] \"value\"         \n\n\n\ncolnames(playpos)\n\n[1] \"FULLNAME\"  \"SEASON_ID\" \"PERSON_ID\" \"Position\" \n\n\nQuick way to get additional information about your data.\n\nstr(data1)\n\nClasses 'data.table' and 'data.frame':  2656638 obs. of  13 variables:\n $ V1            : int  1 2 3 4 5 6 7 8 9 10 ...\n $ MATCH_ID      : int  266569840 266569840 266569840 266569840 266569840 266569840 266569840 266569840 266569840 266569840 ...\n $ GAME_ID       : chr  \"R0123\" \"R0123\" \"R0123\" \"R0123\" ...\n $ SEASON_ID     : int  2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 ...\n $ GROUP_ROUND_NO: int  1 1 1 1 1 1 1 1 1 1 ...\n $ VENUE_NAME    : chr  \"MCG\" \"MCG\" \"MCG\" \"MCG\" ...\n $ PERSON_ID     : int  250395 270146 270896 280819 290627 290847 293813 294036 294592 294674 ...\n $ FULLNAME      : chr  \"Jack Riewoldt\" \"Ed Curnow\" \"Trent Cotchin\" \"Dylan Grimes\" ...\n $ SQUAD_NAME    : chr  \"Richmond\" \"Carlton\" \"Richmond\" \"Richmond\" ...\n $ OPP_SQUAD_NAME: chr  \"Carlton\" \"Richmond\" \"Carlton\" \"Carlton\" ...\n $ SQUAD_MARGIN  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ variable      : chr  \"BALL_UP_CLEARANCE\" \"BALL_UP_CLEARANCE\" \"BALL_UP_CLEARANCE\" \"BALL_UP_CLEARANCE\" ...\n $ value         : num  1 1 3 0 2 0 0 2 1 0 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n\nThe above can be a useful problem solver if you are struggling with code, as sometimes things that you might expect to be saved as a integer may be saved as a character. If you then tried to average a column that is considered to be characters you would run into trouble.\n\n2.3.1 Joins\nWe can notice that both data.frames have columns names called PERSON_ID, what we would like to do is join these databases by SEASON_ID & PERSON_ID so that we could have player position joined with the data1 database. To do this we are going to use a left_join() function from dplyr package, this package is loaded when you load Tidyverse .\nA quick note on joins. There are many different types of joins, Left, Right, Inner, Outer and Cross joins. I struggle to remember what they all represent namely because I have only ever really had to use Left joins and the occasional cross join. Below is an example of both a Left and Cross join. See https://r4ds.had.co.nz/relational-data.html#understanding-joins for more information regarding other join methods.\n\n## Cross join: I am going to use the data.table CJ function here just because it is already loaded and it is faster than tidyverse equivalent\n\nlist1 &lt;-seq(as.Date(\"2023-11-20\"), as.Date(\"2024-09-28\"), by=\"days\")\nlist2 &lt;- c(\"A\",\"B\",\"C\",\"D\")\n\nhead(CJ(list1,list2),12)|&gt;htmlTable::htmlTable()\n\n\n\n\n\nlist1\nlist2\n\n\n\n\n1\n2023-11-20\nA\n\n\n2\n2023-11-20\nB\n\n\n3\n2023-11-20\nC\n\n\n4\n2023-11-20\nD\n\n\n5\n2023-11-21\nA\n\n\n6\n2023-11-21\nB\n\n\n7\n2023-11-21\nC\n\n\n8\n2023-11-21\nD\n\n\n9\n2023-11-22\nA\n\n\n10\n2023-11-22\nB\n\n\n11\n2023-11-22\nC\n\n\n12\n2023-11-22\nD\n\n\n\n\n\n\n## now for a left join example\n## simple rule for left join, large merges with small, in our case data1 with playpos\n\ndata2 = left_join(data1,playpos, by =c(\"SEASON_ID\",\"PERSON_ID\",\"FULLNAME\"))\n\n#names(data2)\n\nhead(data2)|&gt;htmlTable::htmlTable()\n\n\n\n\n\nV1\nMATCH_ID\nGAME_ID\nSEASON_ID\nGROUP_ROUND_NO\nVENUE_NAME\nPERSON_ID\nFULLNAME\nSQUAD_NAME\nOPP_SQUAD_NAME\nSQUAD_MARGIN\nvariable\nvalue\nPosition\n\n\n\n\n1\n1\n266569840\nR0123\n2023\n1\nMCG\n250395\nJack Riewoldt\nRichmond\nCarlton\n0\nBALL_UP_CLEARANCE\n1\nKey Fwd\n\n\n2\n2\n266569840\nR0123\n2023\n1\nMCG\n270146\nEd Curnow\nCarlton\nRichmond\n0\nBALL_UP_CLEARANCE\n1\nMid Fwd\n\n\n3\n3\n266569840\nR0123\n2023\n1\nMCG\n270896\nTrent Cotchin\nRichmond\nCarlton\n0\nBALL_UP_CLEARANCE\n3\nMid Fwd\n\n\n4\n4\n266569840\nR0123\n2023\n1\nMCG\n280819\nDylan Grimes\nRichmond\nCarlton\n0\nBALL_UP_CLEARANCE\n0\nKey Def\n\n\n5\n5\n266569840\nR0123\n2023\n1\nMCG\n290627\nDion Prestia\nRichmond\nCarlton\n0\nBALL_UP_CLEARANCE\n2\nMid\n\n\n6\n6\n266569840\nR0123\n2023\n1\nMCG\n290847\nDustin Martin\nRichmond\nCarlton\n0\nBALL_UP_CLEARANCE\n0\nGen Fwd\n\n\n\n\n\n\n\n2.3.2 Selecting and filtering\nOkay , lets say after inspection of the data.frame we feel like some columns are redundant or we simply just want to move some columns around we will achieve this using the select verb. Additionally, lets say we want to just have the data frame represent certain positions and key variables (game statistics) we think are important, this can be achieved using the filter verb.\n\n# Firstly lets get a list of column names \nnames(data2)\n\n [1] \"V1\"             \"MATCH_ID\"       \"GAME_ID\"        \"SEASON_ID\"     \n [5] \"GROUP_ROUND_NO\" \"VENUE_NAME\"     \"PERSON_ID\"      \"FULLNAME\"      \n [9] \"SQUAD_NAME\"     \"OPP_SQUAD_NAME\" \"SQUAD_MARGIN\"   \"variable\"      \n[13] \"value\"          \"Position\"      \n\n\n\n# Lets have a look at what positions exist within the Position column\nunique(data2$Position)\n\n[1] \"Key Fwd\" \"Mid Fwd\" \"Key Def\" \"Mid\"     \"Gen Fwd\" \"Wing\"    \"Gen Def\"\n[8] \"Ruck\"    NA       \n\n\n\n# finally lets explore what statisitcs are within the variable column\nhead(unique(data2$variable),30)\n\n [1] \"BALL_UP_CLEARANCE\"         \"BALL_UP_FIRST_POSSESSION\" \n [3] \"BALL_UP_HITOUT\"            \"BALL_UP_HITOUT_SHARKED\"   \n [5] \"BAULKED\"                   \"BEHIND\"                   \n [7] \"BROKEN_TACKLE\"             \"BU_HITOUT_TO_ADVANTAGE\"   \n [9] \"CB_FIRST_POSSESSION\"       \"CB_HITOUT_SHARKED\"        \n[11] \"CB_HITOUT_TO_ADVANTAGE\"    \"CENTRE_BOUNCE_CLEARANCE\"  \n[13] \"CENTRE_BOUNCE_HITOUT\"      \"CLANGER\"                  \n[15] \"CLANGER_HANDBALL\"          \"CLANGER_KICK\"             \n[17] \"CLEARANCE\"                 \"CONTESTED_KNOCK_ON\"       \n[19] \"CONTESTED_MARK\"            \"CONTESTED_MARK_FROM_OPP\"  \n[21] \"CONTESTED_MARK_FROM_TEAM\"  \"CONTESTED_POSSESSION\"     \n[23] \"CONTESTED_POSSESSION_POST\" \"CONTESTED_POSSESSION_PRE\" \n[25] \"CRUMB\"                     \"DISPOSAL\"                 \n[27] \"DISPOSAL_POST\"             \"DISPOSAL_PRE\"             \n[29] \"EFFECTIVE_DISPOSAL\"        \"EFFECTIVE_HANDBALL\"       \n\n\n\n## select by number or name\ndata2 = data2|&gt;\n   select(4,2,5:9,14,10:13)|&gt;\n# If you want to remove a column you can do the below\n# select(!c(\"PERSON_ID\"))|&gt; this is an example how you may remove a specific column\n#   filter(Position %in% c(\"Mid Fwd\",\"Mid\"))|&gt;\n   filter(variable %in% c(\"CLEARANCE\",\"CONTESTED_POSSESSION\",\n                          \"EFFECTIVE_DISPOSAL\",\"TURNOVER\",\n                          \"TOTAL_GAINED_METRES\",\"PLY_PRESS_PTS\"))\n\n# Check variables\nunique(data2$variable)\n\n[1] \"CLEARANCE\"            \"CONTESTED_POSSESSION\" \"EFFECTIVE_DISPOSAL\"  \n[4] \"TOTAL_GAINED_METRES\"  \"TURNOVER\"             \"PLY_PRESS_PTS\"       \n\n\n\n\n2.3.3 Adding calculated columns\nLets add a calculated column to the data frame using the mutate function. The column we are going to add is going to convert the score differential into a binary win loss where 0 represents a loss and 1 a win.\n\ndata2 &lt;- data2|&gt;mutate(WL = ifelse(SQUAD_MARGIN&gt;0,1,0))\n\n# if we wanted to have draw we could simply do \n\ndata2 &lt;- data2|&gt;mutate(WLD = ifelse(SQUAD_MARGIN&gt;0,1,\n                                    ifelse(SQUAD_MARGIN==0,0,-1)))\n\n# We don't have to break it up as we did above we could simply add two columns at once\ndata2 &lt;- data2|&gt;mutate(WL = ifelse(SQUAD_MARGIN&gt;0,1,0),\n                       WLD = ifelse(SQUAD_MARGIN&gt;0,1,\n                                    ifelse(SQUAD_MARGIN==0,0,-1)))\n\ndata2|&gt;\n  tail(10)|&gt;\n  htmlTable()\n\n\n\n\n\nSEASON_ID\nMATCH_ID\nGROUP_ROUND_NO\nVENUE_NAME\nPERSON_ID\nFULLNAME\nSQUAD_NAME\nPosition\nOPP_SQUAD_NAME\nSQUAD_MARGIN\nvariable\nvalue\nWL\nWLD\n\n\n\n\n1\n2024\n149741648\n7\nMCG\n1013133\nBraeden Campbell\nSydney Swans\nWing\nHawthorn\n76\nPLY_PRESS_PTS\n3.75\n1\n1\n\n\n2\n2024\n149741648\n7\nMCG\n1013230\nLogan McDonald\nSydney Swans\nKey Fwd\nHawthorn\n76\nPLY_PRESS_PTS\n22.8\n1\n1\n\n\n3\n2024\n149741648\n7\nMCG\n1013409\nJames Jordon\nSydney Swans\nMid Fwd\nHawthorn\n76\nPLY_PRESS_PTS\n29.1\n1\n1\n\n\n4\n2024\n149741648\n7\nMCG\n1017091\nJai Serong\nHawthorn\nKey Def\nSydney Swans\n-76\nPLY_PRESS_PTS\n1.2\n0\n-1\n\n\n5\n2024\n149741648\n7\nMCG\n1017094\nConnor Macdonald\nHawthorn\nGen Fwd\nSydney Swans\n-76\nPLY_PRESS_PTS\n25.65\n0\n-1\n\n\n6\n2024\n149741648\n7\nMCG\n1018016\nSeamus Mitchell\nHawthorn\nGen Def\nSydney Swans\n-76\nPLY_PRESS_PTS\n9.15\n0\n-1\n\n\n7\n2024\n149741648\n7\nMCG\n1020895\nJai Newcombe\nHawthorn\nMid\nSydney Swans\n-76\nPLY_PRESS_PTS\n26.55\n0\n-1\n\n\n8\n2024\n149741648\n7\nMCG\n1023482\nCam Mackenzie\nHawthorn\nMid\nSydney Swans\n-76\nPLY_PRESS_PTS\n43.5\n0\n-1\n\n\n9\n2024\n149741648\n7\nMCG\n1027935\nJosh Weddle\nHawthorn\nKey Def\nSydney Swans\n-76\nPLY_PRESS_PTS\n20.55\n0\n-1\n\n\n10\n2024\n149741648\n7\nMCG\n1027965\nMax Ramsden\nHawthorn\nKey Fwd\nSydney Swans\n-76\nPLY_PRESS_PTS\n9.6\n0\n-1\n\n\n\n\n\n\n\n2.3.4 Adding calculated columns by group\n\ndata2 &lt;- data2|&gt;\n  group_by(variable,Position)|&gt;\n  # if you wanted to specific by position you could do the below\n  #group_by(SEASON_ID,Position,variable)|&gt;\n  mutate(avgByVar = round(mean(value),2))|&gt;\n  ungroup()|&gt;\n  mutate(diff = round(value - avgByVar,2))\n  \nhtmlTable(head(data2,10))\n\n\n\n\n\nSEASON_ID\nMATCH_ID\nGROUP_ROUND_NO\nVENUE_NAME\nPERSON_ID\nFULLNAME\nSQUAD_NAME\nPosition\nOPP_SQUAD_NAME\nSQUAD_MARGIN\nvariable\nvalue\nWL\nWLD\navgByVar\ndiff\n\n\n\n\n1\n2023\n266569840\n1\nMCG\n250395\nJack Riewoldt\nRichmond\nKey Fwd\nCarlton\n0\nCLEARANCE\n1\n0\n0\n0.47\n0.53\n\n\n2\n2023\n266569840\n1\nMCG\n270146\nEd Curnow\nCarlton\nMid Fwd\nRichmond\n0\nCLEARANCE\n1\n0\n0\n2.08\n-1.08\n\n\n3\n2023\n266569840\n1\nMCG\n270896\nTrent Cotchin\nRichmond\nMid Fwd\nCarlton\n0\nCLEARANCE\n4\n0\n0\n2.08\n1.92\n\n\n4\n2023\n266569840\n1\nMCG\n280819\nDylan Grimes\nRichmond\nKey Def\nCarlton\n0\nCLEARANCE\n0\n0\n0\n0.2\n-0.2\n\n\n5\n2023\n266569840\n1\nMCG\n290627\nDion Prestia\nRichmond\nMid\nCarlton\n0\nCLEARANCE\n5\n0\n0\n4.56\n0.44\n\n\n6\n2023\n266569840\n1\nMCG\n290847\nDustin Martin\nRichmond\nGen Fwd\nCarlton\n0\nCLEARANCE\n1\n0\n0\n0.87\n0.13\n\n\n7\n2023\n266569840\n1\nMCG\n293813\nTom Lynch\nRichmond\nKey Fwd\nCarlton\n0\nCLEARANCE\n2\n0\n0\n0.47\n1.53\n\n\n8\n2023\n266569840\n1\nMCG\n294036\nGeorge Hewett\nCarlton\nMid\nRichmond\n0\nCLEARANCE\n8\n0\n0\n4.56\n3.44\n\n\n9\n2023\n266569840\n1\nMCG\n294592\nKamdyn McIntosh\nRichmond\nWing\nCarlton\n0\nCLEARANCE\n1\n0\n0\n1.48\n-0.48\n\n\n10\n2023\n266569840\n1\nMCG\n294674\nNick Vlastuin\nRichmond\nGen Def\nCarlton\n0\nCLEARANCE\n0\n0\n0\n0.75\n-0.75\n\n\n\n\n\n\n\n2.3.5 Summarizing data (creating pivot tables)\nOkay the next section is going to go over how we can create pivot tables using the summarise function from the dplyr package which is loaded when you load tidyverse . I will do a couple of different examples of data summaries you may be interested in making using the data-set at hand. I will also show examples of plots as this can be a quick way to just double check your aggregation procedures\n\nlibrary(tidytext)\n\ndata2|&gt;\n  filter(SEASON_ID==2023)|&gt;\n  group_by(SEASON_ID,SQUAD_NAME,GROUP_ROUND_NO,variable)%&gt;%\n  summarise(sum = sum(value))%&gt;%\n  ungroup()|&gt;\n  group_by(SEASON_ID,SQUAD_NAME,variable)%&gt;%\n  summarise(season_avg = mean(sum),\n            season_sd  = sd(sum),\n            season_max = max(sum),\n            season_sum = sum(sum))|&gt;\n    ungroup()|&gt;\n  #arrange(variable, desc(season_avg))|&gt;\n  # Reorder SQUAD_NAME based on season_avg, within each variable facet\n  mutate(SQUAD_NAME_RO = reorder_within(SQUAD_NAME, season_sum, variable))|&gt;\n  ggplot(aes(season_sum,SQUAD_NAME_RO,col=as.factor(SEASON_ID)))+\n  geom_point()+\n  facet_wrap(~variable, scales = \"free\", labeller = label_wrap_gen(width = 10)) +\n  scale_y_reordered() +  # Necessary to apply the custom ordering\n  theme_bw() +\n  theme(legend.position = \"top\",\n        axis.text = element_text(size = 6))\n\n`summarise()` has grouped output by 'SEASON_ID', 'SQUAD_NAME',\n'GROUP_ROUND_NO'. You can override using the `.groups` argument.\n`summarise()` has grouped output by 'SEASON_ID', 'SQUAD_NAME'. You can override\nusing the `.groups` argument.\n\n\n\n\n\nYou don’t need to necessarily know what the code below is doing for now but It is just going to shorten the club names.\nIn this next example lets have a quick look at the average stats across key variables as a function of score differential and or Win vs Loss.\n\ndata2|&gt;\n  filter(SEASON_ID==2023)|&gt;\n  group_by(SQUAD_NAME,GROUP_ROUND_NO,variable,WL)|&gt;\n  summarise(sum = sum(value))|&gt;\n  ggplot(aes(as.factor(WL),sum))+\n  geom_jitter()+\n  geom_boxplot()+\n  theme_bw()+\n  facet_wrap(~variable,scales = \"free\")\n\n`summarise()` has grouped output by 'SQUAD_NAME', 'GROUP_ROUND_NO', 'variable'.\nYou can override using the `.groups` argument.\n\n\n\n\n\n\ndata2|&gt;\n  filter(SEASON_ID==2023)|&gt;\n  group_by(SQUAD_NAME,GROUP_ROUND_NO,variable,SQUAD_MARGIN)|&gt;\n  summarise(sum = sum(value))|&gt;\n  ggplot(aes(SQUAD_MARGIN,sum, col = SQUAD_NAME))+\n  geom_jitter(col=\"gray80\",alpha=.3)+\n  stat_smooth(method = \"lm\",se=F)+\n  theme_bw()+\n  theme(legend.position=\"top\")+\n  facet_wrap(~variable,scales = \"free\")\n\n`summarise()` has grouped output by 'SQUAD_NAME', 'GROUP_ROUND_NO', 'variable'.\nYou can override using the `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nLets delve a little deeper in to CONTESTED_POSSESSION\n\ndata2|&gt;\n  filter(SEASON_ID==2023)|&gt;\n  group_by(SQUAD_NAME,GROUP_ROUND_NO,variable,SQUAD_MARGIN)|&gt;\n  summarise(sum = sum(value))|&gt;\n  filter(variable==\"CONTESTED_POSSESSION\")|&gt;\n  ggplot(aes(SQUAD_MARGIN,sum, col = SQUAD_NAME))+\n  geom_jitter(col=\"gray80\",alpha=.3)+\n  stat_smooth(method = \"lm\",se=F)+\n  theme_bw()+\n  facet_wrap(~variable,scales = \"free\")\n\n`summarise()` has grouped output by 'SQUAD_NAME', 'GROUP_ROUND_NO', 'variable'.\nYou can override using the `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "module1.html#running-your-first-model",
    "href": "module1.html#running-your-first-model",
    "title": "2  Module 1",
    "section": "2.4 Running your first model",
    "text": "2.4 Running your first model\nOkay we are going to run our first statistical model. The simple linear model we will use in this case isn’t technically appropriate to use for the data set at hand but we will improve on this over the coming months. Before we can run the model we need to change the shape of the data-frame from its “long” format to a “wider” format. We can do that using the spread function.\nPersonally, I prefer using dcast.dafwrifwta.table function for this but I am trying to be consistent within the tidyverse for you here.\n\nwide_TV = data2|&gt;\n  filter(SEASON_ID==2023)|&gt;\n # filter(Position%in%c(\"Mid\",\"Mid Fwd\"))|&gt;\n  group_by(SQUAD_NAME,GROUP_ROUND_NO,variable,SQUAD_MARGIN)|&gt;\n  summarise(sum = sum(value))|&gt;\n  spread(key = c(\"variable\"),value = sum)\n\n`summarise()` has grouped output by 'SQUAD_NAME', 'GROUP_ROUND_NO', 'variable'.\nYou can override using the `.groups` argument.\n\n## the data.table method i prefer\nwide_dt = setDT(data2)[SEASON_ID==2023,#&Position%in%c(\"Mid\",\"Mid Fwd\"),\n         ][, .(sum = sum(value)),\n              by=.(SQUAD_NAME,GROUP_ROUND_NO,variable,SQUAD_MARGIN)\n          ][,dcast.data.table(.SD,...~variable,value.var = \"sum\")\n          ]\n\n#wide_TV|&gt;head()|&gt;htmlTable()\nwide_dt|&gt;head()|&gt;htmlTable()\n\n\n\n\n\nSQUAD_NAME\nGROUP_ROUND_NO\nSQUAD_MARGIN\nCLEARANCE\nCONTESTED_POSSESSION\nEFFECTIVE_DISPOSAL\nPLY_PRESS_PTS\nTOTAL_GAINED_METRES\nTURNOVER\n\n\n\n\n1\nAdelaide Crows\n1\n-16\n33\n111\n242\n613.35\n6077.2\n56\n\n\n2\nAdelaide Crows\n2\n-32\n33\n139\n253\n601.2\n5950.1\n62\n\n\n3\nAdelaide Crows\n3\n31\n39\n146\n251\n577.8\n5882.4\n50\n\n\n4\nAdelaide Crows\n4\n39\n35\n151\n276\n592.8\n6096.5\n71\n\n\n5\nAdelaide Crows\n5\n56\n37\n156\n302\n626.25\n6373\n67\n\n\n6\nAdelaide Crows\n6\n3\n47\n145\n255\n686.1\n5902\n66\n\n\n\n\n\nLets run a multiple regression model\n\npacman::p_load(broom,equatiomatic,lme4,mgcv)\n\nmodel_lm &lt;- lm(SQUAD_MARGIN~CONTESTED_POSSESSION+EFFECTIVE_DISPOSAL+CLEARANCE+TOTAL_GAINED_METRES+TURNOVER,data = wide_dt)\n\nmodel_lmer &lt;- lmer(SQUAD_MARGIN~CONTESTED_POSSESSION+EFFECTIVE_DISPOSAL+CLEARANCE+TOTAL_GAINED_METRES+TURNOVER+\n  (CONTESTED_POSSESSION+CLEARANCE||SQUAD_NAME),data = wide_dt)\n\nboundary (singular) fit: see help('isSingular')\n\nwide_dt$SQUAD_NAME &lt;-as.factor(wide_dt$SQUAD_NAME)\n\nmodel_gam &lt;- mgcv::gam(SQUAD_MARGIN~s(CONTESTED_POSSESSION)+s(EFFECTIVE_DISPOSAL)+\n                      s(TURNOVER)+s(TOTAL_GAINED_METRES)+s(CLEARANCE)+\n                      s(SQUAD_NAME,bs=\"re\"),data = wide_dt)\n\n#equatiomatic::extract_eq(first_model)\n\n#equatiomatic::extract_eq(first_model, use_coefs = TRUE)\n\nBelow is a summary of the output\n\nbroom::tidy(model_lm,conf.int = T,conf.level = .95)|&gt;\n       mutate_if(is.numeric, round, 2)|&gt;\n  htmlTable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n1\n(Intercept)\n-326.69\n19.66\n-16.62\n0\n-365.33\n-288.06\n\n\n2\nCONTESTED_POSSESSION\n0.44\n0.12\n3.68\n0\n0.21\n0.68\n\n\n3\nEFFECTIVE_DISPOSAL\n0.27\n0.05\n5.62\n0\n0.17\n0.36\n\n\n4\nCLEARANCE\n0.85\n0.23\n3.62\n0\n0.39\n1.31\n\n\n5\nTOTAL_GAINED_METRES\n0.05\n0\n14.25\n0\n0.04\n0.05\n\n\n6\nTURNOVER\n-1.49\n0.15\n-10.2\n0\n-1.77\n-1.2\n\n\n\n\n\nMaybe we want to visualize the above\n\nbroom::tidy(model_lm,conf.int = T,conf.level = .95)|&gt;\n       mutate_if(is.numeric, round, 2)|&gt;\n       filter(term%in%c(\"TURNOVER\",\"TOTAL_GAINED_METRES\",\"CLEARANCE\",\n                        \"EFFECTIVE_DISPOSAL\",\"CONTESTED_POSSESSION\"))|&gt;\n       ggplot(aes(estimate,term,xmin=conf.low,xmax=conf.high))+\n       geom_pointrange()+\n       theme_bw()\n\n\n\n\nNow there are many issues with this first figure. Firstly, as they are all on different scales the figure above could potentially exaggerate the impact of turnover when compared to other variables such as metres gained. Also the p values and subsequent confidence intervals are incorrect as we have violated a couple of statistical assumptions with just using a basic linear model.\nPartial dependency plots offer a better way of assessing the impact of a predictor. Effectively, you can think of it as the predicted impact of changing a variable whilst holding other variables constant.\n\npacman::p_load(pdp)\npartial(model_gam, pred.var = c(\"TURNOVER\"), \n        plot = TRUE,plot.engine = \"ggplot\",ice=T,)+\n       geom_hline(yintercept = 0)+\n       theme_bw()\n\n\n\n\nWhat about metres gained\n\npartial(model_gam, pred.var = c(\"TOTAL_GAINED_METRES\"), \n        plot = TRUE,plot.engine = \"ggplot\",ice=T,)+\n       geom_hline(yintercept = 0)+\n       theme_bw()\n\n\n\n\n\npartial(model_gam, pred.var = c(\"TOTAL_GAINED_METRES\",\"TURNOVER\"), \n        plot = TRUE,\n        plot.engine = \"ggplot\",\n        chull = T)+\n       geom_hline(yintercept = 0)+\n       theme_bw()\n\n\n\n\nLets have a look at how well our model performed\n\n## calculate predicted model values from data obtained along with some other summary #stats\nwide_dt &lt;- wide_dt|&gt;\n           mutate(predicted = predict(model_lm),\n                  error = SQUAD_MARGIN - predicted,\n                  absError = abs(error),\n                  squError = error^2)\n\nLets have a look at how well or model can do at predicting SQUAD_MARGIN\n\n ggplot(wide_dt,aes(SQUAD_MARGIN,predicted))+\n   geom_point()+\n#   geom_smooth(method = \"lm\",se=F)+\n   geom_abline(intercept = 0,slope = 1,col=\"red\",linetype=\"dashed\")+\n   theme_bw()\n\n\n\n\nOkay, visually the performance looks pretty bad, ideally most of the dots would fit along the red line. It looks like the model is struggling to pick up the magnitude of wins and loses correctly. Lets numerically summarize this.\n\n## mean absolute error\nround(mean(wide_dt$absError),1)\n\n[1] 21.1\n\n## Root mean squred error\nround(sqrt(mean(wide_dt$squError)),1)\n\n[1] 26.7\n\n\nOkay, so we can say that using a simple multiple regression model on just mid and mid fwd data across the variables we looked at is not doing a great job at predicting SQUAD_MARGIN."
  },
  {
    "objectID": "module1.html#practice-exercises",
    "href": "module1.html#practice-exercises",
    "title": "2  Module 1",
    "section": "2.5 Practice exercises",
    "text": "2.5 Practice exercises\n\nDownload R and Quarto https://quarto.org/docs/download/ if you haven’t already.\nLoad some data into R and attempt some basic data manipulation\nAttempt to build a basic plot\nStart to think about questions you might have regarding data you have access to.\nLook to annoy Isaac atleast once over the next two weeks with a problem you might be having."
  },
  {
    "objectID": "module1.html#additional-resources",
    "href": "module1.html#additional-resources",
    "title": "2  Module 1",
    "section": "2.6 Additional resources",
    "text": "2.6 Additional resources\nplotting in R using GGPLOT: https://ggplot2-book.org/\nFree R for data science resource: https://r4ds.had.co.nz/introduction.html\nFree ISLR resource : https://www.statlearning.com/\nWhilst, we wont use tidymodels too much I would encourage you to be across it https://www.tidymodels.org/start/ as it is a very powerful modular way for machine learning in R.\nyoutube:\nhttps://www.youtube.com/@TidyX_screencast\nhttps://www.youtube.com/@JuliaSilge/videos\nand of course CHATGPT.\nI will continue to update this as I remember more of the resources I have come across."
  },
  {
    "objectID": "module2.html#recap-from-previous-week",
    "href": "module2.html#recap-from-previous-week",
    "title": "3  Module 2",
    "section": "3.1 Recap from previous week",
    "text": "3.1 Recap from previous week\nLets load some data first. This time to make it easier lets see if you can load data from my github.\nOur current outline for the next couple of weeks looks like the below.\n\nIntroduction into R programming language and getting started with some descriptive statistics. (last week)\nUnderstanding key terms; Exploratory analysis, Supervised vs Unsupervised problems, regression vs classification, prediction vs association.\nA quick note on distributions - Supervised regression and classification problems using a generalized linear model framework. (Hypothesis testing) (Next module)\n\n\n3.1.1 Loading packages and data\nWe will load some data that we used from the previous session.\n\npacman::p_load(data.table,tidyverse,htmlTable,factoextra,cluster,GGally)\n\n\nurlfile=\"https://raw.githubusercontent.com/R2mu/GWS_DSPR/main/data/mod2data.csv\"\n\ndata1 &lt;- fread(urlfile)\n\nLets just remind ourselves of the dataset we are using\n\nstr(data1)\n\nClasses 'data.table' and 'data.frame':  9936 obs. of  44 variables:\n $ SEASON_ID                : int  2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 ...\n $ MATCH_ID                 : int  266569840 266569840 266569840 266569840 266569840 266569840 266569840 266569840 266569840 266569840 ...\n $ GROUP_ROUND_NO           : int  1 1 1 1 1 1 1 1 1 1 ...\n $ VENUE_NAME               : chr  \"MCG\" \"MCG\" \"MCG\" \"MCG\" ...\n $ PERSON_ID                : int  250395 270146 270896 280819 290627 290847 293813 294036 294592 294674 ...\n $ FULLNAME                 : chr  \"Jack Riewoldt\" \"Ed Curnow\" \"Trent Cotchin\" \"Dylan Grimes\" ...\n $ SQUAD_NAME               : chr  \"Richmond\" \"Carlton\" \"Richmond\" \"Richmond\" ...\n $ Position                 : chr  \"Key Fwd\" \"Mid Fwd\" \"Mid Fwd\" \"Key Def\" ...\n $ OPP_SQUAD_NAME           : chr  \"Carlton\" \"Richmond\" \"Carlton\" \"Carlton\" ...\n $ SQUAD_MARGIN             : int  0 0 0 0 0 0 0 0 0 0 ...\n $ WL                       : int  0 0 0 0 0 0 0 0 0 0 ...\n $ WLD                      : int  0 0 0 0 0 0 0 0 0 0 ...\n $ BEHIND                   : int  0 0 0 0 1 0 3 0 0 1 ...\n $ CLEARANCE                : int  1 1 4 0 5 1 2 8 1 0 ...\n $ CONTESTED_MARK           : int  4 0 0 1 0 2 4 0 0 0 ...\n $ CONTESTED_POSSESSION     : int  10 2 11 4 13 10 9 14 4 2 ...\n $ CONTESTED_POSSESSION_POST: int  9 1 4 4 5 8 7 4 3 2 ...\n $ CONTESTED_POSSESSION_PRE : int  1 1 7 0 8 2 2 10 1 0 ...\n $ DISPOSAL                 : int  12 14 18 12 23 23 10 28 17 11 ...\n $ EFFECTIVE_DISPOSAL       : int  10 8 12 11 17 13 6 20 12 9 ...\n $ EFFECTIVE_HANDBALL       : int  3 2 7 4 10 6 1 14 4 2 ...\n $ EFFECTIVE_KICK           : int  7 6 5 7 7 7 5 6 8 7 ...\n $ GOAL                     : int  1 0 0 0 0 1 3 0 0 0 ...\n $ HANDBALL                 : int  3 2 9 4 13 8 1 18 6 2 ...\n $ HARD_BALL_GET            : int  0 1 3 1 1 4 2 3 0 1 ...\n $ HITOUT                   : int  1 0 0 0 0 0 1 0 0 0 ...\n $ IN50_KICK                : int  3 4 3 0 2 7 0 1 3 1 ...\n $ INSIDE_50                : int  4 4 3 0 2 8 1 2 3 2 ...\n $ INTERCEPT                : int  3 1 3 6 5 1 0 2 2 4 ...\n $ KICK                     : int  9 12 9 8 10 15 9 10 11 9 ...\n $ LONG_KICK                : int  0 1 3 2 4 3 1 0 3 3 ...\n $ MARK                     : int  6 6 3 5 4 6 6 5 5 4 ...\n $ MARK_ON_LEAD             : int  1 0 1 0 0 1 1 0 0 1 ...\n $ METRES_GAINED_EFF        : int  139 182 178 89 158 230 118 39 139 156 ...\n $ MISSED_TACKLE            : int  0 0 1 1 1 0 0 0 0 0 ...\n $ PLY_PRESS_PTS            : num  39.3 17.4 33.5 12 25.5 ...\n $ POINTS                   : int  6 0 0 0 1 6 21 0 0 1 ...\n $ RATING                   : num  18.2 3.9 8.2 6.4 7.9 13.7 13.2 12.5 2.3 4.2 ...\n $ SMOTHER                  : int  1 0 1 0 1 0 0 0 2 0 ...\n $ SPOIL                    : int  0 0 1 3 0 0 3 0 0 4 ...\n $ TACKLE                   : int  5 4 3 2 1 0 0 4 3 2 ...\n $ TOTAL_GAINED_METRES      : num  198 300 264 111 316 ...\n $ TURNOVER                 : int  3 5 5 1 4 7 2 2 3 1 ...\n $ UNCONTESTED_MARK         : int  2 6 3 4 4 4 2 5 5 4 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\nhead(data1)|&gt;htmlTable()\n\n\n\n\n\nSEASON_ID\nMATCH_ID\nGROUP_ROUND_NO\nVENUE_NAME\nPERSON_ID\nFULLNAME\nSQUAD_NAME\nPosition\nOPP_SQUAD_NAME\nSQUAD_MARGIN\nWL\nWLD\nBEHIND\nCLEARANCE\nCONTESTED_MARK\nCONTESTED_POSSESSION\nCONTESTED_POSSESSION_POST\nCONTESTED_POSSESSION_PRE\nDISPOSAL\nEFFECTIVE_DISPOSAL\nEFFECTIVE_HANDBALL\nEFFECTIVE_KICK\nGOAL\nHANDBALL\nHARD_BALL_GET\nHITOUT\nIN50_KICK\nINSIDE_50\nINTERCEPT\nKICK\nLONG_KICK\nMARK\nMARK_ON_LEAD\nMETRES_GAINED_EFF\nMISSED_TACKLE\nPLY_PRESS_PTS\nPOINTS\nRATING\nSMOTHER\nSPOIL\nTACKLE\nTOTAL_GAINED_METRES\nTURNOVER\nUNCONTESTED_MARK\n\n\n\n\n1\n2023\n266569840\n1\nMCG\n250395\nJack Riewoldt\nRichmond\nKey Fwd\nCarlton\n0\n0\n0\n0\n1\n4\n10\n9\n1\n12\n10\n3\n7\n1\n3\n0\n1\n3\n4\n3\n9\n0\n6\n1\n139\n0\n39.3\n6\n18.2\n1\n0\n5\n197.6\n3\n2\n\n\n2\n2023\n266569840\n1\nMCG\n270146\nEd Curnow\nCarlton\nMid Fwd\nRichmond\n0\n0\n0\n0\n1\n0\n2\n1\n1\n14\n8\n2\n6\n0\n2\n1\n0\n4\n4\n1\n12\n1\n6\n0\n182\n0\n17.4\n0\n3.9\n0\n0\n4\n300.3\n5\n6\n\n\n3\n2023\n266569840\n1\nMCG\n270896\nTrent Cotchin\nRichmond\nMid Fwd\nCarlton\n0\n0\n0\n0\n4\n0\n11\n4\n7\n18\n12\n7\n5\n0\n9\n3\n0\n3\n3\n3\n9\n3\n3\n1\n178\n1\n33.45\n0\n8.2\n1\n1\n3\n263.7\n5\n3\n\n\n4\n2023\n266569840\n1\nMCG\n280819\nDylan Grimes\nRichmond\nKey Def\nCarlton\n0\n0\n0\n0\n0\n1\n4\n4\n0\n12\n11\n4\n7\n0\n4\n1\n0\n0\n0\n6\n8\n2\n5\n0\n89\n1\n12\n0\n6.4\n0\n3\n2\n111\n1\n4\n\n\n5\n2023\n266569840\n1\nMCG\n290627\nDion Prestia\nRichmond\nMid\nCarlton\n0\n0\n0\n1\n5\n0\n13\n5\n8\n23\n17\n10\n7\n0\n13\n1\n0\n2\n2\n5\n10\n4\n4\n0\n158\n1\n25.5\n1\n7.9\n1\n0\n1\n316.3\n4\n4\n\n\n6\n2023\n266569840\n1\nMCG\n290847\nDustin Martin\nRichmond\nGen Fwd\nCarlton\n0\n0\n0\n0\n1\n2\n10\n8\n2\n23\n13\n6\n7\n1\n8\n4\n0\n7\n8\n1\n15\n3\n6\n1\n230\n0\n25.5\n6\n13.7\n0\n0\n0\n432.1\n7\n4\n\n\n\n\n\n\n\n3.1.2 Quick summary\nLets do some quick summaries in a similar fashion to what we did last time. To start of with I am going to have a look at some position summaries by win vs loss across key statistics.\nFirstly to make the analysis a little simpler I am going to make the dataframe into what is called a long format. Now to be fair it is possible to summarize keeping the data in its current wide format but being confident in manipulating data.frames from wide to long and vice versa is very useful for data analysis in general.\nBefore I do that however, lets just check something\n\nunique(data1$Position)\n\n[1] \"Key Fwd\" \"Mid Fwd\" \"Key Def\" \"Mid\"     \"Gen Fwd\" \"Wing\"    \"Gen Def\"\n[8] \"Ruck\"    \"\"       \n\n\nYou may notice a “” is returned this represents a blank value, which ideally there should not be any. Lets have a look a little deeper\n\ndata1|&gt;\n  filter(Position==\"\")|&gt;\n  select(FULLNAME,PERSON_ID,GROUP_ROUND_NO)\n\n        FULLNAME PERSON_ID GROUP_ROUND_NO\n 1: Willie Rioli    296225              2\n 2: Willie Rioli    296225              5\n 3: Willie Rioli    296225              7\n 4: Willie Rioli    296225              8\n 5: Willie Rioli    296225             11\n 6: Willie Rioli    296225             13\n 7: Willie Rioli    296225             19\n 8: Willie Rioli    296225             21\n 9: Willie Rioli    296225             22\n10: Willie Rioli    296225             23\n11: Willie Rioli    296225             24\n12: Willie Rioli    296225             25\n13: Willie Rioli    296225             26\n\n\nOkay this is because he was referred to as “Junior Rioli” that year which means ideally I need to fix the initial join i did from the previous week to just be PERSON_ID and SEASON_ID and not include FULLNAME.\nI know however, that he played as “Gen Fwd” that year so lets quickly update that in our database.\n\ndata1 &lt;- data1|&gt;\n         mutate(Position=ifelse(SEASON_ID==2023&PERSON_ID==296225,\n                                \"Gen Fwd\",Position))\n\nWe can check out work with\n\nunique(data1$Position)\n\n[1] \"Key Fwd\" \"Mid Fwd\" \"Key Def\" \"Mid\"     \"Gen Fwd\" \"Wing\"    \"Gen Def\"\n[8] \"Ruck\"   \n\n\nOkay lets now look to melt the data and drop some columns for the sake of it.\n\ndata.frame(names(data1))\n\n                names.data1.\n1                  SEASON_ID\n2                   MATCH_ID\n3             GROUP_ROUND_NO\n4                 VENUE_NAME\n5                  PERSON_ID\n6                   FULLNAME\n7                 SQUAD_NAME\n8                   Position\n9             OPP_SQUAD_NAME\n10              SQUAD_MARGIN\n11                        WL\n12                       WLD\n13                    BEHIND\n14                 CLEARANCE\n15            CONTESTED_MARK\n16      CONTESTED_POSSESSION\n17 CONTESTED_POSSESSION_POST\n18  CONTESTED_POSSESSION_PRE\n19                  DISPOSAL\n20        EFFECTIVE_DISPOSAL\n21        EFFECTIVE_HANDBALL\n22            EFFECTIVE_KICK\n23                      GOAL\n24                  HANDBALL\n25             HARD_BALL_GET\n26                    HITOUT\n27                 IN50_KICK\n28                 INSIDE_50\n29                 INTERCEPT\n30                      KICK\n31                 LONG_KICK\n32                      MARK\n33              MARK_ON_LEAD\n34         METRES_GAINED_EFF\n35             MISSED_TACKLE\n36             PLY_PRESS_PTS\n37                    POINTS\n38                    RATING\n39                   SMOTHER\n40                     SPOIL\n41                    TACKLE\n42       TOTAL_GAINED_METRES\n43                  TURNOVER\n44          UNCONTESTED_MARK\n\ndtLong = data1|&gt;\n        melt(id.vars = 1:12)|&gt;\n  ## going to remove some columns \n  filter(!variable %in%c(\"POINTS\",\"GOAL\",\"BEHIND\"))\n\nLets now create a quick summary of the stats. Now to reiterate what I mentioned last week, this is where R can be really useful. Effectively, we need to create multiple levels of aggregation to get to the level we want and in excel for example this would require multiple pivot tables.\n\ndtlongSum  &lt;- dtLong|&gt;\n  group_by(GROUP_ROUND_NO,SQUAD_NAME,Position,variable,WL)|&gt;\n  summarise(sum = round(sum(value,na.rm = T),2))|&gt;\n  # the above effectively summarised to the position level\n  ungroup()|&gt;\n  group_by(Position, variable,WL)|&gt;\n  summarise(mu = round(mean(sum),2),\n            sd = round(sd(sum),2))\n\n`summarise()` has grouped output by 'GROUP_ROUND_NO', 'SQUAD_NAME', 'Position',\n'variable'. You can override using the `.groups` argument.\n`summarise()` has grouped output by 'Position', 'variable'. You can override\nusing the `.groups` argument.\n\n # The above then averages the statistics via WL for each position over the season so effectively removes GROUP level data\n\n\n## As I was building it I would use the below as an example of checking as I go  \n#filter(SQUAD_NAME==\"Adelaide Crows\"&variable==\"CLEARANCE\"&Position==\"Mid\")\n\nhead(dtlongSum)|&gt;htmlTable()\n\n\n\n\n\nPosition\nvariable\nWL\nmu\nsd\n\n\n\n\n1\nGen Def\nCLEARANCE\n0\n3.53\n2.47\n\n\n2\nGen Def\nCLEARANCE\n1\n3.95\n2.72\n\n\n3\nGen Def\nCONTESTED_MARK\n0\n1.6\n1.49\n\n\n4\nGen Def\nCONTESTED_MARK\n1\n1.57\n1.26\n\n\n5\nGen Def\nCONTESTED_POSSESSION\n0\n24.42\n7.5\n\n\n6\nGen Def\nCONTESTED_POSSESSION\n1\n24.28\n7.26\n\n\n\n\n\nThe above is still quite a long data.frame with 464 rows which might not be the easiest way to see the differences so lets re shape it again to see if it helps.\n\ndcast.data.table(setDT(dtlongSum),Position+variable~WL,value.var = \"mu\")|&gt;\n  filter(variable==\"DISPOSAL\")|&gt;\n  mutate(difference = round(`1`-`0`,1))|&gt;\n  htmlTable()\n\n\n\n\n\nPosition\nvariable\n0\n1\ndifference\n\n\n\n\n1\nGen Def\nDISPOSAL\n89.28\n89.18\n-0.1\n\n\n2\nGen Fwd\nDISPOSAL\n51.44\n55.79\n4.4\n\n\n3\nKey Def\nDISPOSAL\n30.98\n34.39\n3.4\n\n\n4\nKey Fwd\nDISPOSAL\n23.05\n26.11\n3.1\n\n\n5\nMid\nDISPOSAL\n83.1\n87.57\n4.5\n\n\n6\nMid Fwd\nDISPOSAL\n28.22\n30.66\n2.4\n\n\n7\nRuck\nDISPOSAL\n17.06\n17.94\n0.9\n\n\n8\nWing\nDISPOSAL\n38.56\n38.76\n0.2\n\n\n\n\n\nMaybe we want to plot the differences\n\ndtLong|&gt;\n  group_by(MATCH_ID,SQUAD_NAME,WL,Position,variable)|&gt;\n  summarise(sum = sum(value))|&gt;\n  filter(variable%in%c(\"DISPOSAL\",\"CLEARANCE\"))|&gt;\n  #filter(Position%in%c(\"Mid\",\"Mid Fwd\",\"Wing\"))|&gt;\n  ggplot(aes(as.factor(WL),sum))+\n  geom_jitter(col=\"gray80\")+\n  geom_boxplot()+\n  stat_summary(fun.data = \"mean_sdl\",\n               geom = \"pointrange\",\n               fun.args = list(mult=1),col=\"black\")+\n  facet_grid(vars(variable), vars(Position),scales = \"free_y\")+\n  theme_bw()\n\n`summarise()` has grouped output by 'MATCH_ID', 'SQUAD_NAME', 'WL', 'Position'.\nYou can override using the `.groups` argument.\n\n\n\n\n\n\n\n3.1.3 Important note\nYou guys are the experts here in being able to double check what numbers to expect. I want to strongly reiterate how much of coding is just checking your work as you go, are you getting numbers you expect? As I mentioned the beauty of R being modular is it easily allows for you to continuously check as you build.\n\n\n3.1.4 Final exploration\nLets have a look at the global relationships across a range of variables.\n\nsum2 = dtLong|&gt;\n  group_by(GROUP_ROUND_NO,SQUAD_NAME,variable)|&gt;\n  summarise(sum = round(sum(value),2))|&gt;\n  pivot_wider(values_from = sum,\n              names_from = variable)|&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'GROUP_ROUND_NO', 'SQUAD_NAME'. You can\noverride using the `.groups` argument.\n\nsum2|&gt;head()|&gt;htmlTable()\n\n\n\n\n\nGROUP_ROUND_NO\nSQUAD_NAME\nCLEARANCE\nCONTESTED_MARK\nCONTESTED_POSSESSION\nCONTESTED_POSSESSION_POST\nCONTESTED_POSSESSION_PRE\nDISPOSAL\nEFFECTIVE_DISPOSAL\nEFFECTIVE_HANDBALL\nEFFECTIVE_KICK\nHANDBALL\nHARD_BALL_GET\nHITOUT\nIN50_KICK\nINSIDE_50\nINTERCEPT\nKICK\nLONG_KICK\nMARK\nMARK_ON_LEAD\nMETRES_GAINED_EFF\nMISSED_TACKLE\nPLY_PRESS_PTS\nRATING\nSMOTHER\nSPOIL\nTACKLE\nTOTAL_GAINED_METRES\nTURNOVER\nUNCONTESTED_MARK\n\n\n\n\n1\n1\nAdelaide Crows\n33\n9\n111\n77\n34\n318\n242\n89\n153\n101\n26\n40\n39\n52\n62\n217\n55\n102\n6\n3992\n4\n613.35\n178.6\n6\n30\n40\n6077.2\n56\n93\n\n\n2\n1\nBrisbane Lions\n38\n4\n116\n55\n61\n264\n181\n72\n109\n102\n33\n41\n35\n40\n52\n162\n62\n52\n6\n3879\n10\n650.4\n161.2\n7\n37\n46\n5309.8\n66\n48\n\n\n3\n1\nCarlton\n32\n18\n148\n100\n48\n341\n241\n92\n149\n120\n32\n26\n33\n45\n79\n221\n67\n97\n0\n4119\n10\n580.65\n186.2\n8\n29\n55\n5963.4\n75\n79\n\n\n4\n1\nCollingwood\n43\n10\n135\n79\n56\n372\n286\n120\n166\n143\n18\n45\n52\n62\n62\n229\n62\n105\n7\n4611\n8\n594.45\n250.5\n8\n32\n52\n6293\n61\n95\n\n\n5\n1\nEssendon\n32\n11\n128\n83\n45\n414\n322\n142\n180\n159\n28\n20\n54\n66\n63\n255\n45\n131\n6\n4607\n6\n521.25\n264.1\n4\n30\n37\n6993.8\n59\n120\n\n\n6\n1\nFremantle\n28\n14\n146\n102\n44\n437\n334\n145\n189\n179\n30\n45\n56\n65\n92\n258\n63\n136\n6\n4615\n4\n633.3\n200.2\n11\n24\n53\n6581.1\n82\n122\n\n\n\n\n\n\ncols &lt;- c(\"METRES_GAINED_EFF\", \"PLY_PRESS_PTS\",\"KICK\",\"HANDBALL\",\"MARK\",\"CONTESTED_POSSESSION\",\"TACKLE\")\n\nforPairs &lt;- sum2|&gt;\n           select(!1:2)|&gt;\n           select(cols)\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(cols)\n\n  # Now:\n  data %&gt;% select(all_of(cols))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\nggpairs(forPairs)+theme_light()"
  },
  {
    "objectID": "module2.html#exploratory-and-descriptive-analysis",
    "href": "module2.html#exploratory-and-descriptive-analysis",
    "title": "3  Module 2",
    "section": "3.2 Exploratory and Descriptive analysis",
    "text": "3.2 Exploratory and Descriptive analysis\nThe above is an example of both what we would consider EXPLORATORY and DESCRIPTIVE analysis. These analysis techniques are usually first steps we take when analysing data.\nExploratory analysis can be really useful for thing such as\n\nIdentifying outliers or anomalies in the data (very important)\nObserve if the problem is tractable/solvable (very important)\nAssess distributional assumptions of the data (more on this next week)\nVisual assessment of relationships or patterns that may exist within the data\n\nWhere as descriptive analysis refers to summarizing the main variables of interest. You are not necessarily looking to infer a relationship or hypothesis per se but just are reporting relevant summary statistics e.g. mean, max, min, range, standard deviation, median, sum, proportion,count relating to a question at hand.\n” The average disposal count for mid fielders was ….. for 2023 and …. for 2024.”\n” The proportion of time player x played up forward was …. vs ….. played down back”"
  },
  {
    "objectID": "module2.html#a-quick-disclaimer-regarding-exploratory-analysis",
    "href": "module2.html#a-quick-disclaimer-regarding-exploratory-analysis",
    "title": "3  Module 2",
    "section": "3.3 A quick disclaimer regarding exploratory analysis",
    "text": "3.3 A quick disclaimer regarding exploratory analysis\nWhile exploratory analysis is a powerful tool that is used almost subconsciously in our daily lives, it is important to exercise caution and maintain mental error control. The human brain is notoriously good at finding and justifying relationships in data that may genuinely just be noise. To highlight this problem, an additional chapter called “Multiple testing” was added to the second version of the ISLR book i recommend the previous module.\nHypothesis testing is one type of tool that aims to help reduce the risk of being misled by noise. Ultimately, however, you bear a significant responsibility to develop a hypothesis or causal framework that justifies why you think the observed trend may indeed be valid.\nWhilst, developing causal models can quickly become an extensive process a simply starting point is developing a Directed Acyclic Graph (DAG). A DAG is a graphical representation of a hypothetical model outlining causal effects. Outlined below is a quick example of how one may develop a dag.\nWe want to model the relationships between the following variables:\n\nParental Education (PE)\nStudent’s Motivation (SM)\nTime Spent Studying (TS)\nAttendance (A)\nAcademic Performance (AP)\n\nBelow we are going graphical represent relationships that we believe to impact these variables.\n\n\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n\n\n\n\n\n\n3.3.1 DAG Structure:\nFor the above figure we would interpret it like so:\n\nParental Education (PE) affects both Student’s Motivation (SM) and Time Spent Studying (TS).\nStudent’s Motivation (SM) influences Time Spent Studying (TS).\nTime Spent Studying (TS) and Attendance (A) both affect Academic Performance (AP).\nParental Education (PE) also directly affects Academic Performance (AP).\n\nWhile this may initially seem trivial, a well-thought-out DAG can help identify situations where causal analysis approaches can be explored using observational data—an area long thought to be exclusive to randomized controlled trials. See this paper by Dr Judd Kalkhoven an academic from WSU who gives some examples of causal modelling from an athlete injury perspective https://link.springer.com/article/10.1007/s40279-024-02008-1."
  },
  {
    "objectID": "module2.html#supervised-vs-unsupervised-problems",
    "href": "module2.html#supervised-vs-unsupervised-problems",
    "title": "3  Module 2",
    "section": "3.4 Supervised vs Unsupervised problems",
    "text": "3.4 Supervised vs Unsupervised problems\n\n3.4.1 Supervised Learning Overview\nIn our previous session, we explored our ability to predict score differential using a range of statistics. The simple linear model we examined was an example of a supervised regression problem. The easiest way to identify whether something can be thought of as a supervised problem is by determining whether you have a clear outcome of interest that you want to predict. If the answer is yes, then it is a supervised problem.\n\n\n3.4.2 Types of Supervised Problems\nIn reality, most of the problems/questions you encounter will relate to predicting some quantity, whether it be an absolute number such as score differential (a regression problem), win vs. loss probability (a classification problem), or predicting the next action type, e.g., kick, handball, tackle (a multinomial classification problem).\n\nPrediction of a number = Regression problem\nPrediction of a class/category = Classification problem\n\n\n\n3.4.3 Choosing a Model\nThere are often numerous models to answer the same regression or classification problem, and there will usually be trade-offs between certain models that you need to explore. A useful first step is asking yourself, “Is it important that I understand how the model made the prediction it did, or is it just important that the model can predict accurately?” This question can help you narrow down the scope of models you may use.\n\nThe above picture is taken from ISLR book I recommended (https://www.statlearning.com/) and it highlights some of these tradeoffs. Over the coming weeks we will explore models that exist along this continuum but I want you to also think of the x axis as dataset size required."
  },
  {
    "objectID": "module2.html#unsupervised-problems",
    "href": "module2.html#unsupervised-problems",
    "title": "3  Module 2",
    "section": "3.5 Unsupervised Problems",
    "text": "3.5 Unsupervised Problems\nLet’s say, for example, that you don’t have a specific dependent/outcome variable that you are interested in predicting. Instead, you are more interested in exploring whether there are any interesting patterns, clusters, or groupings that exist within the data. You are not too concerned with how they are structured, just whether there is a structure. This would be considered an unsupervised learning problem.\n\n3.5.1 Common Algorithms for Unsupervised Learning\n\nClustering Algorithms:\n\nAim to find multidimensional similarities between variables, grouping data points into clusters based on their features.\nExamples: K-means clustering, hierarchical clustering, DBSCAN, GMM.\n\nPrincipal Component Analysis (PCA):\n\nAims to find components that can best explain the variance between variables, reducing the dimensionality of the data while retaining most of the variance.\nUseful for data visualization and simplifying complex datasets.\n\nAssociation Rules / Market Basket Analysis:\n\nAim to find associations between items or transactions, identifying how frequently certain items or events occur together.\nExample: Market basket analysis can reveal that customers who buy bread often buy butter as well.\n\n\n\n\n3.5.2 Summary\nUnsupervised learning is about discovering the inherent structure within your data without predefined labels or outcomes. It’s particularly useful when you want to explore and understand the underlying patterns or relationships in your dataset.\nBelow is a quick example where we are using a variant of the kmeans clustering algorithm.\n\n## select some columns to analyse. I do suggest to do select only variables you are interested in. e.g. don;t just put everything in and hope for the best.\n\ncols &lt;- c(\"METRES_GAINED_EFF\", \"PLY_PRESS_PTS\",\"KICK\",\"HANDBALL\",\"MARK\",\"CONTESTED_POSSESSION\",\"count\")\n\n### create a quick summary of all players from 2023\n\nsmalldf &lt;- data1|&gt;\n  group_by(FULLNAME,Position)|&gt;\n  mutate(count = length(FULLNAME))|&gt;\n  group_by(FULLNAME,Position)|&gt;\n  ## the below is a way where you can summarise in the wide format\n  summarise(across(all_of(cols), mean))|&gt;\n  ungroup()|&gt;\n  ## going to remove players with less than 5 games\n  filter(!count&lt;=5)|&gt;\n  # dont need the count column after so going to remove\n  select(!count)\n\n`summarise()` has grouped output by 'FULLNAME'. You can override using the\n`.groups` argument.\n\n## outside the scope of but we need to remove players name from being a column variable to being a row now. \nsmallerdf &lt;- smalldf|&gt;\n             column_to_rownames(\"FULLNAME\")|&gt;\n             select(!Position)\n\n## We are going to compute a slightly more complicated kmeans algo \n# the reasoning is a little complicated but it just better matches our data. \n\n### compute gowers distance. \n# this calcualtes how far away each player is from each other across all statistics and represents as one number. \ndissimilarity_matrix &lt;- daisy(smallerdf,\n                               metric = \"gower\")\n\n## convert to a matrix\ngower_mat &lt;- as.matrix(dissimilarity_matrix)\n\n\n### find number of clusters within group. I know ahead of time that 3 is an okay number but usually you may need to explore a number of cluster sizes. \n\n\n## below is the clustering algo we will use\nset.seed(123) # For reproducibility\npam_result &lt;- pam(gower_mat, k = 3)\n\n## join computed cluster back on the inital dataframe\nsmalldf &lt;- smalldf|&gt;mutate(cluster = pam_result$clustering)\n\n## lets plot the results \nfviz_cluster(pam_result,geom = \"text\",\n               ellipse.type = \"convex\",\n               show.clust.cent = TRUE,\n               ggtheme = theme_classic(),\n               labelsize = 8)\n\n\n\n\nLets get a feel for what differentiates the clusters.\n\nclusterLong =setDT(smalldf)|&gt;melt.data.table(id.vars = c(\"FULLNAME\",\"Position\",\"cluster\"))\n\n\nggplot(clusterLong,aes(value,as.factor(cluster)))+\n  geom_jitter(aes(col=Position))+\n  stat_summary(fun.data = \"mean_sdl\",\n               geom = \"crossbar\",\n               fun.args = list(mult=1),col=\"black\")+\n  facet_wrap(~variable,scales = \"free\",ncol = 3)+\n  theme_light()+\n  theme(legend.position = \"top\")\n\n\n\n\nFinally, a question you may have from this which players are most similar to another player in a multidimensional case.\nFirstly, I need to create a helper function to do this. I know this may seem scary but I was able to do this within a couple of minutes of searching on google and using chatGPT.\n\n# You don;t need to necessarily understand what this function does but it allows us to find the most similar players based off the multidimensioal calculation we did before.\n\nfind_most_similar &lt;- function(df, gower_matrix, individual, n = 1) {\n  # Find the index of the selected individual\n  individual_index &lt;- which(df$FULLNAME == individual)\n  \n  # Get the distances for the selected individual\n  distances &lt;- gower_matrix[individual_index, ]\n  \n  # Set the distance to itself as Inf to exclude it from the nearest neighbors\n  distances[individual_index] &lt;- Inf\n  \n  # Find the indices of the n smallest distances\n  nearest_indices &lt;- order(distances)[1:n]\n  \n  # Include the selected individual in the result\n  all_indices &lt;- c(individual_index, nearest_indices)\n  \n  # Return the rows of the selected individual and nearest individuals\n  return(df[all_indices, ])\n}\n\nRun the helper function\n\nfind_most_similar(smalldf, gower_mat, 'Toby Greene', n = 3)|&gt;\n  mutate(across(where(is.numeric), round, 1))|&gt;htmlTable()\n\n\n\n\n\nFULLNAME\nPosition\nMETRES_GAINED_EFF\nPLY_PRESS_PTS\nKICK\nHANDBALL\nMARK\nCONTESTED_POSSESSION\ncluster\n\n\n\n\n1\nToby Greene\nGen Fwd\n217.6\n30.4\n12\n5.7\n4.4\n7.7\n2\n\n\n2\nChristian Salem\nGen Def\n212.2\n31.6\n12.5\n6.6\n3.9\n5.8\n2\n\n\n3\nSteele Sidebottom\nWing\n227.2\n32.8\n12.8\n8\n4.2\n6.9\n2\n\n\n4\nBen Ainsworth\nGen Fwd\n203\n26.9\n10.9\n6.8\n4.8\n6.5\n2\n\n\n\n\n\nAlways good to cross reference if you think these results make sense.\nWe will talk about some other use cases of unsupervised problems in later modules."
  },
  {
    "objectID": "module2.html#practice-exercises-module-2",
    "href": "module2.html#practice-exercises-module-2",
    "title": "3  Module 2",
    "section": "3.6 Practice exercises module 2",
    "text": "3.6 Practice exercises module 2\n\nThink about questions you may have from datasets you have used. What would you classify your question as? e.g. ( supervised, unsupervised, regression or classification)\nCould you defend or represent your problem as a DAG?\nCan you think of any questions that might be appropriate for an unsupervised learning problem?"
  },
  {
    "objectID": "module2.html#resources.",
    "href": "module2.html#resources.",
    "title": "3  Module 2",
    "section": "3.7 Resources.",
    "text": "3.7 Resources.\nISLR https://www.statlearning.com/"
  }
]