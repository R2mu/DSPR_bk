[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GWS Data Science and Problem Recognition",
    "section": "",
    "text": "Preface\nThis is a Quarto book is designed as a brief introduction to data science and problem/pattern recognition. Look to copy code chunks that exist throughout the book and try run them in your r studio console.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html#why-r",
    "href": "intro.html#why-r",
    "title": "1  Overview",
    "section": "1.1 Why R?",
    "text": "1.1 Why R?\nComparing between programming languages whilst fun is really not a useful exercise. Long story short, there a three programming languages currently that saturate sports analytics. Python, R and SQL. We are using R within R studio, namely because\n\nSlightly more user friendly for non programmers hence, faster to prototype ideas\nThere are more resources available for common analytically techniques that we will use in sport.\nRstudio is a fantastic programming IDE, you are able to also write, SQL, Python, Javascript and more whilst working in an R session.\n\nBelow is a photo which I think encapsulates the goal of R as a programming language very nicely.\n\nThe above photo is taken from https://r4ds.had.co.nz/introduction.html which is a fantastic resource for learning R.\nUltimately whatever you choose, it is better to get really good at one language than get just okay at a couple. You will find once you learn one it will become easier each time you attempt to learn another. Below are some of the aspects that I would consider strengths of other programming languages, below is very much a subjective spiel of my experience so take it with a grain of salt.\nPython: Is the most popular programming language at the moment and there is good reason for it. As the creator of Shiny R Joe Cheng said, “Python is the second-best language for anything”(Initial quote from Dan Callahan). In particular, Python will have a lot more resources available for deep learning, developing servers and working with external hardware or IOT applications. While there are definitely some questions in sports performance that could utilize deep learning approaches these are exceptions and not the rule in my opinion.\nSQL: Is mainly thought of a database language in that it is largely responsible for calling databases that are stored on servers. In particular, SQL utilities key verbs such as From, Select, Filter, Inset, Group, Join, Summarize. One of the reasons why I am teaching you tidyverse is as you will see later, a lot of the key verbs discussed above are used within dplyr, so by learning tidyverse (dplyr in particular) you are developing your SQL knowledge.\nJulia: A newer scientific programming language that is aiming to have the speed of compiled languages such as C++ but the readability of languages like R and Python. I’ve played around with it a little and it is quite a nice language, in particular its ability to program CUDA for utilizing GPU and also how easy it makes it to parallelize code. That being said, I haven’t had a use case yet where it has made sense for me to use it over R or Python, additionally as it is a newer language the amount of available support and information is lacking when compared to other languages.\nJavascript: Love hate relationship with this language. I love that it is non event blocking which means you can run some pretty fast real time simulations with packages such as D3. It is just a hard language to learn added with\n\n1.1.1 A quick note on best coding practices\nThis is a classic case of do as I say not as I do. I have no doubt over the coming months there will be times when I don’t adhere to some of my suggestions. However, I have tried to put down things I wish I did when I was starting my R journey.\n\nComment more than necessary. This can be really useful if you get stuck with code not working and you want to get advice from something like CHATGPT. An example comment may be something like “aggregate value column by group and plot as horizontal bar chart sorted from highest to lowest”\nDon’t be afraid to be overly modular with your code. This will make more sense later on, but is is better to slowly build up steps making sure they are correct then building once massive function then working backwards trying to solve where it may not be working\nAttempt to be consistent with naming conventions\n“We should forget about small inefficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.” Donald Knuth. This is an example of advice I should listen to as I will often spend an hour trying to optimize something that took 15 seconds to run down to 5 seconds.\nTry to start with the end in mind. What do you want your report to look like? What analysis method do you think it will be most appropriate? Are there visuals that you want to use? This should help you then work backwards to the data you current have available.\nBe open minded. Unfortunately/fortunately with many data science problems there are many ways to get to the same outcome."
  },
  {
    "objectID": "module1.html#installing-and-loading-r-packages",
    "href": "module1.html#installing-and-loading-r-packages",
    "title": "2  Module 1",
    "section": "2.1 Installing and loading R packages",
    "text": "2.1 Installing and loading R packages\nIf you have never installed an R package there is a couple of options which I have outlined below :\n\n# \"#\" Will comment out R code and will just print text\n# as I already have pacman loaded I will comment it out.\n# the short cut to remove comments from code is ctrl+shift+c\n\n#install.packages(\"pacman\")\n\npacman::p_load(tidyverse,data.table,httr,jsonlite,htmlTable,sf,tidytext)"
  },
  {
    "objectID": "module1.html#loading-data-locally",
    "href": "module1.html#loading-data-locally",
    "title": "2  Module 1",
    "section": "2.2 Loading data locally",
    "text": "2.2 Loading data locally\nThere a couple of different ways to load data. As I want to attempt to future proof your learning we will use the fread() function from the data.table package. Many options for reading in csv;s exist but this by far is one of the fastest ways to load data in R.\nOption 1: if the data is located in your directory it is a sample as.\n\n# option 1: if the data is located in your directory it is a sample as.\n\ndata1   &lt;- fread(\"PlayerLong.csv\")\nplaypos &lt;- fread(\"PlayPos3.csv\")\n\nOption 2: Data is located elsewhere on your computer you will need to put the full path of where it is located. A simple way to get started with your path is with the getwd() function\n\ngetwd()\n\n[1] \"C:/Users/Research/Desktop/GWS_DSPR\"\n\n\n\n#data1 &lt;- fread(\"/Users/Research/Dropbox/Rscripts/PlayerLong.csv\")\n\nOption 3: Finally if you are feeling lazy or struggling to put the correct path in you file.choose();\n\n#data1 &lt;- fread(file.choose())\n\n\n2.2.1 Loading data from API\nWith the way technologies are heading, connecting to API’s to extract data are only going to become more common. Hence, I will give a quick example of how you may do that in R. Unfortunately, due differing data privacy and safety protocols, some API’s will require different methods of securing a “handshake”. Below is a simple example that does not require authentication.\n\n## get current location of ISS\nurl &lt;- \"https://api.wheretheiss.at/v1/satellites/25544\"\n\nworld_coordinates &lt;- map_data(\"world\") \n\n\n  df  &lt;- fromJSON(url,simplifyDataFrame = T)|&gt;data.frame()\n  \n  #dff &lt;- rbind(dff,df)\n  \n\n  print(ggplot(df,aes(longitude,latitude))+\n          geom_map( \n            data = world_coordinates, map = world_coordinates, \n            aes(long,lat, map_id = region) \n          )+\n          geom_point(col=\"orange\",size=2)+\n          theme_bw())\n\nWarning in geom_map(data = world_coordinates, map = world_coordinates,\naes(long, : Ignoring unknown aesthetics: x and y\n\n\n\n\n\n\n\n2.2.2 Binding multiple CSVs from a folder\nSometimes you will have a list of multiple CSV’s that are the same in structure that just represent different dates of saving. In R it is relatively simple to loop through a directory and append all the files. There are multiple ways of doing this but here is one of the most concise ways I have come across.\n\n## directory\nloc &lt;- \"/Users/Research/Dropbox/Rscripts/listExample\"\n\n# gets a list of files in directoru\nfiles &lt;- list.files(path = loc, pattern = \"\\\\.csv$\",full.names = T)\n\n## loops through and binds them together. \ncombined_df &lt;- rbindlist(lapply(files, fread))\n\nunique(combined_df$FIXED_ID)\n\n[1] 106730101 106730102 106730103\n\nhead(combined_df[,.(MATCH_DATE,GROUP_ROUND_NO,HOME_SQUAD,AWAY_SQUAD,\n                    PERIOD,STATISTIC_CODE,FULLNAME)])|&gt;\n  htmlTable::htmlTable()\n\n\n\n\n\nMATCH_DATE\nGROUP_ROUND_NO\nHOME_SQUAD\nAWAY_SQUAD\nPERIOD\nSTATISTIC_CODE\nFULLNAME\n\n\n\n\n1\n28-FEB-19\n1\nCarlton\nEssendon\n1\nMTCHI\n\n\n\n2\n28-FEB-19\n1\nCarlton\nEssendon\n1\nPERST\n\n\n\n3\n28-FEB-19\n1\nCarlton\nEssendon\n1\nCEBO\n\n\n\n4\n28-FEB-19\n1\nCarlton\nEssendon\n1\nCEBO\n\n\n\n5\n28-FEB-19\n1\nCarlton\nEssendon\n1\nCBVS\nAndrew Phillips\n\n\n6\n28-FEB-19\n1\nCarlton\nEssendon\n1\nCBVS\nZac Clarke"
  },
  {
    "objectID": "module1.html#simple-data-cleaning-procedures",
    "href": "module1.html#simple-data-cleaning-procedures",
    "title": "2  Module 1",
    "section": "2.3 Simple data cleaning procedures",
    "text": "2.3 Simple data cleaning procedures\nIn this example we will be using data1 and playpos data.frames. We will explore how to filter, add variables (mutate) , join data bases and summaries. Below I will tidyverse packages to do this but you could also do everything below using the data.table package (my personal favorite) .\n\n# Lets have a look at column names from both \ncolnames(data1)\n\n [1] \"V1\"             \"MATCH_ID\"       \"GAME_ID\"        \"SEASON_ID\"     \n [5] \"GROUP_ROUND_NO\" \"VENUE_NAME\"     \"PERSON_ID\"      \"FULLNAME\"      \n [9] \"SQUAD_NAME\"     \"OPP_SQUAD_NAME\" \"SQUAD_MARGIN\"   \"variable\"      \n[13] \"value\"         \n\n\n\ncolnames(playpos)\n\n[1] \"FULLNAME\"  \"SEASON_ID\" \"PERSON_ID\" \"Position\" \n\n\nQuick way to get additional information about your data.\n\nstr(data1)\n\nClasses 'data.table' and 'data.frame':  2656638 obs. of  13 variables:\n $ V1            : int  1 2 3 4 5 6 7 8 9 10 ...\n $ MATCH_ID      : int  266569840 266569840 266569840 266569840 266569840 266569840 266569840 266569840 266569840 266569840 ...\n $ GAME_ID       : chr  \"R0123\" \"R0123\" \"R0123\" \"R0123\" ...\n $ SEASON_ID     : int  2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 ...\n $ GROUP_ROUND_NO: int  1 1 1 1 1 1 1 1 1 1 ...\n $ VENUE_NAME    : chr  \"MCG\" \"MCG\" \"MCG\" \"MCG\" ...\n $ PERSON_ID     : int  250395 270146 270896 280819 290627 290847 293813 294036 294592 294674 ...\n $ FULLNAME      : chr  \"Jack Riewoldt\" \"Ed Curnow\" \"Trent Cotchin\" \"Dylan Grimes\" ...\n $ SQUAD_NAME    : chr  \"Richmond\" \"Carlton\" \"Richmond\" \"Richmond\" ...\n $ OPP_SQUAD_NAME: chr  \"Carlton\" \"Richmond\" \"Carlton\" \"Carlton\" ...\n $ SQUAD_MARGIN  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ variable      : chr  \"BALL_UP_CLEARANCE\" \"BALL_UP_CLEARANCE\" \"BALL_UP_CLEARANCE\" \"BALL_UP_CLEARANCE\" ...\n $ value         : num  1 1 3 0 2 0 0 2 1 0 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n\nThe above can be a useful problem solver if you are struggling with code, as sometimes things that you might expect to be saved as a integer may be saved as a character. If you then tried to average a column that is considered to be characters you would run into trouble.\n\n2.3.1 Joins\nWe can notice that both data.frames have columns names called PERSON_ID, what we would like to do is join these databases by SEASON_ID & PERSON_ID so that we could have player position joined with the data1 database. To do this we are going to use a left_join() function from dplyr package, this package is loaded when you load Tidyverse .\nA quick note on joins. There are many different types of joins, Left, Right, Inner, Outer and Cross joins. I struggle to remember what they all represent namely because I have only ever really had to use Left joins and the occasional cross join. Below is an example of both a Left and Cross join. See https://r4ds.had.co.nz/relational-data.html#understanding-joins for more information regarding other join methods.\n\n## Cross join: I am going to use the data.table CJ function here just because it is already loaded and it is faster than tidyverse equivalent\n\nlist1 &lt;-seq(as.Date(\"2023-11-20\"), as.Date(\"2024-09-28\"), by=\"days\")\nlist2 &lt;- c(\"A\",\"B\",\"C\",\"D\")\n\nhead(CJ(list1,list2),12)|&gt;htmlTable::htmlTable()\n\n\n\n\n\nlist1\nlist2\n\n\n\n\n1\n2023-11-20\nA\n\n\n2\n2023-11-20\nB\n\n\n3\n2023-11-20\nC\n\n\n4\n2023-11-20\nD\n\n\n5\n2023-11-21\nA\n\n\n6\n2023-11-21\nB\n\n\n7\n2023-11-21\nC\n\n\n8\n2023-11-21\nD\n\n\n9\n2023-11-22\nA\n\n\n10\n2023-11-22\nB\n\n\n11\n2023-11-22\nC\n\n\n12\n2023-11-22\nD\n\n\n\n\n\n\n## now for a left join example\n## simple rule for left join, large merges with small, in our case data1 with playpos\n\ndata2 = left_join(data1,playpos, by =c(\"SEASON_ID\",\"PERSON_ID\",\"FULLNAME\"))\n\n#names(data2)\n\nhead(data2)|&gt;htmlTable::htmlTable()\n\n\n\n\n\nV1\nMATCH_ID\nGAME_ID\nSEASON_ID\nGROUP_ROUND_NO\nVENUE_NAME\nPERSON_ID\nFULLNAME\nSQUAD_NAME\nOPP_SQUAD_NAME\nSQUAD_MARGIN\nvariable\nvalue\nPosition\n\n\n\n\n1\n1\n266569840\nR0123\n2023\n1\nMCG\n250395\nJack Riewoldt\nRichmond\nCarlton\n0\nBALL_UP_CLEARANCE\n1\nKey Fwd\n\n\n2\n2\n266569840\nR0123\n2023\n1\nMCG\n270146\nEd Curnow\nCarlton\nRichmond\n0\nBALL_UP_CLEARANCE\n1\nMid Fwd\n\n\n3\n3\n266569840\nR0123\n2023\n1\nMCG\n270896\nTrent Cotchin\nRichmond\nCarlton\n0\nBALL_UP_CLEARANCE\n3\nMid Fwd\n\n\n4\n4\n266569840\nR0123\n2023\n1\nMCG\n280819\nDylan Grimes\nRichmond\nCarlton\n0\nBALL_UP_CLEARANCE\n0\nKey Def\n\n\n5\n5\n266569840\nR0123\n2023\n1\nMCG\n290627\nDion Prestia\nRichmond\nCarlton\n0\nBALL_UP_CLEARANCE\n2\nMid\n\n\n6\n6\n266569840\nR0123\n2023\n1\nMCG\n290847\nDustin Martin\nRichmond\nCarlton\n0\nBALL_UP_CLEARANCE\n0\nGen Fwd\n\n\n\n\n\n\n\n2.3.2 Selecting and filtering\nOkay , lets say after inspection of the data.frame we feel like some columns are redundant or we simply just want to move some columns around we will achieve this using the select verb. Additionally, lets say we want to just have the data frame represent certain positions and key variables (game statistics) we think are important, this can be achieved using the filter verb.\n\n# Firstly lets get a list of column names \nnames(data2)\n\n [1] \"V1\"             \"MATCH_ID\"       \"GAME_ID\"        \"SEASON_ID\"     \n [5] \"GROUP_ROUND_NO\" \"VENUE_NAME\"     \"PERSON_ID\"      \"FULLNAME\"      \n [9] \"SQUAD_NAME\"     \"OPP_SQUAD_NAME\" \"SQUAD_MARGIN\"   \"variable\"      \n[13] \"value\"          \"Position\"      \n\n\n\n# Lets have a look at what positions exist within the Position column\nunique(data2$Position)\n\n[1] \"Key Fwd\" \"Mid Fwd\" \"Key Def\" \"Mid\"     \"Gen Fwd\" \"Wing\"    \"Gen Def\"\n[8] \"Ruck\"    NA       \n\n\n\n# finally lets explore what statisitcs are within the variable column\nhead(unique(data2$variable),30)\n\n [1] \"BALL_UP_CLEARANCE\"         \"BALL_UP_FIRST_POSSESSION\" \n [3] \"BALL_UP_HITOUT\"            \"BALL_UP_HITOUT_SHARKED\"   \n [5] \"BAULKED\"                   \"BEHIND\"                   \n [7] \"BROKEN_TACKLE\"             \"BU_HITOUT_TO_ADVANTAGE\"   \n [9] \"CB_FIRST_POSSESSION\"       \"CB_HITOUT_SHARKED\"        \n[11] \"CB_HITOUT_TO_ADVANTAGE\"    \"CENTRE_BOUNCE_CLEARANCE\"  \n[13] \"CENTRE_BOUNCE_HITOUT\"      \"CLANGER\"                  \n[15] \"CLANGER_HANDBALL\"          \"CLANGER_KICK\"             \n[17] \"CLEARANCE\"                 \"CONTESTED_KNOCK_ON\"       \n[19] \"CONTESTED_MARK\"            \"CONTESTED_MARK_FROM_OPP\"  \n[21] \"CONTESTED_MARK_FROM_TEAM\"  \"CONTESTED_POSSESSION\"     \n[23] \"CONTESTED_POSSESSION_POST\" \"CONTESTED_POSSESSION_PRE\" \n[25] \"CRUMB\"                     \"DISPOSAL\"                 \n[27] \"DISPOSAL_POST\"             \"DISPOSAL_PRE\"             \n[29] \"EFFECTIVE_DISPOSAL\"        \"EFFECTIVE_HANDBALL\"       \n\n\n\n## select by number or name\ndata2 = data2|&gt;\n   select(4,2,5:9,14,10:13)|&gt;\n# If you want to remove a column you can do the below\n# select(!c(\"PERSON_ID\"))|&gt; this is an example how you may remove a specific column\n#   filter(Position %in% c(\"Mid Fwd\",\"Mid\"))|&gt;\n   filter(variable %in% c(\"CLEARANCE\",\"CONTESTED_POSSESSION\",\n                          \"EFFECTIVE_DISPOSAL\",\"TURNOVER\",\n                          \"TOTAL_GAINED_METRES\",\"PLY_PRESS_PTS\"))\n\n# Check variables\nunique(data2$variable)\n\n[1] \"CLEARANCE\"            \"CONTESTED_POSSESSION\" \"EFFECTIVE_DISPOSAL\"  \n[4] \"TOTAL_GAINED_METRES\"  \"TURNOVER\"             \"PLY_PRESS_PTS\"       \n\n\n\n\n2.3.3 Adding calculated columns\nLets add a calculated column to the data frame using the mutate function. The column we are going to add is going to convert the score differential into a binary win loss where 0 represents a loss and 1 a win.\n\ndata2 &lt;- data2|&gt;mutate(WL = ifelse(SQUAD_MARGIN&gt;0,1,0))\n\n# if we wanted to have draw we could simply do \n\ndata2 &lt;- data2|&gt;mutate(WLD = ifelse(SQUAD_MARGIN&gt;0,1,\n                                    ifelse(SQUAD_MARGIN==0,0,-1)))\n\n# We don't have to break it up as we did above we could simply add two columns at once\ndata2 &lt;- data2|&gt;mutate(WL = ifelse(SQUAD_MARGIN&gt;0,1,0),\n                       WLD = ifelse(SQUAD_MARGIN&gt;0,1,\n                                    ifelse(SQUAD_MARGIN==0,0,-1)))\n\ndata2|&gt;\n  tail(10)|&gt;\n  htmlTable()\n\n\n\n\n\nSEASON_ID\nMATCH_ID\nGROUP_ROUND_NO\nVENUE_NAME\nPERSON_ID\nFULLNAME\nSQUAD_NAME\nPosition\nOPP_SQUAD_NAME\nSQUAD_MARGIN\nvariable\nvalue\nWL\nWLD\n\n\n\n\n1\n2024\n149741648\n7\nMCG\n1013133\nBraeden Campbell\nSydney Swans\nWing\nHawthorn\n76\nPLY_PRESS_PTS\n3.75\n1\n1\n\n\n2\n2024\n149741648\n7\nMCG\n1013230\nLogan McDonald\nSydney Swans\nKey Fwd\nHawthorn\n76\nPLY_PRESS_PTS\n22.8\n1\n1\n\n\n3\n2024\n149741648\n7\nMCG\n1013409\nJames Jordon\nSydney Swans\nMid Fwd\nHawthorn\n76\nPLY_PRESS_PTS\n29.1\n1\n1\n\n\n4\n2024\n149741648\n7\nMCG\n1017091\nJai Serong\nHawthorn\nKey Def\nSydney Swans\n-76\nPLY_PRESS_PTS\n1.2\n0\n-1\n\n\n5\n2024\n149741648\n7\nMCG\n1017094\nConnor Macdonald\nHawthorn\nGen Fwd\nSydney Swans\n-76\nPLY_PRESS_PTS\n25.65\n0\n-1\n\n\n6\n2024\n149741648\n7\nMCG\n1018016\nSeamus Mitchell\nHawthorn\nGen Def\nSydney Swans\n-76\nPLY_PRESS_PTS\n9.15\n0\n-1\n\n\n7\n2024\n149741648\n7\nMCG\n1020895\nJai Newcombe\nHawthorn\nMid\nSydney Swans\n-76\nPLY_PRESS_PTS\n26.55\n0\n-1\n\n\n8\n2024\n149741648\n7\nMCG\n1023482\nCam Mackenzie\nHawthorn\nMid\nSydney Swans\n-76\nPLY_PRESS_PTS\n43.5\n0\n-1\n\n\n9\n2024\n149741648\n7\nMCG\n1027935\nJosh Weddle\nHawthorn\nKey Def\nSydney Swans\n-76\nPLY_PRESS_PTS\n20.55\n0\n-1\n\n\n10\n2024\n149741648\n7\nMCG\n1027965\nMax Ramsden\nHawthorn\nKey Fwd\nSydney Swans\n-76\nPLY_PRESS_PTS\n9.6\n0\n-1\n\n\n\n\n\n\n\n2.3.4 Adding calculated columns by group\n\ndata2 &lt;- data2|&gt;\n  group_by(variable,Position)|&gt;\n  # if you wanted to specific by position you could do the below\n  #group_by(SEASON_ID,Position,variable)|&gt;\n  mutate(avgByVar = round(mean(value),2))|&gt;\n  ungroup()|&gt;\n  mutate(diff = round(value - avgByVar,2))\n  \nhtmlTable(head(data2,10))\n\n\n\n\n\nSEASON_ID\nMATCH_ID\nGROUP_ROUND_NO\nVENUE_NAME\nPERSON_ID\nFULLNAME\nSQUAD_NAME\nPosition\nOPP_SQUAD_NAME\nSQUAD_MARGIN\nvariable\nvalue\nWL\nWLD\navgByVar\ndiff\n\n\n\n\n1\n2023\n266569840\n1\nMCG\n250395\nJack Riewoldt\nRichmond\nKey Fwd\nCarlton\n0\nCLEARANCE\n1\n0\n0\n0.47\n0.53\n\n\n2\n2023\n266569840\n1\nMCG\n270146\nEd Curnow\nCarlton\nMid Fwd\nRichmond\n0\nCLEARANCE\n1\n0\n0\n2.08\n-1.08\n\n\n3\n2023\n266569840\n1\nMCG\n270896\nTrent Cotchin\nRichmond\nMid Fwd\nCarlton\n0\nCLEARANCE\n4\n0\n0\n2.08\n1.92\n\n\n4\n2023\n266569840\n1\nMCG\n280819\nDylan Grimes\nRichmond\nKey Def\nCarlton\n0\nCLEARANCE\n0\n0\n0\n0.2\n-0.2\n\n\n5\n2023\n266569840\n1\nMCG\n290627\nDion Prestia\nRichmond\nMid\nCarlton\n0\nCLEARANCE\n5\n0\n0\n4.56\n0.44\n\n\n6\n2023\n266569840\n1\nMCG\n290847\nDustin Martin\nRichmond\nGen Fwd\nCarlton\n0\nCLEARANCE\n1\n0\n0\n0.87\n0.13\n\n\n7\n2023\n266569840\n1\nMCG\n293813\nTom Lynch\nRichmond\nKey Fwd\nCarlton\n0\nCLEARANCE\n2\n0\n0\n0.47\n1.53\n\n\n8\n2023\n266569840\n1\nMCG\n294036\nGeorge Hewett\nCarlton\nMid\nRichmond\n0\nCLEARANCE\n8\n0\n0\n4.56\n3.44\n\n\n9\n2023\n266569840\n1\nMCG\n294592\nKamdyn McIntosh\nRichmond\nWing\nCarlton\n0\nCLEARANCE\n1\n0\n0\n1.48\n-0.48\n\n\n10\n2023\n266569840\n1\nMCG\n294674\nNick Vlastuin\nRichmond\nGen Def\nCarlton\n0\nCLEARANCE\n0\n0\n0\n0.75\n-0.75\n\n\n\n\n\n\n\n2.3.5 Summarizing data (creating pivot tables)\nOkay the next section is going to go over how we can create pivot tables using the summarise function from the dplyr package which is loaded when you load tidyverse . I will do a couple of different examples of data summaries you may be interested in making using the data-set at hand. I will also show examples of plots as this can be a quick way to just double check your aggregation procedures\n\nlibrary(tidytext)\n\ndata2|&gt;\n  filter(SEASON_ID==2023)|&gt;\n  group_by(SEASON_ID,SQUAD_NAME,GROUP_ROUND_NO,variable)%&gt;%\n  summarise(sum = sum(value))%&gt;%\n  ungroup()|&gt;\n  group_by(SEASON_ID,SQUAD_NAME,variable)%&gt;%\n  summarise(season_avg = mean(sum),\n            season_sd  = sd(sum),\n            season_max = max(sum),\n            season_sum = sum(sum))|&gt;\n    ungroup()|&gt;\n  #arrange(variable, desc(season_avg))|&gt;\n  # Reorder SQUAD_NAME based on season_avg, within each variable facet\n  mutate(SQUAD_NAME_RO = reorder_within(SQUAD_NAME, season_sum, variable))|&gt;\n  ggplot(aes(season_sum,SQUAD_NAME_RO,col=as.factor(SEASON_ID)))+\n  geom_point()+\n  facet_wrap(~variable, scales = \"free\", labeller = label_wrap_gen(width = 10)) +\n  scale_y_reordered() +  # Necessary to apply the custom ordering\n  theme_bw() +\n  theme(legend.position = \"top\",\n        axis.text = element_text(size = 6))\n\n`summarise()` has grouped output by 'SEASON_ID', 'SQUAD_NAME',\n'GROUP_ROUND_NO'. You can override using the `.groups` argument.\n`summarise()` has grouped output by 'SEASON_ID', 'SQUAD_NAME'. You can override\nusing the `.groups` argument.\n\n\n\n\n\nYou don’t need to necessarily know what the code below is doing for now but It is just going to shorten the club names.\nIn this next example lets have a quick look at the average stats across key variables as a function of score differential and or Win vs Loss.\n\ndata2|&gt;\n  filter(SEASON_ID==2023)|&gt;\n  group_by(SQUAD_NAME,GROUP_ROUND_NO,variable,WL)|&gt;\n  summarise(sum = sum(value))|&gt;\n  ggplot(aes(as.factor(WL),sum))+\n  geom_jitter()+\n  geom_boxplot()+\n  theme_bw()+\n  facet_wrap(~variable,scales = \"free\")\n\n`summarise()` has grouped output by 'SQUAD_NAME', 'GROUP_ROUND_NO', 'variable'.\nYou can override using the `.groups` argument.\n\n\n\n\n\n\ndata2|&gt;\n  filter(SEASON_ID==2023)|&gt;\n  group_by(SQUAD_NAME,GROUP_ROUND_NO,variable,SQUAD_MARGIN)|&gt;\n  summarise(sum = sum(value))|&gt;\n  ggplot(aes(SQUAD_MARGIN,sum, col = SQUAD_NAME))+\n  geom_jitter(col=\"gray80\",alpha=.3)+\n  stat_smooth(method = \"lm\",se=F)+\n  theme_bw()+\n  theme(legend.position=\"top\")+\n  facet_wrap(~variable,scales = \"free\")\n\n`summarise()` has grouped output by 'SQUAD_NAME', 'GROUP_ROUND_NO', 'variable'.\nYou can override using the `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nLets delve a little deeper in to CONTESTED_POSSESSION\n\ndata2|&gt;\n  filter(SEASON_ID==2023)|&gt;\n  group_by(SQUAD_NAME,GROUP_ROUND_NO,variable,SQUAD_MARGIN)|&gt;\n  summarise(sum = sum(value))|&gt;\n  filter(variable==\"CONTESTED_POSSESSION\")|&gt;\n  ggplot(aes(SQUAD_MARGIN,sum, col = SQUAD_NAME))+\n  geom_jitter(col=\"gray80\",alpha=.3)+\n  stat_smooth(method = \"lm\",se=F)+\n  theme_bw()+\n  facet_wrap(~variable,scales = \"free\")\n\n`summarise()` has grouped output by 'SQUAD_NAME', 'GROUP_ROUND_NO', 'variable'.\nYou can override using the `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "module1.html#running-your-first-model",
    "href": "module1.html#running-your-first-model",
    "title": "2  Module 1",
    "section": "2.4 Running your first model",
    "text": "2.4 Running your first model\nOkay we are going to run our first statistical model. The simple linear model we will use in this case isn’t technically appropriate to use for the data set at hand but we will improve on this over the coming months. Before we can run the model we need to change the shape of the data-frame from its “long” format to a “wider” format. We can do that using the spread function.\nPersonally, I prefer using dcast.dafwrifwta.table function for this but I am trying to be consistent within the tidyverse for you here.\n\nwide_TV = data2|&gt;\n  filter(SEASON_ID==2023)|&gt;\n # filter(Position%in%c(\"Mid\",\"Mid Fwd\"))|&gt;\n  group_by(SQUAD_NAME,GROUP_ROUND_NO,variable,SQUAD_MARGIN)|&gt;\n  summarise(sum = sum(value))|&gt;\n  spread(key = c(\"variable\"),value = sum)\n\n`summarise()` has grouped output by 'SQUAD_NAME', 'GROUP_ROUND_NO', 'variable'.\nYou can override using the `.groups` argument.\n\n## the data.table method i prefer\nwide_dt = setDT(data2)[SEASON_ID==2023,#&Position%in%c(\"Mid\",\"Mid Fwd\"),\n         ][, .(sum = sum(value)),\n              by=.(SQUAD_NAME,GROUP_ROUND_NO,variable,SQUAD_MARGIN)\n          ][,dcast.data.table(.SD,...~variable,value.var = \"sum\")\n          ]\n\n#wide_TV|&gt;head()|&gt;htmlTable()\nwide_dt|&gt;head()|&gt;htmlTable()\n\n\n\n\n\nSQUAD_NAME\nGROUP_ROUND_NO\nSQUAD_MARGIN\nCLEARANCE\nCONTESTED_POSSESSION\nEFFECTIVE_DISPOSAL\nPLY_PRESS_PTS\nTOTAL_GAINED_METRES\nTURNOVER\n\n\n\n\n1\nAdelaide Crows\n1\n-16\n33\n111\n242\n613.35\n6077.2\n56\n\n\n2\nAdelaide Crows\n2\n-32\n33\n139\n253\n601.2\n5950.1\n62\n\n\n3\nAdelaide Crows\n3\n31\n39\n146\n251\n577.8\n5882.4\n50\n\n\n4\nAdelaide Crows\n4\n39\n35\n151\n276\n592.8\n6096.5\n71\n\n\n5\nAdelaide Crows\n5\n56\n37\n156\n302\n626.25\n6373\n67\n\n\n6\nAdelaide Crows\n6\n3\n47\n145\n255\n686.1\n5902\n66\n\n\n\n\n\nLets run a multiple regression model\n\npacman::p_load(broom,equatiomatic,lme4,mgcv)\n\nmodel_lm &lt;- lm(SQUAD_MARGIN~CONTESTED_POSSESSION+EFFECTIVE_DISPOSAL+CLEARANCE+TOTAL_GAINED_METRES+TURNOVER,data = wide_dt)\n\nmodel_lmer &lt;- lmer(SQUAD_MARGIN~CONTESTED_POSSESSION+EFFECTIVE_DISPOSAL+CLEARANCE+TOTAL_GAINED_METRES+TURNOVER+\n  (CONTESTED_POSSESSION+CLEARANCE||SQUAD_NAME),data = wide_dt)\n\nboundary (singular) fit: see help('isSingular')\n\nwide_dt$SQUAD_NAME &lt;-as.factor(wide_dt$SQUAD_NAME)\n\nmodel_gam &lt;- mgcv::gam(SQUAD_MARGIN~s(CONTESTED_POSSESSION)+s(EFFECTIVE_DISPOSAL)+\n                      s(TURNOVER)+s(TOTAL_GAINED_METRES)+s(CLEARANCE)+\n                      s(SQUAD_NAME,bs=\"re\"),data = wide_dt)\n\n#equatiomatic::extract_eq(first_model)\n\n#equatiomatic::extract_eq(first_model, use_coefs = TRUE)\n\nBelow is a summary of the output\n\nbroom::tidy(model_lm,conf.int = T,conf.level = .95)|&gt;\n       mutate_if(is.numeric, round, 2)|&gt;\n  htmlTable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n1\n(Intercept)\n-326.69\n19.66\n-16.62\n0\n-365.33\n-288.06\n\n\n2\nCONTESTED_POSSESSION\n0.44\n0.12\n3.68\n0\n0.21\n0.68\n\n\n3\nEFFECTIVE_DISPOSAL\n0.27\n0.05\n5.62\n0\n0.17\n0.36\n\n\n4\nCLEARANCE\n0.85\n0.23\n3.62\n0\n0.39\n1.31\n\n\n5\nTOTAL_GAINED_METRES\n0.05\n0\n14.25\n0\n0.04\n0.05\n\n\n6\nTURNOVER\n-1.49\n0.15\n-10.2\n0\n-1.77\n-1.2\n\n\n\n\n\nMaybe we want to visualize the above\n\nbroom::tidy(model_lm,conf.int = T,conf.level = .95)|&gt;\n       mutate_if(is.numeric, round, 2)|&gt;\n       filter(term%in%c(\"TURNOVER\",\"TOTAL_GAINED_METRES\",\"CLEARANCE\",\n                        \"EFFECTIVE_DISPOSAL\",\"CONTESTED_POSSESSION\"))|&gt;\n       ggplot(aes(estimate,term,xmin=conf.low,xmax=conf.high))+\n       geom_pointrange()+\n       theme_bw()\n\n\n\n\nNow there are many issues with this first figure. Firstly, as they are all on different scales the figure above could potentially exaggerate the impact of turnover when compared to other variables such as metres gained. Also the p values and subsequent confidence intervals are incorrect as we have violated a couple of statistical assumptions with just using a basic linear model.\nPartial dependency plots offer a better way of assessing the impact of a predictor. Effectively, you can think of it as the predicted impact of changing a variable whilst holding other variables constant.\n\npacman::p_load(pdp)\npartial(model_gam, pred.var = c(\"TURNOVER\"), \n        plot = TRUE,plot.engine = \"ggplot\",ice=T,)+\n       geom_hline(yintercept = 0)+\n       theme_bw()\n\n\n\n\nWhat about metres gained\n\npartial(model_gam, pred.var = c(\"TOTAL_GAINED_METRES\"), \n        plot = TRUE,plot.engine = \"ggplot\",ice=T,)+\n       geom_hline(yintercept = 0)+\n       theme_bw()\n\n\n\n\n\npartial(model_gam, pred.var = c(\"TOTAL_GAINED_METRES\",\"TURNOVER\"), \n        plot = TRUE,\n        plot.engine = \"ggplot\",\n        chull = T)+\n       geom_hline(yintercept = 0)+\n       theme_bw()\n\n\n\n\nLets have a look at how well our model performed\n\n## calculate predicted model values from data obtained along with some other summary #stats\nwide_dt &lt;- wide_dt|&gt;\n           mutate(predicted = predict(model_lm),\n                  error = SQUAD_MARGIN - predicted,\n                  absError = abs(error),\n                  squError = error^2)\n\nLets have a look at how well or model can do at predicting SQUAD_MARGIN\n\n ggplot(wide_dt,aes(SQUAD_MARGIN,predicted))+\n   geom_point()+\n#   geom_smooth(method = \"lm\",se=F)+\n   geom_abline(intercept = 0,slope = 1,col=\"red\",linetype=\"dashed\")+\n   theme_bw()\n\n\n\n\nOkay, visually the performance looks pretty bad, ideally most of the dots would fit along the red line. It looks like the model is struggling to pick up the magnitude of wins and loses correctly. Lets numerically summarize this.\n\n## mean absolute error\nround(mean(wide_dt$absError),1)\n\n[1] 21.1\n\n## Root mean squred error\nround(sqrt(mean(wide_dt$squError)),1)\n\n[1] 26.7\n\n\nOkay, so we can say that using a simple multiple regression model on just mid and mid fwd data across the variables we looked at is not doing a great job at predicting SQUAD_MARGIN."
  },
  {
    "objectID": "module1.html#practice-exercises",
    "href": "module1.html#practice-exercises",
    "title": "2  Module 1",
    "section": "2.5 Practice exercises",
    "text": "2.5 Practice exercises\n\nDownload R and Quarto https://quarto.org/docs/download/ if you haven’t already.\nLoad some data into R and attempt some basic data manipulation\nAttempt to build a basic plot\nStart to think about questions you might have regarding data you have access to.\nLook to annoy Isaac atleast once over the next two weeks with a problem you might be having."
  },
  {
    "objectID": "module1.html#additional-resources",
    "href": "module1.html#additional-resources",
    "title": "2  Module 1",
    "section": "2.6 Additional resources",
    "text": "2.6 Additional resources\nplotting in R using GGPLOT: https://ggplot2-book.org/\nFree R for data science resource: https://r4ds.had.co.nz/introduction.html\nFree ISLR resource : https://www.statlearning.com/\nWhilst, we wont use tidymodels too much I would encourage you to be across it https://www.tidymodels.org/start/ as it is a very powerful modular way for machine learning in R.\nyoutube:\nhttps://www.youtube.com/@TidyX_screencast\nhttps://www.youtube.com/@JuliaSilge/videos\nand of course CHATGPT.\nI will continue to update this as I remember more of the resources I have come across."
  },
  {
    "objectID": "module2.html#recap-from-previous-week",
    "href": "module2.html#recap-from-previous-week",
    "title": "3  Module 2",
    "section": "3.1 Recap from previous week",
    "text": "3.1 Recap from previous week\nLets load some data first. This time to make it easier lets see if you can load data from my github.\nOur current outline for the next couple of weeks looks like the below.\n\nIntroduction into R programming language and getting started with some descriptive statistics. (last week)\nUnderstanding key terms; Exploratory analysis, Supervised vs Unsupervised problems, regression vs classification, prediction vs association.\nA quick note on distributions - Supervised regression and classification problems using a generalized linear model framework. (Hypothesis testing) (Next module)\n\n\n3.1.1 Loading packages and data\nWe will load some data that we used from the previous session.\n\npacman::p_load(data.table,tidyverse,htmlTable,factoextra,cluster,GGally)\n\n\nurlfile=\"https://raw.githubusercontent.com/R2mu/GWS_DSPR/main/data/mod2data.csv\"\n\ndata1 &lt;- fread(urlfile)\n\nLets just remind ourselves of the dataset we are using\n\nstr(data1)\n\nClasses 'data.table' and 'data.frame':  9936 obs. of  44 variables:\n $ SEASON_ID                : int  2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 ...\n $ MATCH_ID                 : int  266569840 266569840 266569840 266569840 266569840 266569840 266569840 266569840 266569840 266569840 ...\n $ GROUP_ROUND_NO           : int  1 1 1 1 1 1 1 1 1 1 ...\n $ VENUE_NAME               : chr  \"MCG\" \"MCG\" \"MCG\" \"MCG\" ...\n $ PERSON_ID                : int  250395 270146 270896 280819 290627 290847 293813 294036 294592 294674 ...\n $ FULLNAME                 : chr  \"Jack Riewoldt\" \"Ed Curnow\" \"Trent Cotchin\" \"Dylan Grimes\" ...\n $ SQUAD_NAME               : chr  \"Richmond\" \"Carlton\" \"Richmond\" \"Richmond\" ...\n $ Position                 : chr  \"Key Fwd\" \"Mid Fwd\" \"Mid Fwd\" \"Key Def\" ...\n $ OPP_SQUAD_NAME           : chr  \"Carlton\" \"Richmond\" \"Carlton\" \"Carlton\" ...\n $ SQUAD_MARGIN             : int  0 0 0 0 0 0 0 0 0 0 ...\n $ WL                       : int  0 0 0 0 0 0 0 0 0 0 ...\n $ WLD                      : int  0 0 0 0 0 0 0 0 0 0 ...\n $ BEHIND                   : int  0 0 0 0 1 0 3 0 0 1 ...\n $ CLEARANCE                : int  1 1 4 0 5 1 2 8 1 0 ...\n $ CONTESTED_MARK           : int  4 0 0 1 0 2 4 0 0 0 ...\n $ CONTESTED_POSSESSION     : int  10 2 11 4 13 10 9 14 4 2 ...\n $ CONTESTED_POSSESSION_POST: int  9 1 4 4 5 8 7 4 3 2 ...\n $ CONTESTED_POSSESSION_PRE : int  1 1 7 0 8 2 2 10 1 0 ...\n $ DISPOSAL                 : int  12 14 18 12 23 23 10 28 17 11 ...\n $ EFFECTIVE_DISPOSAL       : int  10 8 12 11 17 13 6 20 12 9 ...\n $ EFFECTIVE_HANDBALL       : int  3 2 7 4 10 6 1 14 4 2 ...\n $ EFFECTIVE_KICK           : int  7 6 5 7 7 7 5 6 8 7 ...\n $ GOAL                     : int  1 0 0 0 0 1 3 0 0 0 ...\n $ HANDBALL                 : int  3 2 9 4 13 8 1 18 6 2 ...\n $ HARD_BALL_GET            : int  0 1 3 1 1 4 2 3 0 1 ...\n $ HITOUT                   : int  1 0 0 0 0 0 1 0 0 0 ...\n $ IN50_KICK                : int  3 4 3 0 2 7 0 1 3 1 ...\n $ INSIDE_50                : int  4 4 3 0 2 8 1 2 3 2 ...\n $ INTERCEPT                : int  3 1 3 6 5 1 0 2 2 4 ...\n $ KICK                     : int  9 12 9 8 10 15 9 10 11 9 ...\n $ LONG_KICK                : int  0 1 3 2 4 3 1 0 3 3 ...\n $ MARK                     : int  6 6 3 5 4 6 6 5 5 4 ...\n $ MARK_ON_LEAD             : int  1 0 1 0 0 1 1 0 0 1 ...\n $ METRES_GAINED_EFF        : int  139 182 178 89 158 230 118 39 139 156 ...\n $ MISSED_TACKLE            : int  0 0 1 1 1 0 0 0 0 0 ...\n $ PLY_PRESS_PTS            : num  39.3 17.4 33.5 12 25.5 ...\n $ POINTS                   : int  6 0 0 0 1 6 21 0 0 1 ...\n $ RATING                   : num  18.2 3.9 8.2 6.4 7.9 13.7 13.2 12.5 2.3 4.2 ...\n $ SMOTHER                  : int  1 0 1 0 1 0 0 0 2 0 ...\n $ SPOIL                    : int  0 0 1 3 0 0 3 0 0 4 ...\n $ TACKLE                   : int  5 4 3 2 1 0 0 4 3 2 ...\n $ TOTAL_GAINED_METRES      : num  198 300 264 111 316 ...\n $ TURNOVER                 : int  3 5 5 1 4 7 2 2 3 1 ...\n $ UNCONTESTED_MARK         : int  2 6 3 4 4 4 2 5 5 4 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\nhead(data1)|&gt;htmlTable()\n\n\n\n\n\nSEASON_ID\nMATCH_ID\nGROUP_ROUND_NO\nVENUE_NAME\nPERSON_ID\nFULLNAME\nSQUAD_NAME\nPosition\nOPP_SQUAD_NAME\nSQUAD_MARGIN\nWL\nWLD\nBEHIND\nCLEARANCE\nCONTESTED_MARK\nCONTESTED_POSSESSION\nCONTESTED_POSSESSION_POST\nCONTESTED_POSSESSION_PRE\nDISPOSAL\nEFFECTIVE_DISPOSAL\nEFFECTIVE_HANDBALL\nEFFECTIVE_KICK\nGOAL\nHANDBALL\nHARD_BALL_GET\nHITOUT\nIN50_KICK\nINSIDE_50\nINTERCEPT\nKICK\nLONG_KICK\nMARK\nMARK_ON_LEAD\nMETRES_GAINED_EFF\nMISSED_TACKLE\nPLY_PRESS_PTS\nPOINTS\nRATING\nSMOTHER\nSPOIL\nTACKLE\nTOTAL_GAINED_METRES\nTURNOVER\nUNCONTESTED_MARK\n\n\n\n\n1\n2023\n266569840\n1\nMCG\n250395\nJack Riewoldt\nRichmond\nKey Fwd\nCarlton\n0\n0\n0\n0\n1\n4\n10\n9\n1\n12\n10\n3\n7\n1\n3\n0\n1\n3\n4\n3\n9\n0\n6\n1\n139\n0\n39.3\n6\n18.2\n1\n0\n5\n197.6\n3\n2\n\n\n2\n2023\n266569840\n1\nMCG\n270146\nEd Curnow\nCarlton\nMid Fwd\nRichmond\n0\n0\n0\n0\n1\n0\n2\n1\n1\n14\n8\n2\n6\n0\n2\n1\n0\n4\n4\n1\n12\n1\n6\n0\n182\n0\n17.4\n0\n3.9\n0\n0\n4\n300.3\n5\n6\n\n\n3\n2023\n266569840\n1\nMCG\n270896\nTrent Cotchin\nRichmond\nMid Fwd\nCarlton\n0\n0\n0\n0\n4\n0\n11\n4\n7\n18\n12\n7\n5\n0\n9\n3\n0\n3\n3\n3\n9\n3\n3\n1\n178\n1\n33.45\n0\n8.2\n1\n1\n3\n263.7\n5\n3\n\n\n4\n2023\n266569840\n1\nMCG\n280819\nDylan Grimes\nRichmond\nKey Def\nCarlton\n0\n0\n0\n0\n0\n1\n4\n4\n0\n12\n11\n4\n7\n0\n4\n1\n0\n0\n0\n6\n8\n2\n5\n0\n89\n1\n12\n0\n6.4\n0\n3\n2\n111\n1\n4\n\n\n5\n2023\n266569840\n1\nMCG\n290627\nDion Prestia\nRichmond\nMid\nCarlton\n0\n0\n0\n1\n5\n0\n13\n5\n8\n23\n17\n10\n7\n0\n13\n1\n0\n2\n2\n5\n10\n4\n4\n0\n158\n1\n25.5\n1\n7.9\n1\n0\n1\n316.3\n4\n4\n\n\n6\n2023\n266569840\n1\nMCG\n290847\nDustin Martin\nRichmond\nGen Fwd\nCarlton\n0\n0\n0\n0\n1\n2\n10\n8\n2\n23\n13\n6\n7\n1\n8\n4\n0\n7\n8\n1\n15\n3\n6\n1\n230\n0\n25.5\n6\n13.7\n0\n0\n0\n432.1\n7\n4\n\n\n\n\n\n\n\n3.1.2 Quick summary\nLets do some quick summaries in a similar fashion to what we did last time. To start of with I am going to have a look at some position summaries by win vs loss across key statistics.\nFirstly to make the analysis a little simpler I am going to make the dataframe into what is called a long format. Now to be fair it is possible to summarize keeping the data in its current wide format but being confident in manipulating data.frames from wide to long and vice versa is very useful for data analysis in general.\nBefore I do that however, lets just check something\n\nunique(data1$Position)\n\n[1] \"Key Fwd\" \"Mid Fwd\" \"Key Def\" \"Mid\"     \"Gen Fwd\" \"Wing\"    \"Gen Def\"\n[8] \"Ruck\"    \"\"       \n\n\nYou may notice a “” is returned this represents a blank value, which ideally there should not be any. Lets have a look a little deeper\n\ndata1|&gt;\n  filter(Position==\"\")|&gt;\n  select(FULLNAME,PERSON_ID,GROUP_ROUND_NO)\n\n        FULLNAME PERSON_ID GROUP_ROUND_NO\n 1: Willie Rioli    296225              2\n 2: Willie Rioli    296225              5\n 3: Willie Rioli    296225              7\n 4: Willie Rioli    296225              8\n 5: Willie Rioli    296225             11\n 6: Willie Rioli    296225             13\n 7: Willie Rioli    296225             19\n 8: Willie Rioli    296225             21\n 9: Willie Rioli    296225             22\n10: Willie Rioli    296225             23\n11: Willie Rioli    296225             24\n12: Willie Rioli    296225             25\n13: Willie Rioli    296225             26\n\n\nOkay this is because he was referred to as “Junior Rioli” that year which means ideally I need to fix the initial join i did from the previous week to just be PERSON_ID and SEASON_ID and not include FULLNAME.\nI know however, that he played as “Gen Fwd” that year so lets quickly update that in our database.\n\ndata1 &lt;- data1|&gt;\n         mutate(Position=ifelse(SEASON_ID==2023&PERSON_ID==296225,\n                                \"Gen Fwd\",Position))\n\nWe can check out work with\n\nunique(data1$Position)\n\n[1] \"Key Fwd\" \"Mid Fwd\" \"Key Def\" \"Mid\"     \"Gen Fwd\" \"Wing\"    \"Gen Def\"\n[8] \"Ruck\"   \n\n\nOkay lets now look to melt the data and drop some columns for the sake of it.\n\ndata.frame(names(data1))\n\n                names.data1.\n1                  SEASON_ID\n2                   MATCH_ID\n3             GROUP_ROUND_NO\n4                 VENUE_NAME\n5                  PERSON_ID\n6                   FULLNAME\n7                 SQUAD_NAME\n8                   Position\n9             OPP_SQUAD_NAME\n10              SQUAD_MARGIN\n11                        WL\n12                       WLD\n13                    BEHIND\n14                 CLEARANCE\n15            CONTESTED_MARK\n16      CONTESTED_POSSESSION\n17 CONTESTED_POSSESSION_POST\n18  CONTESTED_POSSESSION_PRE\n19                  DISPOSAL\n20        EFFECTIVE_DISPOSAL\n21        EFFECTIVE_HANDBALL\n22            EFFECTIVE_KICK\n23                      GOAL\n24                  HANDBALL\n25             HARD_BALL_GET\n26                    HITOUT\n27                 IN50_KICK\n28                 INSIDE_50\n29                 INTERCEPT\n30                      KICK\n31                 LONG_KICK\n32                      MARK\n33              MARK_ON_LEAD\n34         METRES_GAINED_EFF\n35             MISSED_TACKLE\n36             PLY_PRESS_PTS\n37                    POINTS\n38                    RATING\n39                   SMOTHER\n40                     SPOIL\n41                    TACKLE\n42       TOTAL_GAINED_METRES\n43                  TURNOVER\n44          UNCONTESTED_MARK\n\ndtLong = data1|&gt;\n        melt(id.vars = 1:12)|&gt;\n  ## going to remove some columns \n  filter(!variable %in%c(\"POINTS\",\"GOAL\",\"BEHIND\"))\n\nLets now create a quick summary of the stats. Now to reiterate what I mentioned last week, this is where R can be really useful. Effectively, we need to create multiple levels of aggregation to get to the level we want and in excel for example this would require multiple pivot tables.\n\ndtlongSum  &lt;- dtLong|&gt;\n  group_by(GROUP_ROUND_NO,SQUAD_NAME,Position,variable,WL)|&gt;\n  summarise(sum = round(sum(value,na.rm = T),2))|&gt;\n  # the above effectively summarised to the position level\n  ungroup()|&gt;\n  group_by(Position, variable,WL)|&gt;\n  summarise(mu = round(mean(sum),2),\n            sd = round(sd(sum),2))\n\n`summarise()` has grouped output by 'GROUP_ROUND_NO', 'SQUAD_NAME', 'Position',\n'variable'. You can override using the `.groups` argument.\n`summarise()` has grouped output by 'Position', 'variable'. You can override\nusing the `.groups` argument.\n\n # The above then averages the statistics via WL for each position over the season so effectively removes GROUP level data\n\n\n## As I was building it I would use the below as an example of checking as I go  \n#filter(SQUAD_NAME==\"Adelaide Crows\"&variable==\"CLEARANCE\"&Position==\"Mid\")\n\nhead(dtlongSum)|&gt;htmlTable()\n\n\n\n\n\nPosition\nvariable\nWL\nmu\nsd\n\n\n\n\n1\nGen Def\nCLEARANCE\n0\n3.53\n2.47\n\n\n2\nGen Def\nCLEARANCE\n1\n3.95\n2.72\n\n\n3\nGen Def\nCONTESTED_MARK\n0\n1.6\n1.49\n\n\n4\nGen Def\nCONTESTED_MARK\n1\n1.57\n1.26\n\n\n5\nGen Def\nCONTESTED_POSSESSION\n0\n24.42\n7.5\n\n\n6\nGen Def\nCONTESTED_POSSESSION\n1\n24.28\n7.26\n\n\n\n\n\nThe above is still quite a long data.frame with 464 rows which might not be the easiest way to see the differences so lets re shape it again to see if it helps.\n\ndcast.data.table(setDT(dtlongSum),Position+variable~WL,value.var = \"mu\")|&gt;\n  filter(variable==\"DISPOSAL\")|&gt;\n  mutate(difference = round(`1`-`0`,1))|&gt;\n  htmlTable()\n\n\n\n\n\nPosition\nvariable\n0\n1\ndifference\n\n\n\n\n1\nGen Def\nDISPOSAL\n89.28\n89.18\n-0.1\n\n\n2\nGen Fwd\nDISPOSAL\n51.44\n55.79\n4.4\n\n\n3\nKey Def\nDISPOSAL\n30.98\n34.39\n3.4\n\n\n4\nKey Fwd\nDISPOSAL\n23.05\n26.11\n3.1\n\n\n5\nMid\nDISPOSAL\n83.1\n87.57\n4.5\n\n\n6\nMid Fwd\nDISPOSAL\n28.22\n30.66\n2.4\n\n\n7\nRuck\nDISPOSAL\n17.06\n17.94\n0.9\n\n\n8\nWing\nDISPOSAL\n38.56\n38.76\n0.2\n\n\n\n\n\nMaybe we want to plot the differences\n\ndtLong|&gt;\n  group_by(MATCH_ID,SQUAD_NAME,WL,Position,variable)|&gt;\n  summarise(sum = sum(value))|&gt;\n  filter(variable%in%c(\"DISPOSAL\",\"CLEARANCE\"))|&gt;\n  #filter(Position%in%c(\"Mid\",\"Mid Fwd\",\"Wing\"))|&gt;\n  ggplot(aes(as.factor(WL),sum))+\n  geom_jitter(col=\"gray80\")+\n  geom_boxplot()+\n  stat_summary(fun.data = \"mean_sdl\",\n               geom = \"pointrange\",\n               fun.args = list(mult=1),col=\"black\")+\n  facet_grid(vars(variable), vars(Position),scales = \"free_y\")+\n  theme_bw()\n\n`summarise()` has grouped output by 'MATCH_ID', 'SQUAD_NAME', 'WL', 'Position'.\nYou can override using the `.groups` argument.\n\n\n\n\n\n\n\n3.1.3 Important note\nYou guys are the experts here in being able to double check what numbers to expect. I want to strongly reiterate how much of coding is just checking your work as you go, are you getting numbers you expect? As I mentioned the beauty of R being modular is it easily allows for you to continuously check as you build.\n\n\n3.1.4 Final exploration\nLets have a look at the global relationships across a range of variables.\n\nsum2 = dtLong|&gt;\n  group_by(GROUP_ROUND_NO,SQUAD_NAME,variable)|&gt;\n  summarise(sum = round(sum(value),2))|&gt;\n  pivot_wider(values_from = sum,\n              names_from = variable)|&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'GROUP_ROUND_NO', 'SQUAD_NAME'. You can\noverride using the `.groups` argument.\n\nsum2|&gt;head()|&gt;htmlTable()\n\n\n\n\n\nGROUP_ROUND_NO\nSQUAD_NAME\nCLEARANCE\nCONTESTED_MARK\nCONTESTED_POSSESSION\nCONTESTED_POSSESSION_POST\nCONTESTED_POSSESSION_PRE\nDISPOSAL\nEFFECTIVE_DISPOSAL\nEFFECTIVE_HANDBALL\nEFFECTIVE_KICK\nHANDBALL\nHARD_BALL_GET\nHITOUT\nIN50_KICK\nINSIDE_50\nINTERCEPT\nKICK\nLONG_KICK\nMARK\nMARK_ON_LEAD\nMETRES_GAINED_EFF\nMISSED_TACKLE\nPLY_PRESS_PTS\nRATING\nSMOTHER\nSPOIL\nTACKLE\nTOTAL_GAINED_METRES\nTURNOVER\nUNCONTESTED_MARK\n\n\n\n\n1\n1\nAdelaide Crows\n33\n9\n111\n77\n34\n318\n242\n89\n153\n101\n26\n40\n39\n52\n62\n217\n55\n102\n6\n3992\n4\n613.35\n178.6\n6\n30\n40\n6077.2\n56\n93\n\n\n2\n1\nBrisbane Lions\n38\n4\n116\n55\n61\n264\n181\n72\n109\n102\n33\n41\n35\n40\n52\n162\n62\n52\n6\n3879\n10\n650.4\n161.2\n7\n37\n46\n5309.8\n66\n48\n\n\n3\n1\nCarlton\n32\n18\n148\n100\n48\n341\n241\n92\n149\n120\n32\n26\n33\n45\n79\n221\n67\n97\n0\n4119\n10\n580.65\n186.2\n8\n29\n55\n5963.4\n75\n79\n\n\n4\n1\nCollingwood\n43\n10\n135\n79\n56\n372\n286\n120\n166\n143\n18\n45\n52\n62\n62\n229\n62\n105\n7\n4611\n8\n594.45\n250.5\n8\n32\n52\n6293\n61\n95\n\n\n5\n1\nEssendon\n32\n11\n128\n83\n45\n414\n322\n142\n180\n159\n28\n20\n54\n66\n63\n255\n45\n131\n6\n4607\n6\n521.25\n264.1\n4\n30\n37\n6993.8\n59\n120\n\n\n6\n1\nFremantle\n28\n14\n146\n102\n44\n437\n334\n145\n189\n179\n30\n45\n56\n65\n92\n258\n63\n136\n6\n4615\n4\n633.3\n200.2\n11\n24\n53\n6581.1\n82\n122\n\n\n\n\n\n\ncols &lt;- c(\"METRES_GAINED_EFF\", \"PLY_PRESS_PTS\",\"KICK\",\"HANDBALL\",\"MARK\",\"CONTESTED_POSSESSION\",\"TACKLE\")\n\nforPairs &lt;- sum2|&gt;\n           select(!1:2)|&gt;\n           select(cols)\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(cols)\n\n  # Now:\n  data %&gt;% select(all_of(cols))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\nggpairs(forPairs)+theme_light()"
  },
  {
    "objectID": "module2.html#exploratory-and-descriptive-analysis",
    "href": "module2.html#exploratory-and-descriptive-analysis",
    "title": "3  Module 2",
    "section": "3.2 Exploratory and Descriptive analysis",
    "text": "3.2 Exploratory and Descriptive analysis\nThe above is an example of both what we would consider EXPLORATORY and DESCRIPTIVE analysis. These analysis techniques are usually first steps we take when analysing data.\nExploratory analysis can be really useful for thing such as\n\nIdentifying outliers or anomalies in the data (very important)\nObserve if the problem is tractable/solvable (very important)\nAssess distributional assumptions of the data (more on this next week)\nVisual assessment of relationships or patterns that may exist within the data\n\nWhere as descriptive analysis refers to summarizing the main variables of interest. You are not necessarily looking to infer a relationship or hypothesis per se but just are reporting relevant summary statistics e.g. mean, max, min, range, standard deviation, median, sum, proportion,count relating to a question at hand.\n” The average disposal count for mid fielders was ….. for 2023 and …. for 2024.”\n” The proportion of time player x played up forward was …. vs ….. played down back”"
  },
  {
    "objectID": "module2.html#a-quick-disclaimer-regarding-exploratory-analysis",
    "href": "module2.html#a-quick-disclaimer-regarding-exploratory-analysis",
    "title": "3  Module 2",
    "section": "3.3 A quick disclaimer regarding exploratory analysis",
    "text": "3.3 A quick disclaimer regarding exploratory analysis\nWhile exploratory analysis is a powerful tool that is used almost subconsciously in our daily lives, it is important to exercise caution and maintain mental error control. The human brain is notoriously good at finding and justifying relationships in data that may genuinely just be noise. To highlight this problem, an additional chapter called “Multiple testing” was added to the second version of the ISLR book i recommend the previous module.\nHypothesis testing is one type of tool that aims to help reduce the risk of being misled by noise. Ultimately, however, you bear a significant responsibility to develop a hypothesis or causal framework that justifies why you think the observed trend may indeed be valid.\nWhilst, developing causal models can quickly become an extensive process a simply starting point is developing a Directed Acyclic Graph (DAG). A DAG is a graphical representation of a hypothetical model outlining causal effects. Outlined below is a quick example of how one may develop a dag.\nWe want to model the relationships between the following variables:\n\nParental Education (PE)\nStudent’s Motivation (SM)\nTime Spent Studying (TS)\nAttendance (A)\nAcademic Performance (AP)\n\nBelow we are going graphical represent relationships that we believe to impact these variables.\n\n\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n\n\n\n\n\n\n3.3.1 DAG Structure:\nFor the above figure we would interpret it like so:\n\nParental Education (PE) affects both Student’s Motivation (SM) and Time Spent Studying (TS).\nStudent’s Motivation (SM) influences Time Spent Studying (TS).\nTime Spent Studying (TS) and Attendance (A) both affect Academic Performance (AP).\nParental Education (PE) also directly affects Academic Performance (AP).\n\nWhile this may initially seem trivial, a well-thought-out DAG can help identify situations where causal analysis approaches can be explored using observational data—an area long thought to be exclusive to randomized controlled trials. See this paper by Dr Judd Kalkhoven an academic from WSU who gives some examples of causal modelling from an athlete injury perspective https://link.springer.com/article/10.1007/s40279-024-02008-1."
  },
  {
    "objectID": "module2.html#supervised-vs-unsupervised-problems",
    "href": "module2.html#supervised-vs-unsupervised-problems",
    "title": "3  Module 2",
    "section": "3.4 Supervised vs Unsupervised problems",
    "text": "3.4 Supervised vs Unsupervised problems\n\n3.4.1 Supervised Learning Overview\nIn our previous session, we explored our ability to predict score differential using a range of statistics. The simple linear model we examined was an example of a supervised regression problem. The easiest way to identify whether something can be thought of as a supervised problem is by determining whether you have a clear outcome of interest that you want to predict. If the answer is yes, then it is a supervised problem.\n\n\n3.4.2 Types of Supervised Problems\nIn reality, most of the problems/questions you encounter will relate to predicting some quantity, whether it be an absolute number such as score differential (a regression problem), win vs. loss probability (a classification problem), or predicting the next action type, e.g., kick, handball, tackle (a multinomial classification problem).\n\nPrediction of a number = Regression problem\nPrediction of a class/category = Classification problem\n\n\n\n3.4.3 Choosing a Model\nThere are often numerous models to answer the same regression or classification problem, and there will usually be trade-offs between certain models that you need to explore. A useful first step is asking yourself, “Is it important that I understand how the model made the prediction it did, or is it just important that the model can predict accurately?” This question can help you narrow down the scope of models you may use.\n\nThe above picture is taken from ISLR book I recommended (https://www.statlearning.com/) and it highlights some of these tradeoffs. Over the coming weeks we will explore models that exist along this continuum but I want you to also think of the x axis as dataset size required."
  },
  {
    "objectID": "module2.html#unsupervised-problems",
    "href": "module2.html#unsupervised-problems",
    "title": "3  Module 2",
    "section": "3.5 Unsupervised Problems",
    "text": "3.5 Unsupervised Problems\nLet’s say, for example, that you don’t have a specific dependent/outcome variable that you are interested in predicting. Instead, you are more interested in exploring whether there are any interesting patterns, clusters, or groupings that exist within the data. You are not too concerned with how they are structured, just whether there is a structure. This would be considered an unsupervised learning problem.\n\n3.5.1 Common Algorithms for Unsupervised Learning\n\nClustering Algorithms:\n\nAim to find multidimensional similarities between variables, grouping data points into clusters based on their features.\nExamples: K-means clustering, hierarchical clustering, DBSCAN, GMM.\n\nPrincipal Component Analysis (PCA):\n\nAims to find components that can best explain the variance between variables, reducing the dimensionality of the data while retaining most of the variance.\nUseful for data visualization and simplifying complex datasets.\n\nAssociation Rules / Market Basket Analysis:\n\nAim to find associations between items or transactions, identifying how frequently certain items or events occur together.\nExample: Market basket analysis can reveal that customers who buy bread often buy butter as well.\n\n\n\n\n3.5.2 Summary\nUnsupervised learning is about discovering the inherent structure within your data without predefined labels or outcomes. It’s particularly useful when you want to explore and understand the underlying patterns or relationships in your dataset.\nBelow is a quick example where we are using a variant of the kmeans clustering algorithm.\n\n## select some columns to analyse. I do suggest to do select only variables you are interested in. e.g. don;t just put everything in and hope for the best.\n\ncols &lt;- c(\"METRES_GAINED_EFF\", \"PLY_PRESS_PTS\",\"KICK\",\"HANDBALL\",\"MARK\",\"CONTESTED_POSSESSION\",\"count\")\n\n### create a quick summary of all players from 2023\n\nsmalldf &lt;- data1|&gt;\n  group_by(FULLNAME,Position)|&gt;\n  mutate(count = length(FULLNAME))|&gt;\n  group_by(FULLNAME,Position)|&gt;\n  ## the below is a way where you can summarise in the wide format\n  summarise(across(all_of(cols), mean))|&gt;\n  ungroup()|&gt;\n  ## going to remove players with less than 5 games\n  filter(!count&lt;=5)|&gt;\n  # dont need the count column after so going to remove\n  select(!count)\n\n`summarise()` has grouped output by 'FULLNAME'. You can override using the\n`.groups` argument.\n\n## outside the scope of but we need to remove players name from being a column variable to being a row now. \nsmallerdf &lt;- smalldf|&gt;\n             column_to_rownames(\"FULLNAME\")|&gt;\n             select(!Position)\n\n## We are going to compute a slightly more complicated kmeans algo \n# the reasoning is a little complicated but it just better matches our data. \n\n### compute gowers distance. \n# this calcualtes how far away each player is from each other across all statistics and represents as one number. \ndissimilarity_matrix &lt;- daisy(smallerdf,\n                               metric = \"gower\")\n\n## convert to a matrix\ngower_mat &lt;- as.matrix(dissimilarity_matrix)\n\n\n### find number of clusters within group. I know ahead of time that 3 is an okay number but usually you may need to explore a number of cluster sizes. \n\n\n## below is the clustering algo we will use\nset.seed(123) # For reproducibility\npam_result &lt;- pam(gower_mat, k = 3)\n\n## join computed cluster back on the inital dataframe\nsmalldf &lt;- smalldf|&gt;mutate(cluster = pam_result$clustering)\n\n## lets plot the results \nfviz_cluster(pam_result,geom = \"text\",\n               ellipse.type = \"convex\",\n               show.clust.cent = TRUE,\n               ggtheme = theme_classic(),\n               labelsize = 8)\n\n\n\n\nLets get a feel for what differentiates the clusters.\n\nclusterLong =setDT(smalldf)|&gt;melt.data.table(id.vars = c(\"FULLNAME\",\"Position\",\"cluster\"))\n\n\nggplot(clusterLong,aes(value,as.factor(cluster)))+\n  geom_jitter(aes(col=Position))+\n  stat_summary(fun.data = \"mean_sdl\",\n               geom = \"crossbar\",\n               fun.args = list(mult=1),col=\"black\")+\n  facet_wrap(~variable,scales = \"free\",ncol = 3)+\n  theme_light()+\n  theme(legend.position = \"top\")\n\n\n\n\nFinally, a question you may have from this which players are most similar to another player in a multidimensional case.\nFirstly, I need to create a helper function to do this. I know this may seem scary but I was able to do this within a couple of minutes of searching on google and using chatGPT.\n\n# You don;t need to necessarily understand what this function does but it allows us to find the most similar players based off the multidimensioal calculation we did before.\n\nfind_most_similar &lt;- function(df, gower_matrix, individual, n = 1) {\n  # Find the index of the selected individual\n  individual_index &lt;- which(df$FULLNAME == individual)\n  \n  # Get the distances for the selected individual\n  distances &lt;- gower_matrix[individual_index, ]\n  \n  # Set the distance to itself as Inf to exclude it from the nearest neighbors\n  distances[individual_index] &lt;- Inf\n  \n  # Find the indices of the n smallest distances\n  nearest_indices &lt;- order(distances)[1:n]\n  \n  # Include the selected individual in the result\n  all_indices &lt;- c(individual_index, nearest_indices)\n  \n  # Return the rows of the selected individual and nearest individuals\n  return(df[all_indices, ])\n}\n\nRun the helper function\n\nfind_most_similar(smalldf, gower_mat, 'Toby Greene', n = 3)|&gt;\n  mutate(across(where(is.numeric), round, 1))|&gt;htmlTable()\n\n\n\n\n\nFULLNAME\nPosition\nMETRES_GAINED_EFF\nPLY_PRESS_PTS\nKICK\nHANDBALL\nMARK\nCONTESTED_POSSESSION\ncluster\n\n\n\n\n1\nToby Greene\nGen Fwd\n217.6\n30.4\n12\n5.7\n4.4\n7.7\n2\n\n\n2\nChristian Salem\nGen Def\n212.2\n31.6\n12.5\n6.6\n3.9\n5.8\n2\n\n\n3\nSteele Sidebottom\nWing\n227.2\n32.8\n12.8\n8\n4.2\n6.9\n2\n\n\n4\nBen Ainsworth\nGen Fwd\n203\n26.9\n10.9\n6.8\n4.8\n6.5\n2\n\n\n\n\n\nAlways good to cross reference if you think these results make sense.\nWe will talk about some other use cases of unsupervised problems in later modules."
  },
  {
    "objectID": "module2.html#practice-exercises-module-2",
    "href": "module2.html#practice-exercises-module-2",
    "title": "3  Module 2",
    "section": "3.6 Practice exercises module 2",
    "text": "3.6 Practice exercises module 2\n\nThink about questions you may have from datasets you have used. What would you classify your question as? e.g. ( supervised, unsupervised, regression or classification)\nCould you defend or represent your problem as a DAG?\nCan you think of any questions that might be appropriate for an unsupervised learning problem?"
  },
  {
    "objectID": "module2.html#resources.",
    "href": "module2.html#resources.",
    "title": "3  Module 2",
    "section": "3.7 Resources.",
    "text": "3.7 Resources.\nISLR https://www.statlearning.com/"
  },
  {
    "objectID": "module3.html#recap-from-last-week",
    "href": "module3.html#recap-from-last-week",
    "title": "4  Module 3",
    "section": "4.1 Recap from last week",
    "text": "4.1 Recap from last week\n\nWhat is the difference between supervised and unsupervised learning?\nIf I was to try predict a category or a class, what type of prediction problem would this be considered?\nWhat are the benefits of developing a mini DAG or casual structure?"
  },
  {
    "objectID": "module3.html#goals-for-this-week.",
    "href": "module3.html#goals-for-this-week.",
    "title": "4  Module 3",
    "section": "4.2 Goals for this week.",
    "text": "4.2 Goals for this week.\n\nIntroduction into R programming language and getting started with some descriptive statistics.\nUnderstanding key terms; Exploratory analysis, Supervised vs Unsupervised problems, regression vs classification, prediction vs association.\nA quick note on distributions - Supervised regression and classification problems using a generalized linear model framework. (Hypothesis testing)\nMachine learning approach to Supervised learning\n\nThe below is an exert from a book that I have stolen that I think does a great job a summarizing the goal of statistical modelling.\n“Almost every statistical analysis begins with some kind of statistical model. A statistical model generally takes the form of a probability distribution that attempts to quantify the uncertainty that comes with observing a new response. The model is intended to represent the unknown phenomenon that governs the observation process. At the same time, the model needs to be convenient to work with mathematically, so that inference procedures such as confidence intervals and hypothesis tests can be developed. Selecting a model is typically a compromise between two competing goals: providing a more detailed approximation to the process that generates the data and providing inference procedures that are easy to use.”\npg1, Analysis of Categorical data with R Christopher R. Bilder and Thomas M Loughin. 2015.\nThis book also seems to be freely available from a google search.\nhttp://ndl.ethernet.edu.et/bitstream/123456789/28010/1/Christopher%20R.%20Bilder_2015.pdf"
  },
  {
    "objectID": "module3.html#load-data-and-packages",
    "href": "module3.html#load-data-and-packages",
    "title": "4  Module 3",
    "section": "4.3 Load data and packages",
    "text": "4.3 Load data and packages\n\npacman::p_load(data.table,tidyverse,htmlTable,ggforce,ggpubr,ggeffects,lme4,emmeans,mgcv,performance,nnet)\n\n\nurlfile=\"https://raw.githubusercontent.com/R2mu/GWS_DSPR/main/data/mod2data.csv\"\n\ndata1 &lt;- fread(urlfile)\n\nurlfile2=\"https://raw.githubusercontent.com/R2mu/GWS_DSPR/main/data/shotdata.csv\"\n\ndata2 &lt;- fread(urlfile2)\ndata2 &lt;- data2|&gt;\n  rename(Rotated.xStd=shots.details.locationRotated.xStd,\n         Rotated.yStd=shots.details.locationRotated.yStd)"
  },
  {
    "objectID": "module3.html#basic-probability-and-distributions",
    "href": "module3.html#basic-probability-and-distributions",
    "title": "4  Module 3",
    "section": "4.4 Basic probability and distributions",
    "text": "4.4 Basic probability and distributions\nWe are going to explore the idea of modelling a probability distribution today. Now there are many different varieties of probability distributions that exist, but we will just explore a couple of them that I from my experience will cover the largest chunk of questions you may be interested in asking in sport.\n1) The normal distribution, often referred to as the bell curve, is appropriate for many situations involving continuous data. It is characterized by its symmetric shape, where most of the observations cluster around the mean, and the probabilities for values taper off equally on both sides. This distribution is particularly suitable for variables such as score differential, meters gained, and other player statistics with high means.\n\n\n\n\n\n2) Binomial Distribution: The binomial distribution is ideal for modeling the number of successes in a fixed number of independent trials, where each trial has two possible outcomes (success or failure) and a constant probability of success. It is particularly useful for scenarios where you are counting the number of times an event occurs, such as the number of goals scored by a player in a series of attempts, or the number of games won in a season. The binomial distribution is defined by two parameters: the number of trials (n) and the probability of success (p). It helps in situations where the outcomes are discrete and the trials are independent, allowing for the calculation of probabilities for different numbers of successes.\nLets say for example you wanted the model probability of Toby Greene kicking 4 goals from 5 attempts in a game.\n\nx = 4 # represents number of GOALS \nn = 5 # From 5 attempts \np = 0.57 # with a baseline probability per attempt of 0.57\nround(dbinom(x,n,p),2)\n\n[1] 0.23\n\n\nThis means that, under these conditions, there is a 23% chance that Toby will achieve 4 successful goals in 5 attempts.\nIf we wanted to plot all the outcomes e.g 1 from 5, 2 from 5 etc etc it would look like the below\n\nbinom_prob = dbinom(0:5,5,0.57)\n\nplot(0:5, binom_prob, type = \"h\", lwd = 2, col = \"blue\",\n     xlab = \"Number of Goals\", ylab = \"Probability\",\n     main = \"Binomial Distribution of number of goals from 5 attempts\")\npoints(0:5, binom_prob, pch = 16, col = \"blue\")\n\n\n\n\n3) The Poisson distribution: is appropriate for modelling the number of events occurring within a fixed interval of time or space, particularly when these events are rare or infrequent. It is often used for count data where the mean number of occurrences is low, such as the number of goals scored in a single match by a player, the number of customer arrivals in an hour, or the number of system failures in a day. The Poisson distribution is defined by a single parameter (λ), which represents both the mean and the variance of the distribution. In my experience this assumption regarding the mean and variance is often violated so other parameters can be introduced to the assumption to deal with it.\n\ng1= ggplot(data1,aes(TACKLE))+\n   geom_histogram(col=\"white\")\n\ng2 =ggplot(data1,aes(CONTESTED_POSSESSION))+\n   geom_histogram(col=\"white\",binwidth = 1)+\n  scale_x_continuous(limits = c(0,20))\n\nggarrange(g1,g2)"
  },
  {
    "objectID": "module3.html#generalized-linear-models",
    "href": "module3.html#generalized-linear-models",
    "title": "4  Module 3",
    "section": "4.5 Generalized Linear models",
    "text": "4.5 Generalized Linear models\n\n4.5.1 Gaussian example\nNow the we’ve had a look at some distributions we are going to have a look at some methods for modelling different distributions. In week 1 we modeled how well a simple linear regression model could predict score differential. Mathematically, a linear regression model can be thought of as saying a prediction of Score differential is Normally distributed with some mean \\(\\mu\\) and standard deviation \\(\\sigma\\) .\n\\(\\text{Score differential} \\sim \\text{Normal}(\\mu_i,\\sigma)\\)\nWhere the mean can be described by a linear equation\n\\(\\mu_i = \\alpha+\\beta_1 X_1+\\beta2 X_2\\).\nNow this might look a little scary but this effectively just y = mx+b that you have learnt in high school, except in this case \\(X_1\\) might represent inside 50s and \\(X_2\\) might represent disposals. Since we have two variables this is a multiple regression model.\nThe figure below hopefully helps make this idea that a prediction can be thought of as a sample from the normal distribution with a conditional effect based on the predictor variable\n\n\n\n4.5.2 Binomial example.\nLets go back to our binomial probability example we were talking about before. We might say something like the probability of scoring a goal is binomially distributed with some probability \\(p_i\\).\n\\(\\text{Goal success} \\sim \\text{Binomial}(1,p_i)\\)\nThe issue we have here is that \\(p_i\\) which represents the probability success, is obviously going to be impacted by things such as distance from goal, angle from goal and whether it was during play or a set shot. So what we are interested in doing is estimated how this probability \\(p_i\\) will change as a function of those variables, which we do in the same manner we estimated the mean in the linear regression model above.\n\\(logit(p_i) = \\alpha +\\beta_1 X_1+\\beta2 X_2\\)\nYou can think of \\(B_1\\) as maybe distance from goal and \\(B_2\\) as angle from goal possibly. The other thing you may notice is that the \\(p_1\\) has been wrapped in a logit function. This is one example of the super power of generalized linear models. Effectively, we are able to keep the same notation as we would for a simple linear model, but we transform this linear combination to make sure that the result stays between 0 and 1 or Miss and GOAL in our example. This logit (logistic) function is why you will often here modelling 0 and 1s as logistic regression.\n\n\n4.5.3 Poisson Example\nIf we were modeling a count or something that fits a Poisson distribution, the transformation would be:\n\\(log(\\lambda) = \\alpha +\\beta_1 X_1+\\beta2 X_2\\)\nThis time \\(log(\\lambda)\\) would transform the linear predictors to be constrained to match the poisson distribution. It should be noted that Poisson distributions can be a little more tricky to deal with, as there are some other parameters to consider tuning which relate to dispersion etc. These are more advanced topics and outside the scope of this practical."
  },
  {
    "objectID": "module3.html#binomial-example-xg-model",
    "href": "module3.html#binomial-example-xg-model",
    "title": "4  Module 3",
    "section": "4.6 Binomial Example / xG model",
    "text": "4.6 Binomial Example / xG model\nOkay so now we have got past the theory, lets delve into some examples. Our first example is going to be developing our own expected Goals/Points models which can be just simply thought of as a binomial/logistic regression model. We will use information related to shot location and shots detail (General play, Set shot etc) to predict the probability of a goal being scored.\nFirst things first, lets do some quick data cleaning.\n\nshot_small &lt;- data2%&gt;%\n  filter(Rotated.xStd&gt;(-20))|&gt;\n  mutate(Shot_outcome = as.factor(ifelse(shots.result.code==\"G\",1,0)),\n         Champ_Xp_err = round(shots.result.points-shots.result.pointsExpected,3))\n\n\nggplot()+\n  geom_ellipse(aes(x0 = 0, y0 = 0, a = 80, b = 70, angle = 0))+\n  geom_hex(shot_small, \n           mapping= aes(x=Rotated.xStd,y=Rotated.yStd),\n           bins=30)+\n  xlim(c(-80,80))+\n  theme_minimal()\n\n\n\n\n\nggplot(shot_small,\n       aes(x=Rotated.xStd,\n           y=Rotated.yStd,col=shots.result.code))+\n  geom_point(alpha=.3)+\n  geom_density2d(col=\"gray90\")+\n  theme_minimal()\n\n\n\n\nLets have a quick look at how well the champion data expected points model does.\n\nMAE = mean(abs(shot_small$Champ_Xp_err))\nggplot(shot_small,aes(abs(Champ_Xp_err)))+\n  #geom_histogram(col=\"white\")+\n  ggtitle(paste(\"Campion xG error =\", round(MAE,2)))+\n  geom_density(fill=\"gray80\")+\n  theme_minimal()\n\n\n\n\nBefore we look to build our first model. Lets do some simple exploratory statistics of some of the parameters we will likely end up including in our model.\n\nshot_small|&gt;ggplot(aes(y=80-Rotated.xStd,x=Shot_outcome,\n                       col = shots.details.type))+\n  geom_jitter(height = .1,alpha=.1)+\n  geom_boxplot(width=.4)+\n  coord_flip()+\n  facet_wrap(~shots.details.type,ncol = 1)+\n  theme_light()+\n  theme(legend.position = \"bottom\")\n\n\n\n\nNot the best looking plot, but in general we can a small impact of the closer we are to the goals in x_direction the more likely we are to score a goal. Lets try model this along with the y component.\n\nm1 &lt;- glm(Shot_outcome~Rotated.xStd+Rotated.yStd,\n          data = shot_small,family = binomial)\ns =summary(m1)\n\nround(s$coefficients,3)\n\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    -1.197      0.081 -14.701        0\nRotated.xStd    0.022      0.002  14.196        0\nRotated.yStd    0.004      0.001   4.949        0\n\n\nNow lets spend a little bit of time thinking about what the above means. We will focus our time on the Estimate column. Now a logistic regression model returns coefficients (Estimate) on the log odds scale. So currently, if we focus on Rotated.xStd with its estimate of 0.022, this would mean as we get 1 metre closer to the goal holding all other variables constant our log odds of success increases by 0.022. If you still have no idea what that means, that’s okay—you are in the right place. It is often easier to understand if we convert log-odds to what are called odds ratios. We will do this below\n\ndata.frame(round(exp(coef(m1)),3))\n\n             round.exp.coef.m1....3.\n(Intercept)                    0.302\nRotated.xStd                   1.022\nRotated.yStd                   1.004\n\n\nNow we can say the odds of scoring a goal increase by 2.2% for every meter closer to the goal we get, holding all other variables equal. Keen-eyed readers might notice that the odds ratio and log-odds in this case return effectively identical numbers. Although it’s outside the scope of this module, for small values of the coefficients, both the log-odds and odds ratios will be similar.\nHowever, both log-odds and odds ratios still have limitations in that they only express the relative difference between two scenarios. They do not provide information about how this impacts the absolute risk. This is a much bigger problem in datasets with imbalanced outcomes, such as low rates of injuries versus not injured. For instance, we might say the odds of injury increase by 50% under some condition, but this might only represent an absolute risk increase from 3% to 4.5%.\nThe easiest way around all this complexity is just to plot or return the values on the probability scale. Lets do this below.\n\nxstd &lt;- predict_response(m1, \"Rotated.xStd\")\nystd &lt;- predict_response(m1, \"Rotated.yStd\")\np1   &lt;- plot(xstd,show_data = T,jitter = 0.1)\np2   &lt;- plot(ystd,show_data = T,jitter = 0.1)\n\nggarrange(p1,p2,common.legend = T)\n\n\n\n\nWe can see the effects above but if you wanted to be more explicit about contrasting the impacts, you could do something like the below\n\nem1 =emmeans(m1,~Rotated.xStd,\n          at = list(Rotated.xStd = c(25,50)),\n          type = \"response\")\n\nprint(em1)\n\n Rotated.xStd  prob      SE  df asymp.LCL asymp.UCL\n           25 0.343 0.01012 Inf     0.323     0.363\n           50 0.474 0.00486 Inf     0.464     0.483\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n\nem1 |&gt; \n regrid()|&gt;\n  pairs()|&gt;\n  data.frame()|&gt;\n  select(contrast,estimate)\n\n                         contrast   estimate\n1 Rotated.xStd25 - Rotated.xStd50 -0.1312767"
  },
  {
    "objectID": "module3.html#assess-our-first-xg-model",
    "href": "module3.html#assess-our-first-xg-model",
    "title": "4  Module 3",
    "section": "4.7 Assess our first xG model",
    "text": "4.7 Assess our first xG model\nTo do this we are going to predict our model response to the data.frame and multiple that number by 6.\n\nshot_small &lt;- shot_small|&gt;\n  mutate(xG_m1 = predict(m1,type=\"response\")*6,\n         m1_Xp_err = round(shots.result.points-xG_m1,3))\n\n\nMAE_champ  = mean(abs(shot_small$Champ_Xp_err))\nMAE_m1     = mean(abs(shot_small$m1_Xp_err))\nggplot(shot_small,aes(abs(Champ_Xp_err)))+\n  #geom_histogram(col=\"white\")+\n  ggtitle(paste(\"MAE_champ =\",round(MAE_champ,2),\"MAE_m1=\",round(MAE_m1,2)))+\n  geom_density(fill=\"gray80\",alpha=.3)+\n  geom_density(fill=\"blue\",alpha=.3,\n               shot_small, mapping=aes(abs(m1_Xp_err)))+\n  theme_minimal()\n\n\n\n\nOkay so we are slightly better than just flipping a coin on each shot at goal but champion is still a better model at the moment. Lets try one more model and see how we go.\n\nshot_small$shots.player.displayName&lt;-as.factor(shot_small$shots.player.displayName)\nshot_small$shots.details.type&lt;-as.factor(shot_small$shots.details.type)\n\nm2 &lt;- bam(Shot_outcome~te(Rotated.xStd,Rotated.yStd,by=shots.details.type)+\n                    s(shots.player.displayName,bs=\"re\"),\n           family = binomial,\n           nthreads=8,\n           discrete = T,\n           data = shot_small ,method = \"fREML\")\n\n\nlibrary(earth)\n\nWarning: package 'earth' was built under R version 4.3.3\n\n\nLoading required package: Formula\n\n\nLoading required package: plotmo\n\n\nWarning: package 'plotmo' was built under R version 4.3.3\n\n\nLoading required package: plotrix\n\n\nWarning: package 'plotrix' was built under R version 4.3.2\n\nm3 = earth::earth(Shot_outcome~Rotated.xStd+Rotated.yStd+shots.details.type,\n             degree=2, glm=list(family=binomial),data = shot_small)\n\nplot(m3, which = 1)\n\n\n\npdp::partial(m3, pred.var = \"Rotated.xStd\",prob =TRUE,\n             grid.resolution = 10,) %&gt;% autoplot()\n\n\n\n\nWe won’t bother looking at the coefs for now lets just see if we are getting better performance.\n\nshot_small &lt;- shot_small|&gt;\n  mutate(xG_m2 = predict(m2,type=\"response\")*6,\n         xG_m3 = predict(m3,type=\"response\")*6,\n         m2_Xp_err = round(shots.result.points-xG_m2,3),\n         m3_Xp_err = round(shots.result.points-xG_m3,3))\n\nMAE_m2     = mean(abs(shot_small$m2_Xp_err))\nMAE_m3     = mean(abs(shot_small$m3_Xp_err))\n\nggplot(shot_small,aes(abs(Champ_Xp_err)))+\n  #geom_histogram(col=\"white\")+\n  ggtitle(paste(\"MAE_champ =\",round(MAE_champ,2),\"MAE_m1=\",round(MAE_m1,2),\n                \"MAE_m2=\",round(MAE_m2,2),\n                \"MAE_m3=\",round(MAE_m3,2)))+\n  geom_density(fill=\"gray80\",alpha=.3)+\n  geom_density(fill=\"blue\",alpha=.3,\n               shot_small, mapping=aes(abs(m1_Xp_err)))+\n    geom_density(fill=\"red\",alpha=.3,\n               shot_small, mapping=aes(abs(m2_Xp_err)))+\n      geom_density(fill=\"yellow\",alpha=.3,\n               shot_small, mapping=aes(abs(m3_Xp_err)))+\n  theme_minimal()\n\n\n\n\n\nsum(abs(shot_small$Champ_Xp_err))-\nsum(abs(shot_small$m2_Xp_err))\n\n[1] 889.98\n\n\nSo over the 2013 season our GAM model better identified up 889 points when compared to the champion code.\nLets see if we can better visualize what our final model thinks\n\npar(mfrow=c(1,2),cex=.9)\n\nvis.gam(m2, view = c(\"Rotated.xStd\", \"Rotated.yStd\"),\n        cond=list(shots.details.type=\"Set Shot Regular\"),\n        plot.type = \"contour\", \n        color = \"heat\",too.far = 0.1,\n        lwd=2,labcex = 0.8,\n        contour.col  = \"black\",nCol = 20,\n        type = \"response\",main =\"Set Shot Regular\")\n\nvis.gam(m2, view = c(\"Rotated.xStd\", \"Rotated.yStd\"),\n        cond=list(shots.details.type=\"Set Shot Snap\"),\n        plot.type = \"contour\", \n        color = \"heat\",too.far = 0.1,\n        lwd=2,labcex = 0.8,\n        contour.col  = \"black\",nCol = 20,\n        type = \"response\",main =\"Set Shot Snap\")\n\n\n\npar(mfrow=c(1,1))"
  },
  {
    "objectID": "module3.html#circling-back",
    "href": "module3.html#circling-back",
    "title": "4  Module 3",
    "section": "4.8 Circling back",
    "text": "4.8 Circling back\nAt the beginning, you may remember my reference to developing a model that not only describes the data-generating process well but also has good properties that can be tested via hypothesis testing. I have deliberately focused more on using visuals to identify the magnitude of the effects we have observed rather than delving deeply into hypothesis testing. While hypothesis testing is a valuable resource, using it successfully requires at least a basic understanding of which assumptions are valid for a given model and which are not.\nFor example, our initial model did not perform well because it was not specified correctly; we failed to account for the fact that there were many repeat observations among players. This violates the independence assumption of GLMs and affects the calculation of standard errors and p-values.\nIn practice, you would typically first identify the probability family appropriate for your response variable, such as binomial, Poisson, or Gaussian. Next, you would select models capable of handling those probability distributions and then check the assumptions underlying those models. Some assumptions can be identified as broken just by understanding your dataset, while others can only be tested after you have built the model.\n\n4.8.1 Key Assumptions to Consider\n\nIndependence: For many models, including GLMs, observations are assumed to be independent. Violations of this assumption, such as repeated measures on the same subjects, need to be accounted for.\nNormality of Residuals: For linear models, residuals are often assumed to be normally distributed. This can be checked using diagnostic plots.\nHomoscedasticity: Constant variance of residuals is another common assumption. Heteroscedasticity can affect the efficiency of estimates and the validity of hypothesis tests.\n\n\n\n4.8.2 Practical Steps\n\nIdentify the Probability Family:\n\nDetermine whether your response variable follows a binomial, Poisson, Gaussian, or another distribution.\n\nModel Selection:\n\nChoose models that are appropriate for the identified probability distribution.\n\nCheck Assumptions:\n\nSome assumptions, such as independence, can be known to be violated from understanding the dataset. For example, if the data includes repeated measures, this should be accounted for in the model.\nOther assumptions, such as normality of residuals and homoscedasticity, can be tested after the model is built using diagnostic plots and tests.\n\nIterative Process:\n\nModel building is an iterative process. Start with a simple model, check assumptions, refine the model, and repeat as necessary.\n\n\nBy following these steps and being mindful of model assumptions, you can build more robust models and make more reliable inferences from your data\nThe below is a simple example of the subtle difference that can occur when specifying a model\n\nshot_small$dis_char &lt;- as.character(shot_small$shots.player.displayName)\n\nm0 &lt;- glm(Shot_outcome ~ shots.details.type ,data = shot_small,family = binomial)\n\nm1_glmer =glmer(Shot_outcome ~ shots.details.type +(1| dis_char),data = shot_small,family = binomial)\n\ns0 &lt;- summary(m0)\nsglmer &lt;-summary(m1_glmer)\n\nround(s0$coefficients,3)\n\n                                    Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                           -0.088      0.045  -1.958    0.050\nshots.details.typeGeneral Play Snap   -0.348      0.058  -5.966    0.000\nshots.details.typeGround Kick         -0.101      0.142  -0.709    0.478\nshots.details.typeSet Shot Regular     0.167      0.053   3.122    0.002\nshots.details.typeSet Shot Snap        0.481      0.088   5.495    0.000\n\nround(sglmer$coefficients,3)\n\n                                    Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                           -0.096      0.047  -2.047    0.041\nshots.details.typeGeneral Play Snap   -0.363      0.059  -6.131    0.000\nshots.details.typeGround Kick         -0.135      0.144  -0.943    0.346\nshots.details.typeSet Shot Regular     0.135      0.055   2.465    0.014\nshots.details.typeSet Shot Snap        0.459      0.089   5.126    0.000"
  },
  {
    "objectID": "module3.html#final-models",
    "href": "module3.html#final-models",
    "title": "4  Module 3",
    "section": "4.9 Final Models",
    "text": "4.9 Final Models\n\n4.9.1 Poisson model\nEarlier we introduced the Poisson family regression for counts. Lets quickly explore trying to model TACKLES as a function of HARD_BALL_GET.\n\nmean(data1$TACKLE)\n\n[1] 2.66093\n\nvar(data1$TACKLE)\n\n[1] 5.012247\n\ncount_model= glm(TACKLE~CONTESTED_POSSESSION,data1,\n                 family = \"poisson\")\n\nsummary(count_model)\n\n\nCall:\nglm(formula = TACKLE ~ CONTESTED_POSSESSION, family = \"poisson\", \n    data = data1)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          0.456261   0.012017   37.97   &lt;2e-16 ***\nCONTESTED_POSSESSION 0.080189   0.001439   55.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 19097  on 9935  degrees of freedom\nResidual deviance: 16299  on 9934  degrees of freedom\nAIC: 40427\n\nNumber of Fisher Scoring iterations: 5\n\npred_count &lt;- predict_response(count_model)\n\nplot(pred_count,show_data = T,jitter = T)\n\n\n\ncheck_model(count_model)\n\nNot enough model terms in the conditional part of the model to check for\n  multicollinearity.\n\n\n\n\n\n\n\n4.9.2 Multinominial\nLets say you were actually interested in modelling GOAL, BEHIND and MISS. Not just GOAL, MISS as we were before. You could do that using what is called a multinominal regression. This is extension of the GLM frame work but we have to use a different package to do this.\n\nmnommod&lt;- multinom(shots.result.code~Rotated.xStd+Rotated.yStd,data = data2)\n\n# weights:  12 (6 variable)\ninitial  value 11962.789211 \niter  10 value 11098.696107\niter  10 value 11098.696107\niter  10 value 11098.696107\nfinal  value 11098.696107 \nconverged\n\nggeffect(mnommod, terms = \"Rotated.xStd\") |&gt;\n    plot()"
  },
  {
    "objectID": "module3.html#what-we-didnt-get-time-to-explore",
    "href": "module3.html#what-we-didnt-get-time-to-explore",
    "title": "4  Module 3",
    "section": "4.10 What we didn’t get time to explore",
    "text": "4.10 What we didn’t get time to explore\n\nGaussian regression\nPoisson regression\nBinomial regression (classification)\nFractional binomial regression\nQuasibinomial regression\nMultinomial classification\nGamma regression\nOrdinal regression\nNegative Binomial regression\nTweedie distribution"
  },
  {
    "objectID": "module3.html#next-module",
    "href": "module3.html#next-module",
    "title": "4  Module 3",
    "section": "4.11 Next module",
    "text": "4.11 Next module\nWe are going to skip a lot of the math and look to say who really cares about all the technical stuff, lets just see how well we can predict something."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "GWS Data Science and Problem Recognition",
    "section": "",
    "text": "Free R for data science resource: https://r4ds.had.co.nz/introduction.html\nPlotting in R using ggplot2 https://ggplot2-book.org/\n\nFree R for data science resource: https://r4ds.had.co.nz/introduction.html\nFree ISLR resource https://www.statlearning.com/\nHands on machine learning https://bradleyboehmke.github.io/HOML/\nhttps://www.marcellgranat.com/bigdata/content/15-content\nTidymodels: https://www.tidymodels.org/start/\nVisual learning:\nhttps://mlu-explain.github.io/\nhttp://www.r2d3.us/\n\nYoutube:\nhttps://www.youtube.com/@TidyX_screencast\nhttps://www.youtube.com/@JuliaSilge/videos"
  },
  {
    "objectID": "module4.html#recap-from-last-week",
    "href": "module4.html#recap-from-last-week",
    "title": "5  module4",
    "section": "5.1 Recap from last week",
    "text": "5.1 Recap from last week\nWe explored our ability to predict shot success compared to what Champion provides. We primarily did this through the lens of the generalized linear model (GLM) framework. The key strength of the GLM framework is its ability to represent models in a “linear” form even when estimating the effect on a binary variable (goal, miss), a count variable (number of disposals), or a range of other types of outcome distributions that don’t follow a normal distribution.\nAs the GLM framework is still linear, we can interpret the coefficients using standard hypothesis testing procedures (e.g., p-values). Additionally, with a bit of extra effort, we can extend the GLM framework further to allow for non-linear effects (we did this using the GAM model) and also situations where we violate the independence assumption. This final point can be achieved using mixed effects models, where you effectively let the model know that some observations will be clustered within players or teams, etc.\nIn research settings, you can see how the above type of modeling procedure can be quite attractive. We can make inferences about effects within a relatively flexible framework, provided that certain assumptions about the modeling procedure are met"
  },
  {
    "objectID": "module4.html#goals-for-this-week.",
    "href": "module4.html#goals-for-this-week.",
    "title": "5  Module 4",
    "section": "5.2 Goals for this week.",
    "text": "5.2 Goals for this week.\nOur current outline for the next out weeks looks like the below.\n\nIntroduction into R programming language and getting started with some descriptive statistics.\nUnderstanding key terms; Exploratory analysis, Supervised vs Unsupervised problems, regression vs classification, prediction vs association.\nA quick note on distributions - Supervised regression and classification problems using a generalized linear model framework. (Hypothesis testing)\nMachine learning approach to Supervised learning\nMethods of assessing model accuracy\nBias - variance trade-off, Feature selection, hyper-parameter tuning. (this may get blended in with 5)\nUnsupervised learning strategies (cluster analysis, PCA, market basket analysis)\nCreating a machine learning pipeline"
  },
  {
    "objectID": "module4.html#machine-learning-approach-to-supervised-learning",
    "href": "module4.html#machine-learning-approach-to-supervised-learning",
    "title": "5  Module 4",
    "section": "5.3 Machine learning approach to supervised learning",
    "text": "5.3 Machine learning approach to supervised learning\n\n5.3.1 Some quick context\nWhat makes a “machine learning” model?\nGreat question!! With no definitive answer. Technically, the GLM framework above could be considered a machine learning model, and indeed, many papers have referred to things such as logistic regression as machine learning.\nFrom an operational perspective, I like to think of it more along the lines of models that either focus on making reliable inferences about the variables used to predict an outcome (e.g., goal or miss, score differential) or models where the focus is more on the prediction itself, with less concern about how the prediction was made. (This is a very generalized statement)\nPut differently, some models will significantly relax their assumptions to find clever ways to better fit the data. This relaxation of assumptions makes it harder for us to know exactly what has happened under the hood. For example, in a linear model, I effectively tell the model that I assume the effect to be “linear,” which constrains the model to this rule. This makes it easier for us to interpret but not very flexible.\nLast week, when we used the GAM model to fit the shot data, we were effectively fitting a model that allowed us to look for non-linear effects. The easiest way to learn about these potential non-linear effects was to plot them*. Now, imagine a situation where we believe there might be interactions between different variables, some of which may be linear or non-linear, and we don’t really know in advance. It would be beneficial to have models that can potentially identify these interactions for us, and some of those models are what we are going to explore today\n*Note it is still possible to do hypothesis testing on the non-linear effects with GAMS but plotting effects is really the only way to understand the non-linear effects.\n\n\n5.3.2 What is an interaction effect?\n\n\n\n\n\nWe can notice here some different effects, but lets say the effects might be even more complicated.\n\n\n\n\n\nNow imagine that whether the effect was linear, non-linear or linear dependent on other variables as well. Hopefully, you can see how quickly this can get quite complex to model with a simple linear model.\nFor example see this example below taken from https://link.springer.com/article/10.3758/s13428-024-02389-1\nWhere they are looking at factors that impact science ability over time. These factors include SES (Socio Economic Status), GMOTOR (Gross motor skills) and INTERN (internalization problems)."
  },
  {
    "objectID": "module4.html#enough-talking-lets-do-some-machine-learning",
    "href": "module4.html#enough-talking-lets-do-some-machine-learning",
    "title": "5  Module 4",
    "section": "5.4 Enough talking lets do some machine learning",
    "text": "5.4 Enough talking lets do some machine learning\nToday the main models we are going to explore are the Decision Tree and Random Forest Algorithm. Lets have a quick look at where these algorithms sit on the conceptual model map from the ISLR book.\n\nThe decision tree is perhaps considered one of the most interpretable machine learning algorithms in that you can just traverse a tree to get explanations of expected values (more on this in a second). As always, there is no free lunch and decision trees often are extremely sensitive to initial conditions and can vary a lot with small perturbations in the data. This link visualizers what I am talking about https://mlu-explain.github.io/decision-tree/.\nThis subsequently, reduces there performance on hold out data set and makes decision trees not necessarily the best option from a predictive standpoint.\nWe will go over how to use them anyway, as there may be an occasion where the difference in performance between a more complicated model and a decision tree is trivial enough that you choose to go with a model that is more easily interpreted. Or If you are relative confident that the key variables are the same between models, you may use the decision tree as a point of illustration but know that in the back ground you are using a better performing model."
  },
  {
    "objectID": "module4.html#load-data-and-packages",
    "href": "module4.html#load-data-and-packages",
    "title": "5  Module 4",
    "section": "5.5 Load data and packages",
    "text": "5.5 Load data and packages\n\npacman::p_load(data.table,tidyverse,caret,mgcv,ggtext,\n               rpart,rpart.plot,pdp,party,ranger,glue,patchwork)\n\nurlfile=\"https://raw.githubusercontent.com/R2mu/GWS_DSPR/main/data/mod2data.csv\"\n\ndata1 &lt;- fread(urlfile)\n\nurlfile2=\"https://raw.githubusercontent.com/R2mu/GWS_DSPR/main/data/shotdata.csv\"\n\ndata2 &lt;- fread(urlfile2)\ndata2 &lt;- data2|&gt;\n  rename(Rotated.xStd=shots.details.locationRotated.xStd,\n         Rotated.yStd=shots.details.locationRotated.yStd)\n\nshot_small &lt;- data2%&gt;%\n  filter(Rotated.xStd&gt;(-20))|&gt;\n  mutate(Shot_outcome = as.factor(ifelse(shots.result.code==\"G\",1,0)),\n         Champ_Xp_err = round(shots.result.points-shots.result.pointsExpected,3))\n\nshot_small$shots.player.displayName&lt;-as.factor(shot_small$shots.player.displayName)\nshot_small$shots.details.type&lt;-as.factor(shot_small$shots.details.type)\n\nWe will continue to focus on what we explored last week and see if these machine learning models can help do a better job than what we achieved last week. Lets quickly recap where we got to last week.\n\n## simple logistic regression model\nm1 &lt;- glm(Shot_outcome~Rotated.xStd+Rotated.yStd,\n          data = shot_small,\n          family = binomial)\n\n## \nm2 &lt;- bam(Shot_outcome~te(Rotated.xStd,Rotated.yStd,by=shots.details.type)+\n                    s(shots.player.displayName,bs=\"re\"),\n           family = binomial,\n           nthreads=8,\n           discrete = T,\n           data = shot_small ,method = \"fREML\")\n\n\nshot_small &lt;- shot_small|&gt;\n  mutate(xG_m1 = predict(m1,type=\"response\")*6,\n         xG_m2 = predict(m2,type=\"response\")*6,\n         glm_Xp_err = round(shots.result.points-xG_m1,3),\n         gam_Xp_err = round(shots.result.points-xG_m2,3))\n\n\nMAE_champ   = round(mean(abs(shot_small$Champ_Xp_err)),2)\nMAE_glm     = round(mean(abs(shot_small$glm_Xp_err)),2)\nMAE_gam     = round(mean(abs(shot_small$gam_Xp_err)),2)\n\nggplot(shot_small,aes(abs(Champ_Xp_err))) +\n  geom_density(size = 3,fill=\"#0072B2\",alpha=.3,col=NA) +\n    geom_density(fill=\"#D55E00\",alpha=.3,\n               shot_small, mapping=aes(abs(glm_Xp_err)))+\n    geom_density(fill=\"#009E73\",alpha=.3,\n               shot_small, mapping=aes(abs(gam_Xp_err)))+\n  labs(\n    title = glue(\"**Model comparison**&lt;br&gt;\",\n                 \"&lt;span style='color:#0072B2;'&gt;Champ: {MAE_champ}&lt;/span&gt;, \",\n                 \"&lt;span style='color:#D55E00;'&gt;GLM: {MAE_glm}&lt;/span&gt;, and \",\n                 \"&lt;span style='color:#009E73;'&gt;GAM: {MAE_gam}&lt;/span&gt;\")\n  )+\n  theme_minimal() +\n  theme(\n    plot.title = element_markdown(lineheight = 1.1),\n    legend.text = element_markdown(size = 11)\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "module4.html#decision-tree",
    "href": "module4.html#decision-tree",
    "title": "5  Module 4",
    "section": "5.6 Decision tree",
    "text": "5.6 Decision tree\nLets create our first decision tree.\n\nm3_dt &lt;- rpart(\n  formula = Shot_outcome~Rotated.xStd+Rotated.yStd+ shots.details.type,\n  data    = shot_small,\n  method  = \"class\"\n)\n\nsummary(m3_dt)\n\nCall:\nrpart(formula = Shot_outcome ~ Rotated.xStd + Rotated.yStd + \n    shots.details.type, data = shot_small, method = \"class\")\n  n= 10886 \n\n          CP nsplit rel error    xerror        xstd\n1 0.08892714      0 1.0000000 1.0000000 0.009968954\n2 0.04016064      2 0.8221457 0.8328552 0.009775323\n3 0.03155479      3 0.7819851 0.8024479 0.009711314\n4 0.01099637      4 0.7504303 0.7588449 0.009603362\n5 0.01000000      6 0.7284376 0.7481354 0.009573871\n\nVariable importance\n      Rotated.yStd       Rotated.xStd shots.details.type \n                46                 36                 18 \n\nNode number 1: 10886 observations,    complexity param=0.08892714\n  predicted class=0  expected loss=0.4803417  P(node) =1\n    class counts:  5657  5229\n   probabilities: 0.520 0.480 \n  left son=2 (2261 obs) right son=3 (8625 obs)\n  Primary splits:\n      Rotated.yStd       &lt; -21.05 to the left,  improve=139.95050, (0 missing)\n      Rotated.xStd       &lt; 65.05  to the left,  improve= 77.49282, (0 missing)\n      shots.details.type splits as  RLLRR,      improve= 65.45643, (0 missing)\n\nNode number 2: 2261 observations\n  predicted class=0  expected loss=0.3237506  P(node) =0.207698\n    class counts:  1529   732\n   probabilities: 0.676 0.324 \n\nNode number 3: 8625 observations,    complexity param=0.08892714\n  predicted class=1  expected loss=0.4786087  P(node) =0.792302\n    class counts:  4128  4497\n   probabilities: 0.479 0.521 \n  left son=6 (2575 obs) right son=7 (6050 obs)\n  Primary splits:\n      Rotated.yStd       &lt; 17.55  to the right, improve=124.69680, (0 missing)\n      Rotated.xStd       &lt; 57.75  to the left,  improve= 97.15160, (0 missing)\n      shots.details.type splits as  RLLRR,      improve= 88.77016, (0 missing)\n\nNode number 6: 2575 observations\n  predicted class=0  expected loss=0.391068  P(node) =0.2365423\n    class counts:  1568  1007\n   probabilities: 0.609 0.391 \n\nNode number 7: 6050 observations,    complexity param=0.04016064\n  predicted class=1  expected loss=0.4231405  P(node) =0.5557597\n    class counts:  2560  3490\n   probabilities: 0.423 0.577 \n  left son=14 (2274 obs) right son=15 (3776 obs)\n  Primary splits:\n      shots.details.type splits as  RLLRR,      improve=110.3041, (0 missing)\n      Rotated.xStd       &lt; 51.45  to the left,  improve=106.6849, (0 missing)\n      Rotated.yStd       &lt; -11.75 to the left,  improve= 36.7377, (0 missing)\n  Surrogate splits:\n      Rotated.xStd &lt; 14.5   to the left,  agree=0.627, adj=0.008, (0 split)\n\nNode number 14: 2274 observations,    complexity param=0.01099637\n  predicted class=0  expected loss=0.4538259  P(node) =0.2088922\n    class counts:  1242  1032\n   probabilities: 0.546 0.454 \n  left son=28 (721 obs) right son=29 (1553 obs)\n  Primary splits:\n      Rotated.xStd       &lt; 51.45  to the left,  improve=27.450270, (0 missing)\n      Rotated.yStd       &lt; -12.55 to the left,  improve=16.799070, (0 missing)\n      shots.details.type splits as  -RL--,      improve= 2.128917, (0 missing)\n  Surrogate splits:\n      Rotated.yStd &lt; 17.35  to the right, agree=0.683, adj=0.001, (0 split)\n\nNode number 15: 3776 observations,    complexity param=0.03155479\n  predicted class=1  expected loss=0.3490466  P(node) =0.3468675\n    class counts:  1318  2458\n   probabilities: 0.349 0.651 \n  left son=30 (1349 obs) right son=31 (2427 obs)\n  Primary splits:\n      Rotated.xStd       &lt; 41.05  to the left,  improve=188.85380, (0 missing)\n      Rotated.yStd       &lt; -11.75 to the left,  improve= 21.76219, (0 missing)\n      shots.details.type splits as  L--RR,      improve= 18.69419, (0 missing)\n\nNode number 28: 721 observations\n  predicted class=0  expected loss=0.3398058  P(node) =0.06623186\n    class counts:   476   245\n   probabilities: 0.660 0.340 \n\nNode number 29: 1553 observations,    complexity param=0.01099637\n  predicted class=1  expected loss=0.4932389  P(node) =0.1426603\n    class counts:   766   787\n   probabilities: 0.493 0.507 \n  left son=58 (308 obs) right son=59 (1245 obs)\n  Primary splits:\n      Rotated.yStd       &lt; -12.55 to the left,  improve=19.513420, (0 missing)\n      shots.details.type splits as  -RL--,      improve=12.585550, (0 missing)\n      Rotated.xStd       &lt; 75.55  to the right, improve= 1.830883, (0 missing)\n\nNode number 30: 1349 observations\n  predicted class=0  expected loss=0.4388436  P(node) =0.1239206\n    class counts:   757   592\n   probabilities: 0.561 0.439 \n\nNode number 31: 2427 observations\n  predicted class=1  expected loss=0.2311496  P(node) =0.2229469\n    class counts:   561  1866\n   probabilities: 0.231 0.769 \n\nNode number 58: 308 observations\n  predicted class=0  expected loss=0.3474026  P(node) =0.02829322\n    class counts:   201   107\n   probabilities: 0.653 0.347 \n\nNode number 59: 1245 observations\n  predicted class=1  expected loss=0.4538153  P(node) =0.1143671\n    class counts:   565   680\n   probabilities: 0.454 0.546 \n\n\nit is possible to read through this, but its just a lot easier to plot it instead.\n\nrpart.plot(m3_dt)\n\n\n\n\nLets see how it performed even though I can tell already it hasn’t done that well.\n\nshot_small &lt;- shot_small|&gt;\n  mutate(xG_m3 = predict(m3_dt,type=\"prob\")[,2]*6,\n         DT_Xp_err = round(shots.result.points-xG_m3,3))\n\nMAE_DT      = round(mean(abs(shot_small$DT_Xp_err)),2)\n\nggplot(shot_small,aes(abs(Champ_Xp_err))) +\n  geom_density(size = 3,fill=\"#0072B2\",alpha=.3,col=NA) +\n    geom_density(fill=\"#D55E00\",alpha=.3,\n               shot_small, mapping=aes(abs(glm_Xp_err)))+\n    geom_density(fill=\"#009E73\",alpha=.3,\n               shot_small, mapping=aes(abs(gam_Xp_err)))+\n      geom_density(fill=\"#CC79A7\",alpha=.3,\n               shot_small, mapping=aes(abs(DT_Xp_err)))+\n  labs(\n    title = glue(\"**Model comparison**&lt;br&gt;\",\n                 \"&lt;span style='color:#0072B2;'&gt;Champ: {MAE_champ}&lt;/span&gt;, \",\n                 \"&lt;span style='color:#D55E00;'&gt;GLM: {MAE_glm}&lt;/span&gt;, and \",\n                 \"&lt;span style='color:#009E73;'&gt;GAM: {MAE_gam}&lt;/span&gt;, and \",\n                 \"&lt;span style='color:#CC79A7;'&gt;DT: {MAE_DT}&lt;/span&gt;\")\n  )+\n  theme_minimal() +\n  theme(\n    plot.title = element_markdown(lineheight = 1.1),\n    legend.text = element_markdown(size = 11)\n  )\n\n\n\n\nSo we can see it did better than our GLM model but worse than the champ model and GAM model. Now to be fair to the decision tree model there is some additional tuning we could do to it but the above does sort of align with a general summary of DT’s. In that they are most likely never going to have best in house predictive performance despite having a high degree of interpretability.\nAs mentioned earlier decision trees are very sensitive to initial conditions and can vary a lot from small deviations in data. https://mlu-explain.github.io/decision-tree/."
  },
  {
    "objectID": "module4.html#random-forest",
    "href": "module4.html#random-forest",
    "title": "5  Module 4",
    "section": "5.7 Random forest",
    "text": "5.7 Random forest\nA simple solution around the above probelm is to not build one decision tree but many decision tress (100s or 1000s) and take an aggregation of all the decision trees to give a pooled estimate. This approach is called bagging, with a special case of the bagging method called the Random forest algorithm. We will explore how a random forest algorithm will work on our data as it tends to have good out of the bag performance on tabular data but know that there other methods such as bagged trees and boosting methods which can also perform well.\n\n\nm4_rf &lt;- ranger(\n  formula = Shot_outcome ~ Rotated.xStd + Rotated.yStd + shots.details.type,\n  data = shot_small,\n  num.trees = 300,  # Number of trees in the forest\n  mtry = NULL,      # Number of variables to possibly split at in each node\n  importance = 'permutation',  # Calculate variable importance\n  probability = TRUE  # For classification, set to TRUE to get probabilities\n)\n\nLets see how it performed\n\nshot_small &lt;- shot_small|&gt;\n  mutate(xG_m4 = predict(m4_rf,data=shot_small,type = \"response\")$predictions[,2]*6,\n         RF_Xp_err = round(shots.result.points-xG_m4,3))\n\n\nMAE_RF      = round(mean(abs(shot_small$RF_Xp_err)),2)\n\ng1 =ggplot(shot_small,aes(abs(Champ_Xp_err))) +\n  geom_density(size = 3,fill=\"#0072B2\",alpha=.3,col=NA) +\n    geom_density(fill=\"#D55E00\",alpha=.3,\n               shot_small, mapping=aes(abs(glm_Xp_err)))+\n    geom_density(fill=\"#009E73\",alpha=.3,\n               shot_small, mapping=aes(abs(gam_Xp_err)))+\n      geom_density(fill=\"#CC79A7\",alpha=.3,\n               shot_small, mapping=aes(abs(DT_Xp_err)))+\n       geom_density(fill=\"#E69F00\",alpha=.3,\n               shot_small, mapping=aes(abs(RF_Xp_err)))+\n  labs(\n    title = glue(\"**Model comparison**&lt;br&gt;\",\n                 \"&lt;span style='color:#0072B2;'&gt;Champ: {MAE_champ}&lt;/span&gt;, \",\n                 \"&lt;span style='color:#D55E00;'&gt;GLM: {MAE_glm}&lt;/span&gt;, \",\n                 \"&lt;span style='color:#009E73;'&gt;GAM: {MAE_gam}&lt;/span&gt;, \",\n                 \"&lt;span style='color:#CC79A7;'&gt;DT: {MAE_DT}&lt;/span&gt;, \",\n                 \"&lt;span style='color:#E69F00;'&gt;RF: {MAE_RF}&lt;/span&gt;\")\n  )+\n  theme_minimal() +\n  theme(\n    plot.title = element_markdown(lineheight = 1.1),\n    legend.text = element_markdown(size = 11)\n  )\n\n\ng2 =ggplot(shot_small,aes(abs(Champ_Xp_err))) +\n    geom_density(fill=\"#009E73\",alpha=.3,\n               shot_small, mapping=aes(abs(gam_Xp_err)))+\n       geom_density(fill=\"#E69F00\",alpha=.3,\n               shot_small, mapping=aes(abs(RF_Xp_err)))+\n  labs(title = glue(\n                 \"&lt;span style='color:#009E73;'&gt;GAM: {MAE_gam}&lt;/span&gt;, \",\n                 \"&lt;span style='color:#E69F00;'&gt;RF: {MAE_RF}&lt;/span&gt;\"))+\n  theme_minimal() +\n  theme(\n    plot.title = element_markdown(lineheight = 1.1),\n    legend.text = element_markdown(size = 11)\n  )\n\n\nfinres &lt;- data.frame(models = c(\"champ\",\"glm\",\"gam\",\"DT\",\"RF\"),\n                    MAE = c(mean(abs(shot_small$Champ_Xp_err)),MAE_glm,MAE_gam,\n                             MAE_DT,MAE_RF))\n\ng3= ggplot(finres,aes(x=models,y=MAE,col=models))+\n  geom_point()+\n  scale_colour_manual(values = c(\"champ\"=\"#0072B2\",\n                                 \"glm\" = \"#D55E00\",\n                                 \"gam\" = \"#009E73\",\n                                 \"DT\" = \"#CC79A7\",\n                                 \"RF\"=\"#E69F00\"))+\n   theme_minimal()\n(g1|g3)/g2"
  },
  {
    "objectID": "module4.html#interpreting-the-rf",
    "href": "module4.html#interpreting-the-rf",
    "title": "5  Module 4",
    "section": "5.8 Interpreting the RF?",
    "text": "5.8 Interpreting the RF?\nIn this situation it seems to be just beating our GAM model as mentioned earlier one of the strengths of RF is that they have very good out of the box performance and whilst there are parameters that can be tuned usually fine tuning those parameters only result in trivial increases.\nFor a detailed understanding of those parameters have a read of the chapter in the ISLR book https://www.statlearning.com/ or this chapter from book the Hands on Machine learning with R https://bradleyboehmke.github.io/HOML/random-forest.html.\nAs always there is no free lunch and a model that was perceived as easily interpretable in the decision tree now requires a different slightly more complex method for assessing which features are influencing the model.\n\ndata.frame(m4_rf$variable.importance)|&gt;\n  rownames_to_column(var=\"variables\")|&gt;\n  rename(\"importance\"=m4_rf.variable.importance)|&gt;\n  ggplot(aes(x=importance,y=variables))+\n  geom_col()+\n  theme_bw()\n\n\n\n\nNow, what exactly the word “importance” means for a Random Forest can be a tad complicated. There are two common ways it can be represented: impurity and permutation. In general, the key variables will often be the same between the two approaches (impurity and permutation).\nI’ve gone with the permutation-based method here because it has some nice characteristics:\n\nIt’s often more reliable, especially for complex datasets.\nIt’s less impacted by high cardinality features or correlations between variables.\nIt’s calculated after the model is trained, looking at how predictions change.\n\nIn contrast, the impurity method measures importance during training based on how features split the trees.\nSo to simply describe the plot above:\n\nBigger numbers mean more importance.\nThis importance is calculated by seeing how much the model accuracy decreases when data in a variable is randomly shuffled. If randomly shuffling the data in a column decreases the model accuracy a lot, this suggests the variable has high importance.\n\nRemember, these importance measures help us understand which features the model relies on most, but they can be misleading in that they don;t necessarily tell you the practical relevance of the effect observed. For that we need partial dependency plots.\nIf you decide to run this just note this can take some time to run. e.g. on my decently powered comp this can take 10 minutes.\n\nsample_data &lt;- shot_small[sample(nrow(shot_small), 500), ]\n\npdp_result&lt;- partial(m4_rf,\n        pred.var = c(\"Rotated.xStd\",\"Rotated.yStd\",\"shots.details.type\"), \n        prob = T,\n        which.class = 2,\n        train = sample_data)\n\nggplot(pdp_result, aes(x = Rotated.xStd, y = Rotated.yStd, fill = yhat)) +\n  geom_tile() +\n  #geom_contour(aes(z = yhat), color = \"white\") +\n  scale_fill_viridis_c(name = \"Probability of Goal\") +\n  facet_wrap(~ shots.details.type,ncol = 3) +\n  theme_minimal() +\n  labs(title = \"Partial Dependence Plot by shot type\",\n       subtitle = \"Probability of goal\")\n\n\n\n\n\nggplot(setDT(pdp_result)[shots.details.type==\"Set Shot Regular\",],\n       aes(x = Rotated.xStd, y = Rotated.yStd, fill = yhat)) +\n  geom_tile() +\n  #geom_contour(aes(z = yhat), color = \"white\") +\n  scale_fill_viridis_c(name = \"Probability of Goal\") +\n  facet_wrap(~ shots.details.type,ncol = 3) +\n  theme_minimal() +\n  labs(title = \"Partial Dependence Plot by shot type\",\n       subtitle = \"Probability of goal\")\n\n\n\n\nLets compare that to the GAM model.\n\nvis.gam(m2, view = c(\"Rotated.xStd\", \"Rotated.yStd\"),\n        cond=list(shots.details.type=\"Set Shot Regular\"),\n        plot.type = \"contour\",\n       color = \"heat\",\n        too.far = 0.1,\n        lwd=2,\n        #labcex = 0.8,\n        contour.col  = \"black\",nCol = 20,\n        type = \"response\",main =\"Set Shot Regular\")\n\n\n\n\nPretty similar but you may be able to see that the GAM model is smoother while the RF is more segmented. This represents the decision boundaries that happen when creating a decision tree."
  },
  {
    "objectID": "module4.html#so-just-use-random-forest",
    "href": "module4.html#so-just-use-random-forest",
    "title": "5  Module 4",
    "section": "5.9 So just use Random Forest?",
    "text": "5.9 So just use Random Forest?\nWell the Random forest is a very good algorithm but we initially used it on a very big data set and whilst it has lots of procedures in place to limit over-fitting it definitely is not immune to it. Over-fitting refers to when machine learning models start to try and effectively model/capture noise in data. This can cause problems when trying to make future predictions that won;t necessarily follow those same rules learnt.\nLets see if our RF is struggling with that?\n\nlibrary(rsample)\n\nset.seed(123)\n\n## lets make our dataset smaller with only 3000 observations\nshot_smaller &lt;- shot_small[sample(nrow(shot_small), 3000), ]\n\n### we are going to randomly select 75% for training and leave 25% for testing\ntrain_test_split.Z  &lt;- initial_split(shot_smaller,  prop = 3/4) \n\n### here is our 75% split for training\nTRAIN.z &lt;- training(train_test_split.Z\n                    )[,c(\"shots.player.displayName\",\"shots.details.type\"):=\n                        lapply(.SD,as.factor),\n                      .SDcols=c(\"shots.player.displayName\",\"shots.details.type\")]\n\n## helper function for later\ntrainNames = unique(TRAIN.z$shots.player.displayName)\n\n## here is our 25% spliit of data for testing\nTEST.z  &lt;- testing(train_test_split.Z\n                   )[,c(\"shots.player.displayName\",\"shots.details.type\"):=\n                        lapply(.SD,as.factor),\n                      .SDcols=c(\"shots.player.displayName\",\"shots.details.type\")\n                     ][,key:= ifelse(shots.player.displayName%in%trainNames,1,0)]\n\n\nTEST.z = TEST.z[key==1,]\n\n\nm2_gam_TRAIN &lt;- bam(\n           Shot_outcome~te(Rotated.xStd,Rotated.yStd,by=shots.details.type)+\n                    s(shots.player.displayName,bs=\"re\"),\n           family = binomial,\n           nthreads=8,\n           discrete = T,\n           data = TRAIN.z ,\n           method = \"fREML\")\n\n\nm4_rf_TRAIN &lt;- ranger(\n  formula = Shot_outcome ~ Rotated.xStd + Rotated.yStd + shots.details.type,\n  data = TRAIN.z,\n  num.trees = 500,  # Number of trees in the forest\n  mtry = NULL,      # Number of variables to possibly split at in each node\n  importance = 'permutation',  # Calculate variable importance\n  probability = TRUE  # For classification, set to TRUE to get probabilities\n)\n\n\nTEST.z = TEST.z|&gt;\n   mutate(\n    # For GAM model\n    xG_gam = predict(m2_gam_TRAIN, newdata = cur_data(),type=\"response\")*6,\n    gam_Xp_err = round(shots.result.points - xG_gam, 3),\n    \n    # For Random Forest model\n    xG_rf = predict(m4_rf_TRAIN, data = cur_data(), type = \"response\")$predictions[,2] * 6,\n    RF_Xp_err = round(shots.result.points - xG_rf, 3)\n  )\n\nMAE_gam_test = round(mean(abs(TEST.z$gam_Xp_err)),2)\nMAE_RF_test  = round(mean(abs(TEST.z$RF_Xp_err)),2)\n\ng5= ggplot(TEST.z,aes(abs(Champ_Xp_err))) +\n    geom_density(fill=\"#009E73\",alpha=.3,\n               TEST.z, mapping=aes(abs(gam_Xp_err)))+\n       geom_density(fill=\"#E69F00\",alpha=.3,\n               TEST.z, mapping=aes(abs(RF_Xp_err)))+\n  labs(title = glue(\"&lt;b&gt;Test data&lt;/b&gt;&lt;br&gt;\n                 &lt;span style='color:#009E73;'&gt;GAM: {MAE_gam_test}&lt;/span&gt;,\n                 &lt;span style='color:#E69F00;'&gt;RF: {MAE_RF_test}&lt;/span&gt;\"))+\n  theme_minimal() +\n  theme(\n    plot.title = element_markdown(lineheight = 1.1),\n    legend.text = element_markdown(size = 11)\n  )\n\ng2|g5\n\n\n\n\nWhat do you notice?"
  },
  {
    "objectID": "module4.html#wrapping-up",
    "href": "module4.html#wrapping-up",
    "title": "5  Module 4",
    "section": "5.10 Wrapping up",
    "text": "5.10 Wrapping up\nWe have explored some commonly use machine learning algorithms today that can be used for both regression (predicting a number) and classification (predicting a class). There are many other algorithms that could be really useful and are discussed in some of the books I have referenced these include but not limited to.\n\nMARS : Multi Adaptive Regression Splines\nBART: Bayesian Addititve Regression Trees\nGPboost : allows for combining tree-boosting with Gaussian process and mixed effects models\nXGboost: Extreme Gradient Boosting"
  },
  {
    "objectID": "module4.html#next-week",
    "href": "module4.html#next-week",
    "title": "5  Module 4",
    "section": "5.11 Next Week",
    "text": "5.11 Next Week\nWe will explore in more detail over model performance metrics, model tuning and techniques for assessing model validity,reliability and calibration."
  },
  {
    "objectID": "module4.html#recap-from-last-module",
    "href": "module4.html#recap-from-last-module",
    "title": "5  Module 4",
    "section": "5.1 Recap from last module",
    "text": "5.1 Recap from last module\nWe explored our ability to predict shot success compared to what Champion provides. We primarily did this through the lens of the generalized linear model (GLM) framework. The key strength of the GLM framework is its ability to represent models in a “linear” form even when estimating the effect on a binary variable (goal, miss), a count variable (number of disposals), or a range of other types of outcome distributions that don’t follow a normal distribution.\nAs the GLM framework is still linear, we can interpret the coefficients using standard hypothesis testing procedures (e.g., p-values). Additionally, with a bit of extra effort, we can extend the GLM framework further to allow for non-linear effects (we did this using the GAM model) and also situations where we violate the independence assumption. This final point can be achieved using mixed effects models, where you effectively let the model know that some observations will be clustered within players or teams, etc.\nIn research settings, you can see how the above type of modeling procedure can be quite attractive. We can make inferences about effects within a relatively flexible framework, provided that certain assumptions about the modeling procedure are met"
  }
]